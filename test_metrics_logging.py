#!/usr/bin/env python3
"""
æµ‹è¯•trainingå’ŒevalæŒ‡æ ‡æ˜¯å¦èƒ½åŒæ—¶è®°å½•åˆ°WandB
"""

import os
import sys
import json
import time
from typing import Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_metrics_logging():
    """æµ‹è¯•æŒ‡æ ‡è®°å½•é€»è¾‘"""
    
    print("="*80)
    print("ğŸ” åˆ†ætrainingå’ŒevalæŒ‡æ ‡è®°å½•é€»è¾‘")
    print("="*80)
    
    # 1. æ£€æŸ¥è®­ç»ƒå¾ªç¯ä¸­çš„æŒ‡æ ‡è®°å½•é€»è¾‘
    print("\nğŸ“Š 1. è®­ç»ƒå¾ªç¯ä¸­çš„æŒ‡æ ‡è®°å½•é€»è¾‘:")
    print("   - åœ¨evalæ­¥éª¤æ—¶ï¼Œevaluate()æ–¹æ³•è¢«è°ƒç”¨ï¼Œä½†log_to_wandb=False")
    print("   - ç„¶åè®­ç»ƒå¾ªç¯åˆå¹¶trainingå’Œevalæ•°æ®ï¼Œä¸€æ¬¡æ€§è®°å½•")
    print("   - è¿™ç¡®ä¿äº†trainingå’ŒevalæŒ‡æ ‡åœ¨åŒä¸€ä¸ªstepä¸­è®°å½•")
    
    # 2. æ£€æŸ¥evaluateæ–¹æ³•ä¸­çš„æŒ‡æ ‡è®°å½•é€»è¾‘
    print("\nğŸ“Š 2. evaluateæ–¹æ³•ä¸­çš„æŒ‡æ ‡è®°å½•é€»è¾‘:")
    print("   - å½“log_to_wandb=Trueæ—¶ï¼Œä¼šå•ç‹¬è®°å½•evalæŒ‡æ ‡")
    print("   - å½“log_to_wandb=Falseæ—¶ï¼Œåªè¿”å›ç»“æœï¼Œä¸è®°å½•åˆ°WandB")
    print("   - è¿™é¿å…äº†é‡å¤è®°å½•")
    
    # 3. æ£€æŸ¥monitor.log_metricsæ–¹æ³•
    print("\nğŸ“Š 3. monitor.log_metricsæ–¹æ³•:")
    print("   - æ”¯æŒè®°å½•ä»»æ„æ•°é‡çš„æŒ‡æ ‡")
    print("   - æ‰€æœ‰æŒ‡æ ‡éƒ½ä½¿ç”¨ç›¸åŒçš„step")
    print("   - æ²¡æœ‰é™åˆ¶åªèƒ½è®°å½•ä¸€ç»„æŒ‡æ ‡")
    
    # 4. åˆ†æå¯èƒ½çš„é—®é¢˜
    print("\nâš ï¸  4. å¯èƒ½çš„é—®é¢˜åˆ†æ:")
    
    # æ£€æŸ¥è®­ç»ƒå¾ªç¯ä¸­çš„åˆå¹¶é€»è¾‘
    print("   a) è®­ç»ƒå¾ªç¯ä¸­çš„åˆå¹¶é€»è¾‘:")
    print("      - åœ¨evalæ­¥éª¤æ—¶ï¼Œä¼šåˆå¹¶trainingå’Œevalæ•°æ®")
    print("      - ä½¿ç”¨combined_data = {**current_training_data, **eval_data}")
    print("      - è¿™åº”è¯¥èƒ½åŒæ—¶è®°å½•ä¸¤ç»„æŒ‡æ ‡")
    
    # æ£€æŸ¥é¢‘ç‡è®¾ç½®
    print("\n   b) é¢‘ç‡è®¾ç½®æ£€æŸ¥:")
    print("      - eval_steps: æ§åˆ¶è¯„ä¼°é¢‘ç‡")
    print("      - logging_steps: æ§åˆ¶è®­ç»ƒæ—¥å¿—é¢‘ç‡")
    print("      - å¦‚æœeval_steps != logging_stepsï¼Œå¯èƒ½å¯¼è‡´æŸäº›æ­¥éª¤åªæœ‰ä¸€ç»„æŒ‡æ ‡")
    
    # æ£€æŸ¥WandBå›¾è¡¨å®šä¹‰
    print("\n   c) WandBå›¾è¡¨å®šä¹‰æ£€æŸ¥:")
    print("      - éœ€è¦ç¡®ä¿trainingå’ŒevalæŒ‡æ ‡éƒ½å®šä¹‰äº†å¯¹åº”çš„å›¾è¡¨")
    print("      - å¦‚æœå›¾è¡¨å®šä¹‰æœ‰é—®é¢˜ï¼Œå¯èƒ½æ˜¾ç¤ºä¸å®Œæ•´")
    
    # 5. å»ºè®®çš„è§£å†³æ–¹æ¡ˆ
    print("\nğŸ’¡ 5. å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
    print("   a) ç¡®ä¿eval_stepså’Œlogging_stepsè®¾ç½®åˆç†")
    print("   b) æ£€æŸ¥WandBå›¾è¡¨å®šä¹‰æ˜¯å¦å®Œæ•´")
    print("   c) éªŒè¯æŒ‡æ ‡åç§°å‰ç¼€æ˜¯å¦æ­£ç¡®")
    print("   d) æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡æ ‡è¢«è¿‡æ»¤æˆ–å¿½ç•¥")
    
    return True

def analyze_config_frequencies():
    """åˆ†æé…ç½®æ–‡ä»¶ä¸­çš„é¢‘ç‡è®¾ç½®"""
    
    print("\n" + "="*80)
    print("ğŸ“‹ åˆ†æé…ç½®æ–‡ä»¶ä¸­çš„é¢‘ç‡è®¾ç½®")
    print("="*80)
    
    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_dir = "configs"
    if os.path.exists(config_dir):
        config_files = [f for f in os.listdir(config_dir) if f.endswith('.yaml')]
        
        print(f"\næ‰¾åˆ° {len(config_files)} ä¸ªé…ç½®æ–‡ä»¶:")
        for config_file in config_files:
            config_path = os.path.join(config_dir, config_file)
            print(f"  ğŸ“„ {config_file}")
            
            try:
                import yaml
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # æå–å…³é”®é¢‘ç‡è®¾ç½®
                eval_steps = config.get('eval_steps', 'N/A')
                logging_steps = config.get('logging_steps', 'N/A')
                save_steps = config.get('save_steps', 'N/A')
                
                print(f"     - eval_steps: {eval_steps}")
                print(f"     - logging_steps: {logging_steps}")
                print(f"     - save_steps: {save_steps}")
                
                # æ£€æŸ¥é¢‘ç‡æ˜¯å¦åˆç†
                if eval_steps != 'N/A' and logging_steps != 'N/A':
                    if eval_steps == logging_steps:
                        print(f"     âœ… eval_steps == logging_stepsï¼ŒæŒ‡æ ‡ä¼šåŒæ—¶è®°å½•")
                    else:
                        print(f"     âš ï¸  eval_steps != logging_stepsï¼ŒæŸäº›æ­¥éª¤å¯èƒ½åªæœ‰ä¸€ç»„æŒ‡æ ‡")
                
            except Exception as e:
                print(f"     âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    else:
        print("âŒ æœªæ‰¾åˆ°configsç›®å½•")

def create_test_script():
    """åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬æ¥éªŒè¯æŒ‡æ ‡è®°å½•"""
    
    print("\n" + "="*80)
    print("ğŸ§ª åˆ›å»ºæµ‹è¯•è„šæœ¬")
    print("="*80)
    
    test_script = '''#!/usr/bin/env python3
"""
æµ‹è¯•trainingå’ŒevalæŒ‡æ ‡åŒæ—¶è®°å½•åˆ°WandB
"""

import os
import sys
import time
import json
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_combined_metrics_logging():
    """æµ‹è¯•åˆå¹¶æŒ‡æ ‡è®°å½•"""
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åˆå¹¶æŒ‡æ ‡è®°å½•...")
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'output_dir': './test_output',
        'wandb': {
            'project': 'test_metrics',
            'name': 'test_combined_metrics',
            'enabled': True
        },
        'monitor': {
            'freq': {
                'log_freq': 1,
                'eval_log_freq': 1,
                'perf_log_freq': 1,
                'flops_profile_freq': 10
            }
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # åˆå§‹åŒ–monitor
    from training.utils.monitor import TrainingMonitor
    monitor = TrainingMonitor(config['output_dir'], config)
    
    # æ¨¡æ‹Ÿè®­ç»ƒå’Œevalæ•°æ®
    for step in range(1, 11):
        print(f"\\nğŸ“Š æ­¥éª¤ {step}:")
        
        # æ¨¡æ‹Ÿtrainingæ•°æ®
        training_data = {
            "training/loss": 0.1 + step * 0.01,
            "training/lr": 1e-4,
            "training/epoch": 0.1,
            "training/grad_norm": 1.0 + step * 0.1,
        }
        
        # æ¨¡æ‹Ÿevalæ•°æ®ï¼ˆæ¯5æ­¥è¯„ä¼°ä¸€æ¬¡ï¼‰
        eval_data = {}
        if step % 5 == 0:
            eval_data = {
                "eval/overall_loss": 0.2 + step * 0.01,
                "eval/overall_accuracy": 0.8 - step * 0.01,
                "eval/overall_samples": 1000,
                "eval/overall_correct": 800 - step * 10,
            }
            print(f"   ğŸ“ˆ åŒ…å«evalæŒ‡æ ‡: {list(eval_data.keys())}")
        else:
            print(f"   ğŸ“ˆ ä»…åŒ…å«trainingæŒ‡æ ‡")
        
        # åˆå¹¶æ•°æ®
        combined_data = {**training_data, **eval_data}
        combined_data["step"] = step
        
        # è®°å½•åˆ°WandB
        monitor.log_metrics(combined_data, step, commit=True)
        
        print(f"   âœ… å·²è®°å½• {len(combined_data)} ä¸ªæŒ‡æ ‡")
        print(f"   ğŸ“Š æŒ‡æ ‡keys: {list(combined_data.keys())}")
        
        time.sleep(1)  # é¿å…WandB APIé™åˆ¶
    
    print("\\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("è¯·æ£€æŸ¥WandBç•Œé¢ï¼Œåº”è¯¥èƒ½çœ‹åˆ°:")
    print("  - training/loss, training/lr, training/epoch, training/grad_norm")
    print("  - eval/overall_loss, eval/overall_accuracy (æ¯5æ­¥)")
    print("  - æ‰€æœ‰æŒ‡æ ‡éƒ½ä½¿ç”¨ç›¸åŒçš„stepè½´")

if __name__ == "__main__":
    test_combined_metrics_logging()
'''
    
    with open('test_combined_metrics.py', 'w', encoding='utf-8') as f:
        f.write(test_script)
    
    print("âœ… å·²åˆ›å»ºæµ‹è¯•è„šæœ¬: test_combined_metrics.py")
    print("è¿è¡Œå‘½ä»¤: python test_combined_metrics.py")

def main():
    """ä¸»å‡½æ•°"""
    test_metrics_logging()
    analyze_config_frequencies()
    create_test_script()
    
    print("\n" + "="*80)
    print("ğŸ“‹ æ€»ç»“")
    print("="*80)
    print("æ ¹æ®ä»£ç åˆ†æï¼Œå½“å‰çš„å®ç°åº”è¯¥èƒ½å¤ŸåŒæ—¶è®°å½•trainingå’ŒevalæŒ‡æ ‡:")
    print("1. âœ… è®­ç»ƒå¾ªç¯ä¼šåˆå¹¶trainingå’Œevalæ•°æ®")
    print("2. âœ… monitor.log_metricsæ”¯æŒè®°å½•å¤šä¸ªæŒ‡æ ‡")
    print("3. âœ… æ‰€æœ‰æŒ‡æ ‡ä½¿ç”¨ç»Ÿä¸€çš„stepè½´")
    print("4. âš ï¸  éœ€è¦æ£€æŸ¥eval_stepså’Œlogging_stepsçš„è®¾ç½®")
    print("5. âš ï¸  éœ€è¦æ£€æŸ¥WandBå›¾è¡¨å®šä¹‰")
    print("\nå»ºè®®è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯å®é™…æ•ˆæœ")

if __name__ == "__main__":
    main() 