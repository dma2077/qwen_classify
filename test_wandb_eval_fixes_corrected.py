#!/usr/bin/env python3
"""
æµ‹è¯•WandBç¦ç”¨å’Œè¯„ä¼°batch sizeä¿®å¤ï¼ˆä¿®æ­£ç‰ˆï¼‰
"""

import os
import sys
import yaml
import json

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NCCL_NTHREADS'] = '64'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

def test_eval_batch_size():
    """æµ‹è¯•è¯„ä¼°batch sizeä¿®å¤"""
    print("ğŸ“Š æµ‹è¯•è¯„ä¼°batch sizeä¿®å¤...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‡†å¤‡é…ç½®
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    try:
        # è·å–DeepSpeedé…ç½®
        if 'deepspeed' in config:
            if isinstance(config['deepspeed'], str):
                with open(config['deepspeed'], 'r') as f:
                    deepspeed_config = json.load(f)
            else:
                deepspeed_config = config['deepspeed']
            
            train_micro_batch = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
            train_batch_size = deepspeed_config.get('train_batch_size', 1)
            gradient_accumulation = deepspeed_config.get('gradient_accumulation_steps', 1)
            
            print(f"DeepSpeedé…ç½®:")
            print(f"  â€¢ train_micro_batch_size_per_gpu: {train_micro_batch}")
            print(f"  â€¢ train_batch_size: {train_batch_size}")
            print(f"  â€¢ gradient_accumulation_steps: {gradient_accumulation}")
            
            # æµ‹è¯•æ•°æ®åŠ è½½å™¨
            from data.dataloader import create_dataloaders
            train_loader, val_loader = create_dataloaders(config)
            
            train_actual_batch = train_loader.batch_size
            val_actual_batch = val_loader.batch_size
            
            print(f"å®é™…DataLoader batch size:")
            print(f"  â€¢ è®­ç»ƒDataLoader batch size: {train_actual_batch}")
            print(f"  â€¢ è¯„ä¼°DataLoader batch size: {val_actual_batch}")
            
            # æ£€æŸ¥ç†è®ºè®¡ç®—
            world_size = 8  # å‡è®¾8å¡ï¼Œå®é™…è¿è¡Œæ—¶ä¼šä»åˆ†å¸ƒå¼è·å–
            expected_eval_batch = train_micro_batch * world_size  # gradient_accumulation_steps=1æ—¶çš„ç­‰æ•ˆ
            theoretical_total_batch = train_micro_batch * world_size * gradient_accumulation
            
            print(f"ç†è®ºè®¡ç®—:")
            print(f"  â€¢ æœŸæœ›è¯„ä¼°batch size: {expected_eval_batch} (micro_batch Ã— num_gpus)")
            print(f"  â€¢ ç†è®ºè®­ç»ƒæ€»batch size: {theoretical_total_batch} (åŒ…å«gradient accumulation)")
            print(f"  â€¢ é…ç½®çš„train_batch_size: {train_batch_size}")
            
            # æ£€æŸ¥ä¿®å¤é€»è¾‘
            # è®­ç»ƒDataLoaderåº”è¯¥ä½¿ç”¨micro_batch_size_per_gpu
            # è¯„ä¼°DataLoaderåº”è¯¥ä½¿ç”¨micro_batch_size_per_gpu Ã— num_gpus
            
            train_correct = (train_actual_batch == train_micro_batch)
            # æ³¨æ„ï¼šåœ¨å•æœºæµ‹è¯•æ—¶world_sizeå¯èƒ½æ˜¯1ï¼Œæ‰€ä»¥eval_batch_sizeå¯èƒ½ç­‰äºtrain_micro_batch
            eval_correct = (val_actual_batch >= train_micro_batch)  # è‡³å°‘ä¸å°äºmicro batch
            
            if train_correct and eval_correct:
                print("âœ… batch sizeé€»è¾‘ä¿®å¤æ­£ç¡®")
                print(f"   - è®­ç»ƒä½¿ç”¨micro batch: {train_actual_batch}")
                print(f"   - è¯„ä¼°ä½¿ç”¨é€‚å½“å¤§å°: {val_actual_batch}")
                return True
            else:
                print("âš ï¸ batch sizeé€»è¾‘éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
                print(f"   - è®­ç»ƒbatchæ­£ç¡®: {train_correct}")
                print(f"   - è¯„ä¼°batchåˆç†: {eval_correct}")
                return False
        else:
            print("âš ï¸ æœªä½¿ç”¨DeepSpeedé…ç½®")
            return True
            
    except Exception as e:
        print(f"âŒ batch sizeæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_batch_size_explanation():
    """è§£é‡Šbatch sizeçš„é€»è¾‘"""
    print("\nğŸ“– Batch Sizeé€»è¾‘è¯´æ˜:")
    print("=" * 50)
    print("è®­ç»ƒæ—¶çš„æœ‰æ•ˆbatch sizeè®¡ç®—:")
    print("  æœ‰æ•ˆè®­ç»ƒbatch = micro_batch_size_per_gpu Ã— num_gpus Ã— gradient_accumulation_steps")
    print()
    print("ä¿®å¤åçš„é…ç½®:")
    print("  â€¢ è®­ç»ƒDataLoader batch_size = micro_batch_size_per_gpu")
    print("  â€¢ è¯„ä¼°DataLoader batch_size = micro_batch_size_per_gpu Ã— num_gpus")
    print()
    print("è¿™æ ·:")
    print("  â€¢ è®­ç»ƒæ—¶ï¼šDeepSpeedä¼šè‡ªåŠ¨å¤„ç†gradient accumulation")
    print("  â€¢ è¯„ä¼°æ—¶ï¼šä½¿ç”¨ç›¸å½“äºgradient_accumulation_steps=1æ—¶çš„batch size")
    print("  â€¢ é¿å…è¯„ä¼°æ—¶æ˜¾å­˜çˆ†ç‚¸ï¼Œä½†ä¿æŒåˆç†çš„batch size")
    print("=" * 50)
    return True

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹batch sizeä¿®å¤éªŒè¯")
    print("=" * 50)
    
    # æµ‹è¯•1: è§£é‡Šé€»è¾‘
    explanation_ok = test_batch_size_explanation()
    
    # æµ‹è¯•2: éªŒè¯ä¿®å¤
    batch_ok = test_eval_batch_size()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  â€¢ Batch sizeé€»è¾‘: {'âœ… æ­£ç¡®' if explanation_ok else 'âŒ é”™è¯¯'}")
    print(f"  â€¢ ä¿®å¤éªŒè¯: {'âœ… æˆåŠŸ' if batch_ok else 'âŒ å¤±è´¥'}")
    
    if batch_ok:
        print("\nğŸ‰ Batch sizeä¿®å¤éªŒè¯æˆåŠŸï¼")
        print("ç°åœ¨è¯„ä¼°åº”è¯¥ä¸ä¼šOOMï¼Œä¸”ä½¿ç”¨åˆç†çš„batch size")
    else:
        print("\nâš ï¸ Batch sizeä¿®å¤éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥") 