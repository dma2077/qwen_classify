#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç®€åŒ–åçš„æ¨¡å‹ä»£ç 
"""

import torch
import sys
import os

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from models.qwen2_5_vl_classify import Qwen2_5_VLForImageClassification

def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–...")
    
    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = Qwen2_5_VLForImageClassification(
            pretrained_model_name="/llm_reco/dehua/model/Qwen2.5-VL-7B-Instruct",
            num_labels=101,
            loss_config={'type': 'label_smoothing', 'smoothing': 0.1},
            enable_logits_masking=True
        )
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        if hasattr(model.model, 'config') and hasattr(model.model.config, '_attn_implementation'):
            attn_impl = model.model.config._attn_implementation
            print(f"ğŸ“‹ Attentionå®ç°: {attn_impl}")
        else:
            print("ğŸ“‹ æ— æ³•æ£€æµ‹attentionå®ç°")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        print(f"ğŸ“‹ æ¨¡å‹æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
        
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def test_forward_pass(model):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = 512
        hidden_size = 4096  # Qwen2.5-VLçš„hidden_size
        
        # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        pixel_values = torch.randn(batch_size, 3, 224, 224, dtype=torch.bfloat16)
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        attention_mask = torch.ones(batch_size, seq_len)
        labels = torch.randint(0, 101, (batch_size,))
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            pixel_values=pixel_values,
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"ğŸ“‹ Loss: {outputs.loss}")
        print(f"ğŸ“‹ Logits shape: {outputs.logits.shape}")
        print(f"ğŸ“‹ Logits dtype: {outputs.logits.dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç®€åŒ–åçš„æ¨¡å‹ä»£ç ")
    print("=" * 50)
    
    # æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    model = test_model_initialization()
    
    if model is not None:
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_forward_pass(model)
    
    print("\n" + "=" * 50)
    print("âœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main() 