# è¶…å¿«é€Ÿè¯„ä¼°è®­ç»ƒé…ç½®æ–‡ä»¶
model:
  pretrained_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  num_labels: 101  # Food101æ•°æ®é›†

training:
  num_epochs: 10
  output_dir: "./outputs/ultra_fast_eval_training"
  
  # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–é…ç½®
  gradient_checkpointing: false         # ä¸å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦
  memory_efficient_attention: true      # å¯ç”¨FlashAttention
  amp: false                           # ä¸å¯ç”¨AMPï¼ŒDeepSpeedå·²å¯ç”¨bf16
  dataloader_pin_memory: true          # å¯ç”¨pin_memory
  dataloader_num_workers: 16           # æ•°æ®åŠ è½½å™¨workeræ•°é‡
  dataloader_prefetch_factor: 2        # é¢„å–å› å­
  
  # å­¦ä¹ ç‡é…ç½®
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  
  # ğŸ”¥ å¤§æ‰¹æ¬¡é…ç½®
  batch_size: 64                       # å¤§å¹…å¢åŠ æ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 1       # å‡å°‘æ¢¯åº¦ç´¯ç§¯
  
  # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå‡å°‘è¯„ä¼°é¢‘ç‡
  logging_steps: 10
  eval_steps: 500                      # è¿›ä¸€æ­¥å‡å°‘è¯„ä¼°é¢‘ç‡
  save_steps: 1000                     # å‡å°‘ä¿å­˜é¢‘ç‡
  
  # æœ€ä½³æ¨¡å‹è¿½è¸ª
  best_model_tracking:
    enabled: true
    metric: "overall_accuracy"
    mode: "max"
    save_best_only: true
  
  # è¯„ä¼°é…ç½®
  evaluation:
    partial_eval_during_training: true
    full_eval_at_end: true
    eval_best_model_only: true

# æ•°æ®é…ç½®
data:
  train_jsonl: "data/food101/train.jsonl"
  val_jsonl: "data/food101/val.jsonl"
  max_length: 512
  image_size: 224

# æŸå¤±å‡½æ•°é…ç½®
loss:
  type: "cross_entropy"

# å¤šæ•°æ®é›†é…ç½®
datasets:
  dataset_configs:
    food101:
      num_classes: 101
      eval_ratio: 1.0                  # ğŸ”¥ ä½¿ç”¨å…¨é‡è¯„ä¼°æ•°æ®
      description: "Food101 dataset"
  enable_logits_masking: true
  shuffle_datasets: true

# WandBé…ç½®
wandb:
  enabled: true
  project: "qwen-classification-ultra-fast"
  run_name: "ultra_fast_eval_training_run"
  tags: ["ultra-fast", "large-batch"]
  notes: "Ultra fast evaluation with large batch size and full eval data"
  log_dataset_metrics: true

# ç›‘æ§é…ç½®
monitor:
  use_wandb: true
  freq:
    training_log_freq: 10
    eval_log_freq: 500                 # ä¸eval_stepsä¿æŒä¸€è‡´
    perf_log_freq: 50                  # å‡å°‘æ€§èƒ½æŒ‡æ ‡è®°å½•é¢‘ç‡
    gpu_log_freq: 100                  # å‡å°‘GPUç›‘æ§é¢‘ç‡
    local_save_freq: 500               # å‡å°‘æœ¬åœ°ä¿å­˜é¢‘ç‡
    progress_update_freq: 50           # å‡å°‘è¿›åº¦æ¡æ›´æ–°é¢‘ç‡
  flops_profile_freq: 200              # å‡å°‘FLOPsåˆ†æé¢‘ç‡

# DeepSpeedé…ç½®
deepspeed: "configs/ds_large_eval_batch.json"

# ä¼˜åŒ–å™¨é…ç½®
optimizer:
  type: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
lr_scheduler:
  type: "cosine"
  warmup_steps: 100
  num_training_steps: 1000  # å°†åœ¨è¿è¡Œæ—¶è‡ªåŠ¨è®¡ç®— 