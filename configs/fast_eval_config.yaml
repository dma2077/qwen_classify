# å¿«é€Ÿè¯„ä¼°è®­ç»ƒé…ç½®æ–‡ä»¶
model:
  pretrained_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  num_labels: 101  # Food101æ•°æ®é›†

training:
  num_epochs: 10
  output_dir: "./outputs/fast_eval_training"
  
  # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–é…ç½®
  gradient_checkpointing: false         # ä¸å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦
  memory_efficient_attention: true      # å¯ç”¨FlashAttention
  amp: false                           # ä¸å¯ç”¨AMPï¼ŒDeepSpeedå·²å¯ç”¨bf16
  dataloader_pin_memory: true          # å¯ç”¨pin_memory
  dataloader_num_workers: 16           # æ•°æ®åŠ è½½å™¨workeræ•°é‡
  dataloader_prefetch_factor: 2        # é¢„å–å› å­
  
  # å­¦ä¹ ç‡é…ç½®
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  
  # æ‰¹æ¬¡é…ç½®
  batch_size: 32                      # ğŸ”¥ å¢åŠ è®­ç»ƒæ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 2
  
  # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå‡å°‘è¯„ä¼°é¢‘ç‡
  logging_steps: 10
  eval_steps: 200                      # ä»50å¢åŠ åˆ°200ï¼Œå‡å°‘è¯„ä¼°é¢‘ç‡
  save_steps: 500                      # ä»200å¢åŠ åˆ°500ï¼Œå‡å°‘ä¿å­˜é¢‘ç‡
  
  # æœ€ä½³æ¨¡å‹è¿½è¸ª
  best_model_tracking:
    enabled: true
    metric: "overall_accuracy"
    mode: "max"
    save_best_only: true
  
  # è¯„ä¼°é…ç½®
  evaluation:
    partial_eval_during_training: true
    full_eval_at_end: true
    eval_best_model_only: true

# æ•°æ®é…ç½®
data:
  train_jsonl: "data/food101/train.jsonl"
  val_jsonl: "data/food101/val.jsonl"
  max_length: 512
  image_size: 224

# æŸå¤±å‡½æ•°é…ç½®
loss:
  type: "cross_entropy"

# å¤šæ•°æ®é›†é…ç½®
datasets:
  dataset_configs:
    food101:
      num_classes: 101
      eval_ratio: 0.05                 # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šä»0.2é™ä½åˆ°0.05ï¼Œåªä½¿ç”¨5%çš„æ•°æ®è¯„ä¼°
      description: "Food101 dataset"
  enable_logits_masking: true
  shuffle_datasets: true

# WandBé…ç½®
wandb:
  enabled: true
  project: "qwen-classification-fast-eval"
  run_name: "fast_eval_training_run"
  tags: ["fast-eval", "performance"]
  notes: "Fast evaluation training with reduced eval frequency and data ratio"
  log_dataset_metrics: true

# ç›‘æ§é…ç½®
monitor:
  use_wandb: true
  freq:
    training_log_freq: 10
    eval_log_freq: 200                 # ä¸eval_stepsä¿æŒä¸€è‡´
    perf_log_freq: 20                  # å‡å°‘æ€§èƒ½æŒ‡æ ‡è®°å½•é¢‘ç‡
    gpu_log_freq: 50                   # å‡å°‘GPUç›‘æ§é¢‘ç‡
    local_save_freq: 200               # å‡å°‘æœ¬åœ°ä¿å­˜é¢‘ç‡
    progress_update_freq: 20           # å‡å°‘è¿›åº¦æ¡æ›´æ–°é¢‘ç‡
  flops_profile_freq: 100              # å‡å°‘FLOPsåˆ†æé¢‘ç‡

# DeepSpeedé…ç½®
deepspeed: "configs/ds_eval_optimized.json"

# ä¼˜åŒ–å™¨é…ç½®
optimizer:
  type: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
lr_scheduler:
  type: "cosine"
  warmup_steps: 100
  num_training_steps: 1000  # å°†åœ¨è¿è¡Œæ—¶è‡ªåŠ¨è®¡ç®— 