model:
  pretrained_name: "/llm_reco/dehua/model/Qwen2.5-VL-7B-Instruct"  # or your本地路径
  num_labels: 101

data:
  train_jsonl: "/llm_reco/dehua/code/qwen_classify/data/dataset_food_label/food101_train.jsonl"  # Changed to jsonl format
  val_jsonl:   "/llm_reco/dehua/code/qwen_classify/data/dataset_food_label/food101_test.jsonl"    # Changed to jsonl format

training:
  epochs:          10
  lr:              2e-5
  weight_decay:    0.01
  warmup_steps:    500
  output_dir:      "./outputs"
  logging_steps:   50
  save_steps:      1000
  num_workers:     0  # Set to 0 to avoid multiprocessing issues
  use_fused_adam:  true  # Use FusedAdam for better performance
  # Note: micro_batch_size_per_gpu and gradient_accumulation_steps are now in ds_s2.json

# DeepSpeed specific settings
deepspeed:
  config_file: "configs/ds_s2.json"
  zero_stage: 2
  bf16: true  # Using BF16 instead of FP16
  
# Weights & Biases configuration
wandb:
  enabled: true  # Set to false to disable wandb logging
  project: "qwen_classification"  # wandb project name
  run_name: null  # Optional run name, will be auto-generated if null
  tags: ["qwen", "deepspeed", "classification"]  # Optional tags
  notes: "Qwen2.5-VL classification training"  # Optional notes

# Legacy accelerate settings (for compatibility)
accelerate:
  bf16: true  # Using BF16 instead of FP16
  # Note: gradient_accumulation_steps is now managed by DeepSpeed (ds_s2.json)