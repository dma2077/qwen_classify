model:
  pretrained_name: "Qwen2.5-VL-Chat"  # or your本地路径
  num_labels: 101

data:
  train_jsonl: "/path/to/train.jsonl"  # Changed to jsonl format
  val_jsonl:   "/path/to/val.jsonl"    # Changed to jsonl format
  img_dir:     "/path/to/images"
  img_size:    224

training:
  epochs:          10
  batch_size:      16  # micro batch size per GPU
  gradient_accumulation_steps: 4  # effective batch size = batch_size * gradient_accumulation_steps * num_gpus
  lr:              2e-5
  weight_decay:    0.01
  warmup_steps:    500
  output_dir:      "./outputs"
  logging_steps:   50
  save_steps:      1000
  num_workers:     4
  use_fused_adam:  true  # Use FusedAdam for better performance

# DeepSpeed specific settings
deepspeed:
  config_file: "configs/deepspeed_config.json"
  zero_stage: 2
  fp16: true
  
# Weights & Biases configuration
wandb:
  enabled: true  # Set to false to disable wandb logging
  project: "qwen_classification"  # wandb project name
  run_name: null  # Optional run name, will be auto-generated if null
  tags: ["qwen", "deepspeed", "classification"]  # Optional tags
  notes: "Qwen2.5-VL classification training"  # Optional notes

# Legacy accelerate settings (for compatibility)
accelerate:
  gradient_accumulation_steps: 4  # Updated to match training config
  fp16: true