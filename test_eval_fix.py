#!/usr/bin/env python3
"""
æµ‹è¯•evalæŒ‡æ ‡ä¿®å¤
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_eval_metrics_fix():
    """æµ‹è¯•evalæŒ‡æ ‡ä¿®å¤"""
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'output_dir': '/tmp/test_eval_fix',
        'wandb': {
            'enabled': True,
            'project': 'qwen_classification_test',
            'run_name': 'test_eval_fix',
            'tags': ['test', 'eval', 'fix'],
            'notes': 'Testing eval metrics fix'
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    try:
        from training.utils.monitor import TrainingMonitor
        
        # åˆ›å»ºmonitor
        monitor = TrainingMonitor(config['output_dir'], config)
        
        print("âœ… Monitoråˆ›å»ºæˆåŠŸ")
        
        # ç­‰å¾…WandBåˆå§‹åŒ–
        time.sleep(2)
        
        # æµ‹è¯•è®°å½•evalæŒ‡æ ‡
        eval_metrics = {
            "eval/overall_loss": 0.1234,
            "eval/overall_accuracy": 0.8567,
            "eval/overall_samples": 1000,
            "eval/overall_correct": 857
        }
        
        print(f"ğŸ“Š å‡†å¤‡è®°å½•evalæŒ‡æ ‡: {list(eval_metrics.keys())}")
        
        # è®°å½•åˆ°WandB
        monitor.log_metrics(eval_metrics, step=100, commit=True)
        
        print("âœ… evalæŒ‡æ ‡è®°å½•å®Œæˆ")
        
        # ç­‰å¾…ä¸€ä¸‹è®©WandBåŒæ­¥
        time.sleep(3)
        
        # å†æ¬¡è®°å½•ä¸€äº›æŒ‡æ ‡
        eval_metrics_2 = {
            "eval/overall_loss": 0.0987,
            "eval/overall_accuracy": 0.9012,
            "eval/overall_samples": 1000,
            "eval/overall_correct": 901
        }
        
        monitor.log_metrics(eval_metrics_2, step=200, commit=True)
        
        print("âœ… ç¬¬äºŒæ¬¡evalæŒ‡æ ‡è®°å½•å®Œæˆ")
        
        # ç­‰å¾…åŒæ­¥
        time.sleep(3)
        
        # ç»“æŸWandB
        monitor.finish_training()
        
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("ğŸ”— è¯·æ£€æŸ¥WandBç•Œé¢ï¼Œåº”è¯¥èƒ½çœ‹åˆ°evalæŒ‡æ ‡")
        print("ğŸ“Š åº”è¯¥åŒ…å«: eval/overall_loss, eval/overall_accuracy, eval/overall_samples, eval/overall_correct")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_eval_metrics_fix() 