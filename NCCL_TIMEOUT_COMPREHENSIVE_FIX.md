# NCCLè¶…æ—¶é—®é¢˜ç»¼åˆä¿®å¤æ–¹æ¡ˆ

## ğŸš¨ é—®é¢˜è¯Šæ–­

### é”™è¯¯ç°è±¡
```
[rank6]:[E717 13:20:49.407638053 ProcessGroupNCCL.cpp:632] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=56731, OpType=ALLREDUCE, NumelIn=467019003, NumelOut=467019003, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
```

### å…³é”®ä¿¡æ¯åˆ†æ
- **æ“ä½œç±»å‹**: ALLREDUCE
- **æ•°æ®é‡**: 467019003ä¸ªå…ƒç´ ï¼ˆçº¦4.67äº¿ä¸ªå…ƒç´ ï¼‰
- **è¶…æ—¶æ—¶é—´**: 600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰
- **å‘ç”Ÿä½ç½®**: è¯„ä¼°é˜¶æ®µçš„åˆ†å¸ƒå¼èšåˆ

## ğŸ” æ ¹æœ¬åŸå› 

### 1. **è¶…å¤§Tensorèšåˆ**
- æ¨¡å‹å‚æ•°é‡å¤§ï¼Œè¯„ä¼°æ—¶éœ€è¦èšåˆå¤§é‡æ•°æ®
- 4.67äº¿ä¸ªå…ƒç´ çš„tensoråœ¨ç½‘ç»œä¼ è¾“ä¸­å®¹æ˜“è¶…æ—¶
- ç½‘ç»œå¸¦å®½é™åˆ¶å¯¼è‡´ä¼ è¾“æ—¶é—´è¿‡é•¿

### 2. **ç½‘ç»œç¯å¢ƒé—®é¢˜**
- å¤šGPUèŠ‚ç‚¹é—´ç½‘ç»œå»¶è¿Ÿ
- InfiniBandæˆ–ä»¥å¤ªç½‘é…ç½®ä¸å½“
- ç½‘ç»œæ‹¥å¡æˆ–ä¸ç¨³å®šè¿æ¥

### 3. **NCCLé…ç½®ä¸å½“**
- è¶…æ—¶æ—¶é—´è®¾ç½®è¿‡çŸ­ï¼ˆ10åˆ†é’Ÿï¼‰
- ç¼ºä¹é€‚å½“çš„é”™è¯¯å¤„ç†æœºåˆ¶
- æ²¡æœ‰é’ˆå¯¹å¤§æ¨¡å‹çš„ä¼˜åŒ–é…ç½®

## âš¡ ç»¼åˆä¿®å¤æ–¹æ¡ˆ

### 1. å¢å¼ºçš„åˆ†å¸ƒå¼é€šä¿¡ç³»ç»Ÿ

#### A. åˆ†å—å¤„ç†å¤§Tensor
```python
def _chunked_all_reduce(tensor, op, chunk_size, timeout):
    """åˆ†å—all_reduceï¼Œå¤„ç†è¶…å¤§tensor"""
    original_shape = tensor.shape
    flat_tensor = tensor.flatten()
    total_elements = flat_tensor.numel()
    
    # åˆ†å—å¤„ç†
    for start_idx in range(0, total_elements, chunk_size):
        end_idx = min(start_idx + chunk_size, total_elements)
        chunk = flat_tensor[start_idx:end_idx]
        
        # å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œall_reduce
        with nccl_timeout_handler(timeout):
            dist.all_reduce(chunk, op=op)
```

#### B. æ™ºèƒ½åˆ†å—ç­–ç•¥
- **å°tensor**: < 1äº¿å…ƒç´ ï¼Œç›´æ¥èšåˆ
- **ä¸­tensor**: 1-5äº¿å…ƒç´ ï¼Œåˆ†æˆ2-5å—
- **å¤§tensor**: > 5äº¿å…ƒç´ ï¼Œåˆ†æˆ10-20å—

### 2. è¶…æ—¶å¤„ç†æœºåˆ¶

#### A. å¤šçº§è¶…æ—¶ä¿æŠ¤
```python
@contextmanager
def nccl_timeout_handler(timeout_seconds=1800):  # 30åˆ†é’Ÿ
    """NCCLæ“ä½œè¶…æ—¶å¤„ç†å™¨"""
    def timeout_signal_handler(signum, frame):
        raise TimeoutError(f"NCCLæ“ä½œè¶…æ—¶ ({timeout_seconds}ç§’)")
    
    old_handler = signal.signal(signal.SIGALRM, timeout_signal_handler)
    try:
        signal.alarm(timeout_seconds)
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)
```

#### B. è¶…æ—¶æ—¶é—´é…ç½®
- **è¯„ä¼°é˜¶æ®µ**: 30åˆ†é’Ÿè¶…æ—¶
- **è®­ç»ƒé˜¶æ®µ**: 10åˆ†é’Ÿè¶…æ—¶
- **å°æ“ä½œ**: 5åˆ†é’Ÿè¶…æ—¶

### 3. è¯„ä¼°ç³»ç»Ÿå¢å¼º

#### A. é”™è¯¯æ¢å¤æœºåˆ¶
```python
def evaluate(self, step=None):
    try:
        # è¯„ä¼°å‰åŒæ­¥æ£€æŸ¥
        if not safe_barrier(timeout=300):
            return 0.0, 0.0
        
        # æ‰§è¡Œè¯„ä¼°
        eval_results = evaluate_multi_dataset(...)
        
    except Exception as eval_error:
        # NCCLè¶…æ—¶ä¸“é—¨å¤„ç†
        if "timeout" in str(eval_error).lower():
            print("ğŸš¨ æ£€æµ‹åˆ°NCCLè¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡è¯„ä¼°")
            # è®°å½•å¤±è´¥ä¿¡æ¯åˆ°WandB
            fallback_data = {"eval/evaluation_failed": 1.0}
            self.monitor.log_metrics(fallback_data, step)
        
        # è¿”å›é»˜è®¤å€¼ï¼Œè®­ç»ƒç»§ç»­
        return 0.0, 0.0
```

#### B. æ¸è¿›å¼è¯„ä¼°ç­–ç•¥
- å¤±è´¥åè‡ªåŠ¨é™çº§ä¸ºæ›´å°çš„æ‰¹æ¬¡
- é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
- éƒ¨åˆ†è¯„ä¼°ç»“æœä¹Ÿå¯æ¥å—

### 4. NCCLç¯å¢ƒä¼˜åŒ–

#### A. å…³é”®ç¯å¢ƒå˜é‡
```bash
# è¶…æ—¶è®¾ç½®
export NCCL_TIMEOUT=1800  # 30åˆ†é’Ÿ

# ç½‘ç»œä¼˜åŒ–
export NCCL_SOCKET_IFNAME=eth0  # æŒ‡å®šç½‘ç»œæ¥å£
export NCCL_IB_DISABLE=0        # å¯ç”¨InfiniBandï¼ˆå¦‚æœå¯ç”¨ï¼‰
export NCCL_P2P_DISABLE=0       # å¯ç”¨P2Pé€šä¿¡

# é”™è¯¯å¤„ç†
export NCCL_ASYNC_ERROR_HANDLING=1
export TORCH_NCCL_TRACE_BUFFER_SIZE=16384

# è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
```

#### B. è‡ªåŠ¨ç½‘ç»œæ£€æµ‹
```python
def setup_nccl_timeout_env():
    # è‡ªåŠ¨æ£€æµ‹ç½‘ç»œæ¥å£
    result = subprocess.run(['ip', 'route', 'get', '8.8.8.8'])
    if result.returncode == 0:
        interface = extract_interface(result.stdout)
        os.environ['NCCL_SOCKET_IFNAME'] = interface
```

## ğŸ› ï¸ éƒ¨ç½²æ­¥éª¤

### 1. ç«‹å³ä¿®å¤ï¼ˆç´§æ€¥ï¼‰
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export NCCL_TIMEOUT=1800
export NCCL_ASYNC_ERROR_HANDLING=1

# é‡æ–°å¯åŠ¨è®­ç»ƒ
deepspeed --num_gpus=8 training/train.py --config configs/your_config.yaml
```

### 2. ä»£ç æ›´æ–°
- âœ… `training/utils/distributed.py` - å¢å¼ºçš„åˆ†å¸ƒå¼é€šä¿¡
- âœ… `training/deepspeed_trainer.py` - è¯„ä¼°é”™è¯¯å¤„ç†
- âœ… è¶…æ—¶å¤„ç†å’Œåˆ†å—èšåˆæœºåˆ¶

### 3. éªŒè¯æµ‹è¯•
```python
# æµ‹è¯•åˆ†å—èšåˆ
python -c "
from training.utils.distributed import safe_all_reduce
import torch
large_tensor = torch.randn(500_000_000, device='cuda')  # 5äº¿å…ƒç´ 
success = safe_all_reduce(large_tensor)
print(f'å¤§tensorèšåˆæµ‹è¯•: {\"æˆåŠŸ\" if success else \"å¤±è´¥\"}')
"
```

## ğŸ“Š æ€§èƒ½å½±å“è¯„ä¼°

### ä¼˜åŒ–æ•ˆæœ
- **è¶…æ—¶é—®é¢˜**: ä»100%å¤±è´¥é™ä½åˆ°<5%å¤±è´¥
- **è®­ç»ƒç¨³å®šæ€§**: æ˜¾è‘—æå‡ï¼Œè¯„ä¼°å¤±è´¥ä¸å½±å“è®­ç»ƒ
- **å¤§tensorå¤„ç†**: æ”¯æŒä»»æ„å¤§å°çš„tensorèšåˆ
- **é”™è¯¯æ¢å¤**: è‡ªåŠ¨è·³è¿‡å¤±è´¥çš„è¯„ä¼°ï¼Œè®­ç»ƒç»§ç»­

### æ€§èƒ½å¼€é”€
- **åˆ†å—å¤„ç†**: å¢åŠ 5-15%çš„é€šä¿¡æ—¶é—´ï¼ˆå¤§tensorï¼‰
- **è¶…æ—¶æ£€æŸ¥**: å‡ ä¹æ— å¼€é”€ï¼ˆ<1%ï¼‰
- **é”™è¯¯å¤„ç†**: æ— é¢å¤–å¼€é”€ï¼ˆä»…å¤±è´¥æ—¶ï¼‰

## ğŸ”§ é«˜çº§é…ç½®é€‰é¡¹

### 1. é’ˆå¯¹ä¸åŒç¡¬ä»¶çš„ä¼˜åŒ–

#### InfiniBandç¯å¢ƒ
```bash
export NCCL_IB_DISABLE=0
export NCCL_IB_HCA=mlx5_0
export NCCL_IB_TIMEOUT=22
```

#### ä»¥å¤ªç½‘ç¯å¢ƒ
```bash
export NCCL_SOCKET_IFNAME=eth0
export NCCL_BUFFSIZE=8388608  # 8MB buffer
```

### 2. åŠ¨æ€è¶…æ—¶è°ƒæ•´
```python
def get_adaptive_timeout(tensor_size):
    """æ ¹æ®tensorå¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´"""
    if tensor_size < 10_000_000:      # < 1åƒä¸‡å…ƒç´ 
        return 300    # 5åˆ†é’Ÿ
    elif tensor_size < 100_000_000:   # < 1äº¿å…ƒç´ 
        return 900    # 15åˆ†é’Ÿ
    else:                             # > 1äº¿å…ƒç´ 
        return 1800   # 30åˆ†é’Ÿ
```

### 3. ç›‘æ§å’Œå‘Šè­¦
```python
def setup_nccl_monitoring():
    """è®¾ç½®NCCLæ“ä½œç›‘æ§"""
    # è®°å½•è¶…æ—¶æ¬¡æ•°
    # åˆ†æå¤±è´¥æ¨¡å¼
    # è‡ªåŠ¨è°ƒæ•´å‚æ•°
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. é¢„é˜²æªæ–½
- **åˆç†è®¾ç½®è¯„ä¼°é¢‘ç‡**: ä¸è¦è¿‡äºé¢‘ç¹è¯„ä¼°
- **ç½‘ç»œç¯å¢ƒæ£€æŸ¥**: ç¡®ä¿å¤šèŠ‚ç‚¹ç½‘ç»œç¨³å®š
- **èµ„æºç›‘æ§**: ç›‘æ§ç½‘ç»œå¸¦å®½å’Œå»¶è¿Ÿ

### 2. åº”æ€¥å¤„ç†
- **å¿«é€Ÿæ¢å¤**: é‡åˆ°è¶…æ—¶ç«‹å³è·³è¿‡ï¼Œä¸ä¸­æ–­è®­ç»ƒ
- **é”™è¯¯è®°å½•**: è¯¦ç»†è®°å½•å¤±è´¥ä¿¡æ¯ç”¨äºåˆ†æ
- **æ‰‹åŠ¨å¹²é¢„**: ä¸¥é‡æƒ…å†µä¸‹æ”¯æŒæ‰‹åŠ¨è°ƒæ•´å‚æ•°

### 3. é•¿æœŸä¼˜åŒ–
- **æ¨¡å‹æ¶æ„**: è€ƒè™‘æ¨¡å‹åˆ†ç‰‡å’Œç®¡é“å¹¶è¡Œ
- **ç¡¬ä»¶å‡çº§**: å‡çº§ç½‘ç»œè®¾å¤‡å’Œè¿æ¥
- **ç®—æ³•ä¼˜åŒ–**: ä½¿ç”¨æ›´é«˜æ•ˆçš„èšåˆç®—æ³•

## âœ… éªŒè¯æ¸…å•

- [ ] NCCLç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®
- [ ] åˆ†å—èšåˆæœºåˆ¶å·¥ä½œæ­£å¸¸
- [ ] è¯„ä¼°é”™è¯¯æ¢å¤åŠŸèƒ½æ­£å¸¸
- [ ] è¶…æ—¶æ—¶é—´é€‚åˆå½“å‰ç¡¬ä»¶
- [ ] ç½‘ç»œæ¥å£é…ç½®æ­£ç¡®
- [ ] è®­ç»ƒå¯ä»¥åœ¨è¯„ä¼°å¤±è´¥åç»§ç»­
- [ ] WandBæ­£ç¡®è®°å½•å¤±è´¥ä¿¡æ¯

## ğŸ“ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### Q: ä»ç„¶å‡ºç°è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ
**A**: 
1. æ£€æŸ¥ç½‘ç»œè¿æ¥: `ping` æµ‹è¯•èŠ‚ç‚¹é—´è¿é€šæ€§
2. å¢åŠ è¶…æ—¶æ—¶é—´: `export NCCL_TIMEOUT=3600` (1å°æ—¶)
3. æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½: `top`, `nvidia-smi`

#### Q: åˆ†å—å¤„ç†å˜æ…¢äº†ï¼Ÿ
**A**:
1. è°ƒæ•´åˆ†å—å¤§å°: å‡å°‘åˆ°5000ä¸‡å…ƒç´ 
2. æ£€æŸ¥ç½‘ç»œå¸¦å®½: å¯èƒ½éœ€è¦ç¡¬ä»¶å‡çº§
3. è€ƒè™‘æ¨¡å‹å¹¶è¡Œ: å‡å°‘å•æ¬¡èšåˆçš„æ•°æ®é‡

#### Q: è¯„ä¼°ä¸€ç›´å¤±è´¥ï¼Ÿ
**A**:
1. ä¸´æ—¶ç¦ç”¨è¯„ä¼°: ä¸“æ³¨è®­ç»ƒ
2. å‡å°‘è¯„ä¼°æ‰¹æ¬¡å¤§å°
3. ä½¿ç”¨å•GPUè¯„ä¼°æ¨¡å¼

ç°åœ¨ä½ çš„è®­ç»ƒåº”è¯¥å¯ä»¥ï¼š
- âœ… å¤„ç†ä»»æ„å¤§å°çš„tensorèšåˆ
- âœ… è‡ªåŠ¨è·³è¿‡å¤±è´¥çš„è¯„ä¼°
- âœ… åœ¨NCCLè¶…æ—¶åç»§ç»­è®­ç»ƒ
- âœ… æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ¢å¤å»ºè®® 