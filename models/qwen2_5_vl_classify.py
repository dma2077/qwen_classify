import torch
import torch.nn as nn
from transformers import (
    Qwen2_5_VLPreTrainedModel,
    Qwen2_5_VLModel,
    AutoConfig,
    AutoProcessor,
)
from transformers.modeling_outputs import SequenceClassifierOutput

from transformers import Qwen2_5_VLForConditionalGeneration

class Qwen2_5_VLForImageClassification(Qwen2_5_VLPreTrainedModel):
    """
    Image classification model that runs the full multimodal -> language pipeline
    and applies a classification head on the final token representations.

    This class loads pretrained weights from a Qwen2.5-VL-ForConditionalGeneration checkpoint.
    """
    def __init__(self,
                 pretrained_model_name: str,
                 num_labels: int,
                 loss_config: dict = None):
        config = AutoConfig.from_pretrained(pretrained_model_name)
        config.num_labels = num_labels
        super().__init__(config)

        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            pretrained_model_name,
            config=config,
            ignore_mismatched_sizes=True,
        )
        self.model = base_model.model
        self.processor = AutoProcessor.from_pretrained(pretrained_model_name)
        text_cfg = config.get_text_config()
        hidden_size = text_cfg.hidden_size
        self.classifier = nn.Linear(hidden_size, num_labels)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self.loss_config = loss_config or {'type': 'cross_entropy'}
        self.loss_function = self._create_loss_function()
        
        self.post_init()
        
    def _create_loss_function(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        from training.losses import create_loss_function
        
        loss_type = self.loss_config.get('type', 'cross_entropy')
        
        # åˆ›å»ºæŸå¤±å‡½æ•°å‚æ•°çš„å‰¯æœ¬ï¼Œæ’é™¤'type'é”®
        loss_kwargs = {k: v for k, v in self.loss_config.items() if k != 'type'}
        
        # ä¸ºArcFaceæŸå¤±ä¼ å…¥æ­£ç¡®çš„ç‰¹å¾ç»´åº¦
        if loss_type == 'arcface':
            text_cfg = self.config.get_text_config()
            hidden_size = text_cfg.hidden_size
            loss_kwargs.update({
                'in_features': hidden_size,
                'out_features': self.config.num_labels
            })
            
        print(f"ğŸ“‹ åˆ›å»ºæŸå¤±å‡½æ•°: {loss_type}")
        print(f"ğŸ“‹ æŸå¤±å‡½æ•°å‚æ•°: {loss_kwargs}")
        
        return create_loss_function(loss_type, **loss_kwargs)

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: torch.LongTensor,
        attention_mask: torch.LongTensor,
        labels: torch.LongTensor = None,
        **kwargs,
    ) -> SequenceClassifierOutput:
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            return_dict=True,
            **kwargs,
        )
        hidden_states = outputs.last_hidden_state

        # ä½¿ç”¨attention_maskæ¥è·å–æ­£ç¡®çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokens + text tokensï¼‰
        if attention_mask is not None:
            # attention_maskè¦†ç›–å®Œæ•´åºåˆ—ï¼ˆvisual + text tokensï¼‰
            valid_lengths = attention_mask.sum(dim=1)  # æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦
            last_positions = valid_lengths - 1  # æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            last_positions = torch.clamp(last_positions, min=0, max=hidden_states.size(1)-1)
            pooled = hidden_states[torch.arange(hidden_states.size(0)), last_positions]
        else:
            # å¦‚æœæ²¡æœ‰attention_maskï¼Œä½¿ç”¨åºåˆ—çš„æœ€åä½ç½®
            pooled = hidden_states[:, -1, :]

        # è®¡ç®—logits
        logits = self.classifier(pooled)
        
        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            loss_type = self.loss_config.get('type', 'cross_entropy')
            
            if loss_type == 'arcface':
                # ArcFaceæŸå¤±éœ€è¦åŸå§‹ç‰¹å¾å’Œæ ‡ç­¾
                loss = self.loss_function(pooled, labels)
            elif loss_type == 'supcon':
                # SupConæŸå¤±éœ€è¦ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆéœ€è¦ç‰¹æ®Šçš„æ•°æ®æ ¼å¼ï¼‰
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å‡†å¤‡å¯¹æ¯”å­¦ä¹ çš„æ•°æ®æ ¼å¼
                features = pooled.unsqueeze(1)  # [batch_size, 1, hidden_size]
                loss = self.loss_function(features, labels)
            else:
                # æ ‡å‡†æŸå¤±å‡½æ•°ä½¿ç”¨logitså’Œæ ‡ç­¾
                loss = self.loss_function(logits, labels)
                
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )