import torch
import torch.nn as nn
from transformers import (
    Qwen2_5_VLPreTrainedModel,
    Qwen2_5_VLModel,
    AutoConfig,
    AutoProcessor,
)
from transformers.modeling_outputs import SequenceClassifierOutput

from transformers import Qwen2_5_VLForConditionalGeneration

class Qwen2_5_VLForImageClassification(Qwen2_5_VLPreTrainedModel):
    """
    Image classification model that runs the full multimodal -> language pipeline
    and applies a classification head on the final token representations.

    This class loads pretrained weights from a Qwen2.5-VL-ForConditionalGeneration checkpoint.
    Supports multi-dataset functionality with logits masking.
    """
    def __init__(self,  
                 pretrained_model_name: str,
                 num_labels: int,
                 loss_config: dict = None,
                 dataset_configs: dict = None,
                 enable_logits_masking: bool = True):
        config = AutoConfig.from_pretrained(pretrained_model_name)
        config.num_labels = num_labels
        
        # ç¡®ä¿é…ç½®ä¸ä¼šè§¦å‘ForSequenceClassificationLoss
        config.problem_type = None  # é‡ç½®problem_type
        if hasattr(config, 'use_cache'):
            config.use_cache = False
        
        super().__init__(config)

        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            pretrained_model_name,
            config=config,
            ignore_mismatched_sizes=True,
            attn_implementation=config._attn_implementation,
            torch_dtype=torch.bfloat16
        )
        self.model = base_model.model
        self.processor = AutoProcessor.from_pretrained(pretrained_model_name)
        text_cfg = config.get_text_config()
        hidden_size = text_cfg.hidden_size
        self.classifier = nn.Linear(hidden_size, num_labels)
        
        # è®¾ç½®æŸå¤±å‡½æ•°é…ç½® - ä¸åˆ›å»ºloss_functionå¯¹è±¡ï¼Œé¿å…ç»§æ‰¿é—®é¢˜
        self.loss_config = loss_config or {'type': 'cross_entropy'}
        
        # å¤šæ•°æ®é›†é…ç½®
        self.dataset_configs = dataset_configs or {}
        self.enable_logits_masking = enable_logits_masking
        
        self.post_init()

    def _apply_logits_masking(self, logits, dataset_names=None, num_classes_list=None):
        """
        æ ¹æ®æ•°æ®é›†çš„ç±»åˆ«æ•°é‡å¯¹logitsè¿›è¡Œmaskingï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰
        
        Args:
            logits: [batch_size, num_labels] çš„logits tensor
            dataset_names: æ¯ä¸ªæ ·æœ¬çš„æ•°æ®é›†åç§°åˆ—è¡¨
            num_classes_list: æ¯ä¸ªæ ·æœ¬å¯¹åº”æ•°æ®é›†çš„ç±»åˆ«æ•°é‡åˆ—è¡¨
        
        Returns:
            masked_logits: maskingåçš„logits tensor
        """
        if not self.enable_logits_masking:
            return logits
            
        if dataset_names is None and num_classes_list is None:
            return logits
            
        # ğŸ”¥ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šè£å‰ªè¿‡å¤§çš„logitså€¼
        logits = torch.clamp(logits, min=-50.0, max=50.0)
        
        masked_logits = logits.clone()
        batch_size = logits.size(0)
        
        for i in range(batch_size):
            num_classes = None
            
            # ä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„num_classes
            if num_classes_list is not None and i < len(num_classes_list):
                num_classes = num_classes_list[i]
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥æä¾›ï¼Œä»dataset_configsä¸­è·å–
            if num_classes is None and dataset_names is not None and i < len(dataset_names):
                dataset_name = dataset_names[i]
                dataset_config = self.dataset_configs.get(dataset_name, {})
                num_classes = dataset_config.get("num_classes", None)
            
            # åº”ç”¨masking - ä½¿ç”¨æ›´å®‰å…¨çš„maskingå€¼
            if num_classes is not None and num_classes < logits.size(-1):
                # ğŸ”¥ ä½¿ç”¨-1e9è€Œä¸æ˜¯-infï¼Œé¿å…æ•°å€¼é—®é¢˜
                mask_value = -1e9
                masked_logits[i, num_classes:] = mask_value
                
                # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰æ•ˆä½ç½®ä¸å…¨ä¸ºæå°å€¼
                valid_logits = masked_logits[i, :num_classes]
                if torch.all(valid_logits < -10.0):
                    # å¦‚æœæœ‰æ•ˆä½ç½®çš„logitséƒ½å¤ªå°ï¼Œè¿›è¡Œè°ƒæ•´
                    masked_logits[i, :num_classes] = torch.clamp(valid_logits, min=-10.0)
        
        return masked_logits

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: torch.LongTensor,
        attention_mask: torch.LongTensor,
        labels: torch.LongTensor = None,
        dataset_names: list = None,
        num_classes_list: list = None,
        **kwargs,
    ) -> SequenceClassifierOutput:
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            return_dict=True,
            **kwargs,
        )
        hidden_states = outputs.last_hidden_state

        # ä½¿ç”¨attention_maskæ¥è·å–æ­£ç¡®çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokens + text tokensï¼‰
        if attention_mask is not None:
            # attention_maskè¦†ç›–å®Œæ•´åºåˆ—ï¼ˆvisual + text tokensï¼‰
            valid_lengths = attention_mask.sum(dim=1)  # æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦
            last_positions = valid_lengths - 1  # æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            last_positions = torch.clamp(last_positions, min=0, max=hidden_states.size(1)-1)
            pooled = hidden_states[torch.arange(hidden_states.size(0)), last_positions]
        else:
            # å¦‚æœæ²¡æœ‰attention_maskï¼Œä½¿ç”¨åºåˆ—çš„æœ€åä½ç½®
            pooled = hidden_states[:, -1, :]

        # è®¡ç®—logits
        logits = self.classifier(pooled)
        
        # åº”ç”¨logits masking
        if self.enable_logits_masking:
            logits = self._apply_logits_masking(logits, dataset_names, num_classes_list)
        
        # è®¡ç®—æŸå¤± - å¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        loss = None
        if labels is not None:
            try:
                # ğŸ”¥ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šæ ‡ç­¾è¾¹ç•Œæ£€æŸ¥
                max_label = labels.max().item()
                if max_label >= logits.size(-1):
                    print(f"âš ï¸ å‘ç°è¶Šç•Œæ ‡ç­¾: max_label={max_label}, logits_classes={logits.size(-1)}")
                    # è£å‰ªè¶Šç•Œçš„æ ‡ç­¾
                    labels = torch.clamp(labels, min=0, max=logits.size(-1)-1)
                
                # ğŸ”¥ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šlogitså€¼æ£€æŸ¥
                if torch.any(torch.isnan(logits)) or torch.any(torch.isinf(logits)):
                    print(f"âš ï¸ logitsåŒ…å«NaNæˆ–Infï¼Œè¿›è¡Œæ¸…ç†")
                    logits = torch.nan_to_num(logits, nan=0.0, posinf=1e9, neginf=-1e9)
                
                # ä¸ä½¿ç”¨self.loss_functionï¼Œç›´æ¥åœ¨è¿™é‡Œåˆ›å»ºæŸå¤±å‡½æ•°
                loss_type = self.loss_config.get('type', 'cross_entropy')
                
                if loss_type == 'label_smoothing':
                    smoothing = self.loss_config.get('smoothing', 0.1)
                    temperature = self.loss_config.get('temperature', 1.0)
                    
                    # å†…è”Label Smoothingå®ç°
                    import torch.nn.functional as F
                    log_probs = F.log_softmax(logits / temperature, dim=-1)
                    targets_one_hot = torch.zeros_like(log_probs)
                    targets_one_hot.scatter_(1, labels.unsqueeze(1), 1)
                    
                    # Apply label smoothing
                    targets_smooth = (1 - smoothing) * targets_one_hot + smoothing / logits.size(-1)
                    loss = -torch.sum(targets_smooth * log_probs, dim=-1).mean()
                        
                elif loss_type == 'focal':
                    alpha = self.loss_config.get('alpha', 1.0)
                    gamma = self.loss_config.get('gamma', 2.0)
                    
                    # å†…è”Focal Losså®ç°
                    import torch.nn.functional as F
                    ce_loss = F.cross_entropy(logits, labels, reduction='none')
                    pt = torch.exp(-ce_loss)
                    loss = (alpha * (1 - pt) ** gamma * ce_loss).mean()
                        
                else:
                    # æ ‡å‡†CrossEntropyLoss
                    import torch.nn.functional as F
                    loss = F.cross_entropy(logits, labels)
                
                # ğŸ”¥ æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âŒ æŸå¤±è®¡ç®—ç»“æœä¸ºNaNæˆ–Inf: {loss}")
                    # ä½¿ç”¨ä¸€ä¸ªå°çš„å›ºå®šæŸå¤±å€¼ï¼Œé¿å…è®­ç»ƒå´©æºƒ
                    loss = torch.tensor(1.0, device=logits.device, requires_grad=True)
                    print(f"ğŸ”§ ä½¿ç”¨å›é€€æŸå¤±å€¼: {loss}")
                    
            except Exception as e:
                print(f"âŒ æŸå¤±è®¡ç®—è¿‡ç¨‹å‡ºé”™: {e}")
                # é™é»˜å›é€€åˆ°æ ‡å‡†æŸå¤±å‡½æ•°
                import torch.nn.functional as F
                try:
                    loss = F.cross_entropy(logits, labels)
                    if torch.isnan(loss) or torch.isinf(loss):
                        loss = torch.tensor(1.0, device=logits.device, requires_grad=True)
                except:
                    loss = torch.tensor(1.0, device=logits.device, requires_grad=True)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ— è®ºè®­ç»ƒè¿˜æ˜¯è¯„ä¼°éƒ½ä¸è¿”å›å¤§tensorï¼Œé¿å…NCCLè¶…æ—¶
        # ç»è¿‡ä»£ç åˆ†æç¡®è®¤ï¼šè®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­éƒ½ä¸éœ€è¦hidden_stateså’Œattentions
        # åªéœ€è¦losså’Œlogitsè¿›è¡Œåå‘ä¼ æ’­å’Œé¢„æµ‹è®¡ç®—
        
        # ç»Ÿä¸€è¿”å›ç®€åŒ–è¾“å‡º - å¤§å¹…èŠ‚çœå†…å­˜å’Œé€šä¿¡å¸¦å®½
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=None,  # âœ… è®­ç»ƒå’Œè¯„ä¼°éƒ½ä¸éœ€è¦ï¼Œé¿å…4.67äº¿å…ƒç´ çš„NCCL reduce
            attentions=None,     # âœ… è®­ç»ƒå’Œè¯„ä¼°éƒ½ä¸éœ€è¦ï¼ŒèŠ‚çœå†…å­˜å’Œé€šä¿¡å¸¦å®½
        )