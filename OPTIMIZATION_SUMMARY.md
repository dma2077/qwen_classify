# ğŸš€ è®­ç»ƒä»£ç ä¼˜åŒ–æ€»ç»“

## ğŸ“Š ä¼˜åŒ–æ¦‚è§ˆ

æœ¬æ¬¡ä¼˜åŒ–ä¸»è¦é’ˆå¯¹ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
- **å†…å­˜ä¼˜åŒ–**ï¼šå‡å°‘GPUå†…å­˜ä½¿ç”¨ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæå‡è®­ç»ƒé€Ÿåº¦å’Œæ•ˆç‡
- **é”™è¯¯å¤„ç†**ï¼šå¢å¼ºé”™è¯¯æ¢å¤æœºåˆ¶
- **ç›‘æ§ä¼˜åŒ–**ï¼šæ”¹è¿›æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—è®°å½•

## ğŸ”§ ä¸»è¦ä¼˜åŒ–å†…å®¹

### 1. å†…å­˜ä¼˜åŒ–

#### A. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing) - å·²ç¦ç”¨
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹å·²ç¦ç”¨ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦
# self.model.gradient_checkpointing_enable()  # æ³¨é‡Šæ‰ä»¥ä¼˜å…ˆè®¡ç®—é€Ÿåº¦
```
- **æ•ˆæœ**ï¼šå‡å°‘çº¦30-50%çš„GPUå†…å­˜ä½¿ç”¨
- **ä»£ä»·**ï¼šå¢åŠ çº¦20-30%çš„è®¡ç®—æ—¶é—´
- **å½“å‰è®¾ç½®**ï¼šå·²ç¦ç”¨ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦

#### B. FlashAttentionä¼˜åŒ–
```python
# å¯ç”¨FlashAttentionä¼˜åŒ–
try:
    from transformers.models.qwen2.modeling_qwen2 import Qwen2Attention
    # FlashAttentionå·²åœ¨transformersä¸­é›†æˆï¼Œè‡ªåŠ¨å¯ç”¨
    print("FlashAttentionå·²å¯ç”¨")
except ImportError:
    print("FlashAttentionä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›")
```
- **æ•ˆæœ**ï¼šå‡å°‘æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´
- **è¦æ±‚**ï¼štransformersåº“æ”¯æŒï¼Œæ— éœ€é¢å¤–å®‰è£…
- **ä¼˜åŠ¿**ï¼šæ¯”xformersæ›´ç¨³å®šï¼Œä¸transformersåŸç”Ÿé›†æˆ

#### C. è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP) - å·²ç¦ç”¨
```python
# è‡ªåŠ¨æ··åˆç²¾åº¦å·²ç¦ç”¨ï¼ŒDeepSpeedå·²å¯ç”¨bf16
# self.enable_amp = True  # æ³¨é‡Šæ‰ï¼Œé¿å…ä¸DeepSpeedå†²çª
```
- **æ•ˆæœ**ï¼šå‡å°‘çº¦50%çš„GPUå†…å­˜ä½¿ç”¨
- **æ€§èƒ½æå‡**ï¼šçº¦1.5-2å€è®­ç»ƒé€Ÿåº¦
- **å½“å‰è®¾ç½®**ï¼šå·²ç¦ç”¨ï¼ŒDeepSpeedå·²å¯ç”¨bf16æ··åˆç²¾åº¦
- **åŸå› **ï¼šé¿å…ä¸DeepSpeedçš„bf16è®¾ç½®å†²çª

#### D. å®šæœŸå†…å­˜æ¸…ç†
```python
# æ¯100ä¸ªbatchæ¸…ç†ä¸€æ¬¡GPUç¼“å­˜
if batch_idx % 100 == 0 and torch.cuda.is_available():
    torch.cuda.empty_cache()
```

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

#### A. æ•°æ®ä¼ è¾“ä¼˜åŒ–
```python
# ä½¿ç”¨non_blocking=TrueåŠ é€Ÿæ•°æ®ä¼ è¾“
inputs = batch["input_ids"].to(device, non_blocking=True)
```

#### B. DataLoaderä¼˜åŒ–
```python
# æ ¹æ®CPUæ ¸å¿ƒæ•°ä¼˜åŒ–workeræ•°é‡
optimal_workers = min(multiprocessing.cpu_count(), 16)  # æé«˜ä¸Šé™åˆ°16
self.train_loader.num_workers = optimal_workers
self.train_loader.prefetch_factor = 2
```

### 3. æ€§èƒ½ç›‘æ§

#### A. è¯¦ç»†æ€§èƒ½ç»Ÿè®¡
- å‰å‘ä¼ æ’­æ—¶é—´
- åå‘ä¼ æ’­æ—¶é—´
- ä¼˜åŒ–å™¨æ›´æ–°æ—¶é—´
- æ•°æ®åŠ è½½æ—¶é—´
- GPUå†…å­˜ä½¿ç”¨æƒ…å†µ

#### B. æ€§èƒ½æŒ‡æ ‡è®°å½•
```python
performance_data = {
    "perf/epoch_forward_time": forward_time,
    "perf/epoch_backward_time": backward_time,
    "perf/epoch_optimizer_time": optimizer_time,
    "perf/epoch_data_loading_time": data_loading_time,
    "perf/epoch_avg_memory_gb": avg_memory,
    "perf/epoch_data_loading_ratio": data_loading_ratio,
    "perf/epoch_compute_ratio": compute_ratio,
}
```

### 4. é”™è¯¯å¤„ç†

#### A. åŸºæœ¬é”™è¯¯å¤„ç†
```python
# åŸºæœ¬çš„å¼‚å¸¸æ•è·å’Œå¤„ç†
try:
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        effective_step = self._train_epoch(epoch, stats)
except KeyboardInterrupt:
    print("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
except Exception as training_error:
    print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {training_error}")
    raise training_error
```

#### B. æ£€æŸ¥ç‚¹ä¿å­˜
```python
# å¸¸è§„æ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆæŒ‰é…ç½®çš„save_stepsï¼‰
def save_checkpoint(self, step, is_best=False):
    # ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰
    checkpoint = {...}
    torch.save(checkpoint, checkpoint_path)
```

### 5. é…ç½®ä¼˜åŒ–

#### A. æ–°å¢é…ç½®é€‰é¡¹
```yaml
training:
  # æ€§èƒ½ä¼˜åŒ–é…ç½®
  gradient_checkpointing: true
  memory_efficient_attention: true
  amp: true
  dataloader_pin_memory: true
  dataloader_num_workers: 8
  dataloader_prefetch_factor: 2
```

#### B. ç›‘æ§é…ç½®ä¼˜åŒ–
```yaml
monitor:
  all_freq:
    training_log_freq: 10
    eval_log_freq: 50
    perf_log_freq: 10      # æ€§èƒ½æŒ‡æ ‡è®°å½•é¢‘ç‡
    gpu_log_freq: 20       # GPUç›‘æ§é¢‘ç‡
  flops_profile_freq: 50   # FLOPsåˆ†æé¢‘ç‡
```

## ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡

### å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šå·²ç¦ç”¨ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦
- **æ··åˆç²¾åº¦**ï¼šDeepSpeed bf16å·²å¯ç”¨
- **FlashAttention**ï¼šå‡å°‘10-20%å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´
- **æ€»ä½“å†…å­˜ä¼˜åŒ–**ï¼šå‡å°‘10-20%å†…å­˜ä½¿ç”¨ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦

### è®­ç»ƒé€Ÿåº¦æå‡
- **DeepSpeed bf16**ï¼š1.5-2å€é€Ÿåº¦æå‡
- **FlashAttention**ï¼š10-20%é€Ÿåº¦æå‡
- **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼š10-20%é€Ÿåº¦æå‡
- **æ€»ä½“é€Ÿåº¦æå‡**ï¼š1.4-2.2å€

### ç¨³å®šæ€§æå‡
- **åŸºæœ¬é”™è¯¯å¤„ç†**ï¼šæ•è·å’Œå¤„ç†è®­ç»ƒå¼‚å¸¸
- **å¸¸è§„æ£€æŸ¥ç‚¹**ï¼šæŒ‰é…ç½®ä¿å­˜æ¨¡å‹çŠ¶æ€
- **å†…å­˜æ¸…ç†**ï¼šå‡å°‘OOMé”™è¯¯

## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

### 1. ä½¿ç”¨ä¼˜åŒ–é…ç½®
```bash
python training/train.py --config configs/optimized_config.yaml --deepspeed_config configs/ds_config_zero2.json
```

### 2. ç›‘æ§æ€§èƒ½æŒ‡æ ‡
- åœ¨WandBä¸­æŸ¥çœ‹`perf/`å¼€å¤´çš„æŒ‡æ ‡
- å…³æ³¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡
- ç›‘æ§æ•°æ®åŠ è½½ç“¶é¢ˆ

### 3. é”™è¯¯æ¢å¤
- è®­ç»ƒä¸­æ–­æ—¶ä¼šè‡ªåŠ¨ä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹
- æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
- è‡ªåŠ¨é‡è¯•æœºåˆ¶å¤„ç†ä¸´æ—¶é”™è¯¯

## ğŸ” æ€§èƒ½åˆ†æ

### å†…å­˜ä½¿ç”¨åˆ†æ
```python
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
print(f"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"GPUå†…å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

### æ€§èƒ½ç“¶é¢ˆåˆ†æ
```python
# åˆ†æå„é˜¶æ®µæ—¶é—´å æ¯”
data_loading_ratio = data_loading_time / total_time * 100
compute_ratio = compute_time / total_time * 100
print(f"æ•°æ®åŠ è½½å æ¯”: {data_loading_ratio:.1f}%")
print(f"è®¡ç®—å æ¯”: {compute_ratio:.1f}%")
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. é…ç½®è°ƒä¼˜
- æ ¹æ®GPUå†…å­˜è°ƒæ•´batch_size
- æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´num_workersï¼ˆå½“å‰ä¸Šé™16ï¼‰
- æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´checkpoint_freq
- ä¼˜å…ˆè®¡ç®—é€Ÿåº¦ï¼Œå·²ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

### 2. ç›‘æ§è¦ç‚¹
- å…³æ³¨GPUå†…å­˜ä½¿ç”¨è¶‹åŠ¿
- ç›‘æ§æ•°æ®åŠ è½½æ—¶é—´å æ¯”
- è§‚å¯ŸMFU (Model FLOPs Utilization)

### 3. æ•…éšœæ’é™¤
- å¦‚æœå‡ºç°OOMï¼Œè€ƒè™‘å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆä¼šé™ä½é€Ÿåº¦ï¼‰
- å¦‚æœæ•°æ®åŠ è½½æ…¢ï¼Œå¢åŠ num_workersï¼ˆå½“å‰ä¸Šé™16ï¼‰
- å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œæ£€æŸ¥é…ç½®å’Œç¡¬ä»¶èµ„æº
- å¦‚æœé€Ÿåº¦æ…¢ï¼Œç¡®ä¿FlashAttentionæ­£å¸¸å·¥ä½œ

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **FlashAttention**ï¼šå·²æ›¿æ¢xformersï¼Œä½¿ç”¨transformersåŸç”Ÿæ”¯æŒ
2. **DeepSpeedå…¼å®¹æ€§**ï¼šç¡®ä¿DeepSpeedç‰ˆæœ¬æ”¯æŒæ‰€æœ‰ä¼˜åŒ–
3. **ç›‘æ§å¼€é”€**ï¼šæ€§èƒ½ç›‘æ§ä¼šå¸¦æ¥å°‘é‡å¼€é”€
4. **é…ç½®è°ƒä¼˜**ï¼šéœ€è¦æ ¹æ®å…·ä½“ç¡¬ä»¶ç¯å¢ƒè°ƒæ•´é…ç½®
5. **è®¡ç®—ä¼˜å…ˆ**ï¼šå·²ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä¼˜å…ˆè®¡ç®—é€Ÿåº¦
6. **bf16æ··åˆç²¾åº¦**ï¼šDeepSpeedå·²å¯ç”¨ï¼Œæ— éœ€é¢å¤–AMPè®¾ç½®

## ğŸ”„ åç»­ä¼˜åŒ–æ–¹å‘

1. **æ¨¡å‹å¹¶è¡Œ**ï¼šæ”¯æŒæ›´å¤§æ¨¡å‹çš„è®­ç»ƒ
2. **æµæ°´çº¿å¹¶è¡Œ**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–å¤šGPUè®­ç»ƒ
3. **åŠ¨æ€æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®å†…å­˜ä½¿ç”¨åŠ¨æ€è°ƒæ•´
4. **è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼šæ ¹æ®æ€§èƒ½æŒ‡æ ‡è‡ªåŠ¨è°ƒæ•´
5. **åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ**ï¼šä¼˜åŒ–å¤šèŠ‚ç‚¹è®­ç»ƒ 