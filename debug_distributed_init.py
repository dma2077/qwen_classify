#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼åˆå§‹åŒ–è°ƒè¯•è„šæœ¬
"""

import os
import sys
import torch
import deepspeed

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒå˜é‡:")
    env_vars = [
        'WORLD_SIZE', 'RANK', 'LOCAL_RANK', 
        'MASTER_ADDR', 'MASTER_PORT',
        'CUDA_VISIBLE_DEVICES', 'NCCL_DEBUG'
    ]
    
    for var in env_vars:
        value = os.environ.get(var, 'Not Set')
        print(f"  â€¢ {var}: {value}")

def check_cuda():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥CUDAç¯å¢ƒ:")
    print(f"  â€¢ CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  â€¢ CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"  â€¢ å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
        for i in range(torch.cuda.device_count()):
            print(f"  â€¢ GPU {i}: {torch.cuda.get_device_name(i)}")

def test_distributed_init():
    """æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–"""
    print("\nğŸ” æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–:")
    
    try:
        # è®¾ç½®åŸºæœ¬ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½®ï¼‰
        if 'MASTER_ADDR' not in os.environ:
            os.environ['MASTER_ADDR'] = 'localhost'
        if 'MASTER_PORT' not in os.environ:
            os.environ['MASTER_PORT'] = '29501'
        
        print("  â€¢ å°è¯•è°ƒç”¨ deepspeed.init_distributed()...")
        deepspeed.init_distributed()
        print("  âœ… deepspeed.init_distributed() æˆåŠŸ")
        
        # æ£€æŸ¥åˆ†å¸ƒå¼çŠ¶æ€
        import torch.distributed as dist
        if dist.is_available():
            print(f"  â€¢ torch.distributed å¯ç”¨: True")
            if dist.is_initialized():
                print(f"  âœ… åˆ†å¸ƒå¼å·²åˆå§‹åŒ–")
                print(f"  â€¢ World Size: {dist.get_world_size()}")
                print(f"  â€¢ Rank: {dist.get_rank()}")
                print(f"  â€¢ Backend: {dist.get_backend()}")
            else:
                print(f"  âŒ åˆ†å¸ƒå¼æœªåˆå§‹åŒ–")
        else:
            print(f"  âŒ torch.distributed ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"  âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_dataloader_batch_calculation():
    """æµ‹è¯•DataLoader batch sizeè®¡ç®—"""
    print("\nğŸ” æµ‹è¯•batch sizeè®¡ç®—:")
    
    # æ¨¡æ‹ŸDeepSpeedé…ç½®
    deepspeed_config = {
        "train_batch_size": 256,
        "train_micro_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 4
    }
    
    micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
    total_batch_size = deepspeed_config.get('train_batch_size', 1)
    gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
    
    print(f"  â€¢ micro_batch_size_per_gpu: {micro_batch_size_per_gpu}")
    print(f"  â€¢ total_batch_size: {total_batch_size}")
    print(f"  â€¢ gradient_accumulation_steps: {gradient_accumulation_steps}")
    
    # æµ‹è¯•åˆ†å¸ƒå¼çŠ¶æ€
    import torch.distributed as dist
    if dist.is_available() and dist.is_initialized():
        num_gpus = dist.get_world_size()
        print(f"  âœ… ä»åˆ†å¸ƒå¼ç¯å¢ƒè·å–GPUæ•°é‡: {num_gpus}")
    else:
        # Fallbackè®¡ç®—
        calculated_num_gpus = total_batch_size // (micro_batch_size_per_gpu * gradient_accumulation_steps)
        num_gpus = max(1, calculated_num_gpus)
        print(f"  ğŸ”§ ä»DeepSpeedé…ç½®è®¡ç®—GPUæ•°é‡: {num_gpus}")
        print(f"     è®¡ç®—å…¬å¼: {total_batch_size} / ({micro_batch_size_per_gpu} Ã— {gradient_accumulation_steps}) = {num_gpus}")
    
    # è®¡ç®—batch sizes
    train_batch_size = micro_batch_size_per_gpu
    eval_batch_size = micro_batch_size_per_gpu * num_gpus
    
    print(f"  â€¢ Training DataLoader batch_size: {train_batch_size}")
    print(f"  â€¢ Evaluation DataLoader batch_size: {eval_batch_size}")
    
    # éªŒè¯è®¡ç®—
    expected_total = micro_batch_size_per_gpu * num_gpus * gradient_accumulation_steps
    print(f"  â€¢ éªŒè¯æ€»batch size: {expected_total} (æœŸæœ›: {total_batch_size})")
    if expected_total == total_batch_size:
        print(f"  âœ… batch sizeè®¡ç®—æ­£ç¡®")
    else:
        print(f"  âŒ batch sizeè®¡ç®—ä¸ä¸€è‡´")

if __name__ == "__main__":
    print("ğŸš€ åˆ†å¸ƒå¼åˆå§‹åŒ–è°ƒè¯•")
    print("=" * 60)
    
    check_environment()
    check_cuda()
    test_distributed_init()
    test_dataloader_batch_calculation()
    
    print("\n" + "=" * 60)
    print("ğŸ” è°ƒè¯•å®Œæˆï¼è¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºä»¥è¯Šæ–­é—®é¢˜") 