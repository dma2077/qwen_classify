#!/usr/bin/env python3
"""
æµ‹è¯•FlashAttentionæ˜¯å¦æ­£ç¡®å¯ç”¨
"""

import os
import sys
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_flash_attention():
    """æµ‹è¯•FlashAttentionæ˜¯å¦æ­£ç¡®å¯ç”¨"""
    
    print("ğŸ” æµ‹è¯•FlashAttentionæ”¯æŒ...")
    
    # 1. æ£€æŸ¥CUDAæ”¯æŒ
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
    
    # 2. æ£€æŸ¥transformersç‰ˆæœ¬
    try:
        import transformers
        print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformersæœªå®‰è£…")
        return
    
    # 3. æ£€æŸ¥FlashAttentionæ”¯æŒ
    try:
        from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLPreTrainedModel
        print("âœ… Qwen2.5-VLæ¨¡å‹æ”¯æŒFlashAttention")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥Qwen2.5-VLæ¨¡å‹: {e}")
        return
    
    # 4. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    try:
        from models.qwen2_5_vl_classify import Qwen2_5_VLForImageClassification
        
        # åˆ›å»ºä¸€ä¸ªå°æ¨¡å‹è¿›è¡Œæµ‹è¯•
        model = Qwen2_5_VLForImageClassification(
            pretrained_model_name="Qwen/Qwen2.5-VL-7B-Instruct",
            num_labels=10
        )
        
        # æ£€æŸ¥attentionå®ç°
        if hasattr(model, 'config') and hasattr(model.config, '_attn_implementation'):
            attn_impl = model.config._attn_implementation
            print(f"âœ… æ¨¡å‹attentionå®ç°: {attn_impl}")
            
            if attn_impl == "flash_attention_2":
                print("ğŸ‰ FlashAttention 2 å·²æˆåŠŸå¯ç”¨!")
            elif attn_impl == "flash_attention_1":
                print("ğŸ‰ FlashAttention 1 å·²æˆåŠŸå¯ç”¨!")
            else:
                print(f"â„¹ï¸ ä½¿ç”¨ {attn_impl} attention")
        else:
            print("âš ï¸ æ— æ³•æ£€æµ‹attentionå®ç°")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_flash_attention() 