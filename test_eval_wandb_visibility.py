#!/usr/bin/env python3
"""
æµ‹è¯•evalæŒ‡æ ‡åœ¨WandBç•Œé¢ä¸Šçš„å¯è§æ€§
"""

import os
import sys
import time
import random
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from training.utils.monitor import TrainingMonitor

def test_eval_wandb_visibility():
    """æµ‹è¯•evalæŒ‡æ ‡åœ¨WandBç•Œé¢ä¸Šçš„å¯è§æ€§"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•evalæŒ‡æ ‡åœ¨WandBç•Œé¢ä¸Šçš„å¯è§æ€§...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        'model': {
            'pretrained_name': 'test_model',
            'num_labels': 10
        },
        'training': {
            'num_epochs': 1,
            'learning_rate': 1e-4
        },
        'wandb': {
            'enabled': True,
            'project': 'test_eval_visibility',
            'run_name': f'eval_visibility_test_{int(time.time())}'
        },
        'monitor': {
            'freq': {
                'training_log_freq': 1,
                'eval_log_freq': 1,
                'perf_log_freq': 1
            }
        },
        'datasets': {
            'dataset_configs': {
                'test_dataset': {
                    'num_classes': 10,
                    'description': 'Test dataset'
                }
            }
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./test_eval_visibility_output"
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = TrainingMonitor(output_dir, test_config)
    
    if not monitor.use_wandb:
        print("âŒ WandBæœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    print("âœ… WandBå·²å¯ç”¨ï¼Œå¼€å§‹è®°å½•æµ‹è¯•æŒ‡æ ‡...")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œè®°å½•å¤šä¸ªæ•°æ®ç‚¹
    for step in range(1, 21):  # è®°å½•20ä¸ªæ•°æ®ç‚¹
        # è®°å½•trainingæŒ‡æ ‡
        training_data = {
            "training/loss": 0.5 - step * 0.01,
            "training/lr": 1e-4,
            "training/epoch": 0.1 * step,
            "training/grad_norm": 1.0 + step * 0.1
        }
        
        monitor.log_metrics(training_data, step=step, commit=True)
        print(f"  âœ… Step {step}: trainingæŒ‡æ ‡è®°å½•æˆåŠŸ")
        
        # æ¯5æ­¥è®°å½•ä¸€æ¬¡evalæŒ‡æ ‡ï¼ˆæ¨¡æ‹Ÿè¯„ä¼°é¢‘ç‡ï¼‰
        if step % 5 == 0:
            # æ¨¡æ‹ŸevalæŒ‡æ ‡ï¼ˆä½¿ç”¨éšæœºå€¼æ¨¡æ‹ŸçœŸå®è®­ç»ƒè¿‡ç¨‹ï¼‰
            eval_loss = 0.3 - step * 0.005 + random.uniform(-0.02, 0.02)
            eval_accuracy = 0.8 + step * 0.01 + random.uniform(-0.05, 0.05)
            
            eval_data = {
                "eval/overall_loss": max(0.1, eval_loss),  # ç¡®ä¿æŸå¤±ä¸ºæ­£
                "eval/overall_accuracy": min(1.0, max(0.0, eval_accuracy)),  # ç¡®ä¿å‡†ç¡®ç‡åœ¨0-1ä¹‹é—´
                "eval/overall_samples": 1000,
                "eval/overall_correct": int(1000 * min(1.0, max(0.0, eval_accuracy))),
                "eval/test_dataset_loss": max(0.1, eval_loss),
                "eval/test_dataset_accuracy": min(1.0, max(0.0, eval_accuracy)),
                "eval/test_dataset_samples": 1000
            }
            
            monitor.log_metrics(eval_data, step=step, commit=True)
            print(f"  ğŸ“Š Step {step}: evalæŒ‡æ ‡è®°å½•æˆåŠŸ")
            print(f"     ğŸ“ˆ Eval Loss: {eval_data['eval/overall_loss']:.4f}")
            print(f"     ğŸ“ˆ Eval Accuracy: {eval_data['eval/overall_accuracy']:.4f}")
            
            # æ˜¾ç¤ºevalæŒ‡æ ‡è¯¦æƒ…
            eval_metrics_list = [k for k in eval_data.keys() if k.startswith('eval/')]
            print(f"     ğŸ“Š EvalæŒ‡æ ‡: {eval_metrics_list}")
        
        # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ€§èƒ½æŒ‡æ ‡
        if step % 10 == 0:
            perf_data = {
                "perf/step_time": 0.1 + random.uniform(-0.02, 0.02),
                "perf/mfu": 0.3 + random.uniform(-0.05, 0.05),
                "perf/tokens_per_second": 1000 + random.uniform(-100, 100)
            }
            
            monitor.log_metrics(perf_data, step=step, commit=True)
            print(f"  âš¡ Step {step}: perfæŒ‡æ ‡è®°å½•æˆåŠŸ")
        
        # çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®è®­ç»ƒ
        time.sleep(0.1)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š è¯·æ£€æŸ¥WandBç•Œé¢ï¼Œåº”è¯¥èƒ½çœ‹åˆ°:")
    print("   â€¢ TrainingæŒ‡æ ‡å›¾è¡¨: loss, lr, epoch, grad_norm")
    print("   â€¢ EvalæŒ‡æ ‡å›¾è¡¨: overall_loss, overall_accuracy, overall_samples, overall_correct")
    print("   â€¢ æ•°æ®é›†ç‰¹å®šæŒ‡æ ‡: test_dataset_loss, test_dataset_accuracy, test_dataset_samples")
    print("   â€¢ æ€§èƒ½æŒ‡æ ‡å›¾è¡¨: step_time, mfu, tokens_per_second")
    print("   â€¢ EvalæŒ‡æ ‡åœ¨step 5, 10, 15, 20æ—¶è®°å½•")
    print("   â€¢ æ€»å…±è®°å½•äº†4ä¸ªevalæ•°æ®ç‚¹")
    
    # æ˜¾ç¤ºWandB URL
    try:
        import wandb
        if wandb.run is not None:
            print(f"\nğŸ”— WandB URL: {wandb.run.url}")
            print(f"ğŸ“Š é¡¹ç›®: {wandb.run.project}")
            print(f"ğŸƒ è¿è¡Œåç§°: {wandb.run.name}")
    except Exception as e:
        print(f"âš ï¸ è·å–WandB URLå¤±è´¥: {e}")

if __name__ == "__main__":
    test_eval_wandb_visibility() 