#!/usr/bin/env python3
"""
è¯Šæ–­åˆ†å¸ƒå¼ranké—®é¢˜
"""

import os
import sys
import argparse
import torch
import deepspeed

def debug_distributed_info():
    """è¯Šæ–­åˆ†å¸ƒå¼ç¯å¢ƒä¿¡æ¯"""
    
    print("=" * 80)
    print("ğŸ” åˆ†å¸ƒå¼ç¯å¢ƒè¯Šæ–­")
    print("=" * 80)
    
    # 1. æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--local_rank", type=int, default=-1)
    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()
    
    print(f"ğŸ“‹ å‘½ä»¤è¡Œå‚æ•°:")
    print(f"  â€¢ --local_rank: {args.local_rank}")
    print(f"  â€¢ sys.argv: {sys.argv}")
    
    # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
    print(f"\nğŸŒ ç¯å¢ƒå˜é‡:")
    important_vars = [
        'LOCAL_RANK', 'RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT',
        'CUDA_VISIBLE_DEVICES', 'NCCL_DEBUG', 'NCCL_SOCKET_IFNAME'
    ]
    
    for var in important_vars:
        value = os.environ.get(var, 'NOT SET')
        print(f"  â€¢ {var}: {value}")
    
    # 3. åˆå§‹åŒ–åˆ†å¸ƒå¼
    print(f"\nğŸ”§ åˆå§‹åŒ–åˆ†å¸ƒå¼...")
    try:
        deepspeed.init_distributed()
        print("âœ… åˆ†å¸ƒå¼åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 4. æ£€æŸ¥åˆ†å¸ƒå¼çŠ¶æ€
    import torch.distributed as dist
    if dist.is_available() and dist.is_initialized():
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get('LOCAL_RANK', args.local_rank))
        
        print(f"\nğŸ“Š åˆ†å¸ƒå¼çŠ¶æ€:")
        print(f"  â€¢ Global Rank: {rank}")
        print(f"  â€¢ World Size: {world_size}")
        print(f"  â€¢ Local Rank: {local_rank}")
        print(f"  â€¢ Device: cuda:{local_rank}")
        
        # 5. æ£€æŸ¥GPUè®¾å¤‡
        if torch.cuda.is_available():
            current_device = torch.cuda.current_device()
            device_count = torch.cuda.device_count()
            print(f"  â€¢ Current CUDA Device: {current_device}")
            print(f"  â€¢ Available CUDA Devices: {device_count}")
            
            # è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
            torch.cuda.set_device(local_rank)
            print(f"  â€¢ Set CUDA Device to: {local_rank}")
        
        # 6. æµ‹è¯•ç®€å•çš„åˆ†å¸ƒå¼æ“ä½œ
        print(f"\nğŸ§ª æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ:")
        test_tensor = torch.tensor([rank], dtype=torch.float32, device=f'cuda:{local_rank}')
        print(f"  â€¢ Rank {rank} created tensor: {test_tensor}")
        
        # All-reduceæµ‹è¯•
        try:
            dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)
            print(f"  â€¢ Rank {rank} after all_reduce: {test_tensor}")
        except Exception as e:
            print(f"  â€¢ Rank {rank} all_reduce failed: {e}")
        
        # Barrieræµ‹è¯•
        try:
            dist.barrier()
            print(f"  â€¢ Rank {rank} passed barrier")
        except Exception as e:
            print(f"  â€¢ Rank {rank} barrier failed: {e}")
            
    else:
        print("âŒ åˆ†å¸ƒå¼æœªåˆå§‹åŒ–")
    
    print("\n" + "=" * 80)
    print(f"ğŸ è¯Šæ–­å®Œæˆ - Rank {rank if 'rank' in locals() else 'UNKNOWN'}")
    print("=" * 80)

if __name__ == "__main__":
    debug_distributed_info() 