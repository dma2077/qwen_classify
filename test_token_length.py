import torch
import os
from PIL import Image
from transformers import AutoProcessor
from models.qwen2_5_vl_classify import Qwen2_5_VLForImageClassification

def test_token_lengths_and_pooling():
    """æµ‹è¯•tokené•¿åº¦å’Œpoolingä½ç½®"""
    
    # é…ç½®
    model_path = "/llm_reco/dehua/model/Qwen2.5-VL-7B-Instruct"  # æ ¹æ®ä½ çš„è·¯å¾„è°ƒæ•´
    test_image_path = "/llm_reco/dehua/code/qwen_classify/data/dataset_food_label/images/test_image.jpg"  # è¯·æä¾›ä¸€ä¸ªæµ‹è¯•å›¾ç‰‡è·¯å¾„
    
    print("="*80)
    print("ğŸ” æµ‹è¯•Qwen2.5-VLæ¨¡å‹çš„Tokené•¿åº¦å’ŒPoolingä½ç½®")
    print("="*80)
    
    # 1. åŠ è½½æ¨¡å‹å’Œprocessor
    print("ğŸ“¦ åŠ è½½æ¨¡å‹å’Œprocessor...")
    try:
        model = Qwen2_5_VLForImageClassification(
            pretrained_model_name=model_path,
            num_labels=101
        )
        processor = AutoProcessor.from_pretrained(model_path)
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®...")
    
    # å¦‚æœæ²¡æœ‰æµ‹è¯•å›¾ç‰‡ï¼Œåˆ›å»ºä¸€ä¸ªdummyå›¾ç‰‡
    if not os.path.exists(test_image_path):
        print(f"âš ï¸  æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªdummyå›¾ç‰‡")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„RGBå›¾ç‰‡
        import numpy as np
        dummy_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        test_image = dummy_image
    else:
        test_image = Image.open(test_image_path).convert("RGB")
    
    # å‡†å¤‡messages
    messages = [
        {
            "role": "user", 
            "content": [
                {"type": "image", "image": test_image},
                {"type": "text", "text": "This is an image of food, what dish is it?"}
            ]
        }
    ]
    
    # 3. å¤„ç†è¾“å…¥
    print("ğŸ”„ å¤„ç†è¾“å…¥æ•°æ®...")
    try:
        # è½¬æ¢ä¸ºchatæ¨¡æ¿
        text = processor.apply_chat_template(
            conversation=messages,
            tokenize=False,
            add_generation_prompt=True
        )
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡æœ¬æ¨¡æ¿:\n{text}")
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # ä½¿ç”¨processorå¤„ç†
        inputs = processor(
            text=[text],
            images=[test_image],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        )
        
        print("âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ è¾“å…¥å¤„ç†å¤±è´¥: {e}")
        return
    
    # 4. åˆ†æè¾“å…¥æ•°æ®
    print("\nğŸ“‹ è¾“å…¥æ•°æ®åˆ†æ:")
    print("-" * 60)
    
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']
    pixel_values = inputs['pixel_values']
    
    print(f"ğŸ”¤ input_ids å½¢çŠ¶: {input_ids.shape}")
    print(f"ğŸ¯ attention_mask å½¢çŠ¶: {attention_mask.shape}")
    print(f"ğŸ–¼ï¸  pixel_values å½¢çŠ¶: {pixel_values.shape}")
    
    if 'image_grid_thw' in inputs:
        image_grid_thw = inputs['image_grid_thw']
        print(f"ğŸ“ image_grid_thw å½¢çŠ¶: {image_grid_thw.shape}")
        print(f"ğŸ“ image_grid_thw å€¼: {image_grid_thw}")
        
        # è®¡ç®—visual tokensæ•°é‡
        visual_tokens = image_grid_thw[0, 0] * image_grid_thw[0, 1] * image_grid_thw[0, 2]
        print(f"ğŸ‘ï¸  ä¼°ç®—çš„visual tokensæ•°é‡: {visual_tokens}")
    else:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°image_grid_thwå‚æ•°")
    
    # 5. åˆ†æåºåˆ—é•¿åº¦
    print(f"\nğŸ“ åºåˆ—é•¿åº¦åˆ†æ:")
    print("-" * 60)
    
    text_length = input_ids.size(1)
    attention_length = attention_mask.size(1)
    valid_tokens = attention_mask.sum(dim=1).item()
    
    print(f"ğŸ“ æ–‡æœ¬tokensé•¿åº¦ (input_ids): {text_length}")
    print(f"ğŸ¯ attention_maské•¿åº¦: {attention_length}")
    print(f"âœ… æœ‰æ•ˆtokensæ•°é‡: {valid_tokens}")
    print(f"ğŸ” æ¨æ–­çš„visual tokens: {valid_tokens - text_length}")
    
    # æ˜¾ç¤ºattention_maskçš„æ¨¡å¼
    print(f"\nğŸ¯ Attention Mask åˆ†æ:")
    print("-" * 60)
    mask_values = attention_mask[0].tolist()
    print(f"å‰20ä¸ªä½ç½®: {mask_values[:20]}")
    if len(mask_values) > 40:
        print(f"ä¸­é—´20ä¸ªä½ç½®: {mask_values[len(mask_values)//2-10:len(mask_values)//2+10]}")
    print(f"å20ä¸ªä½ç½®: {mask_values[-20:]}")
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
    first_valid = attention_mask[0].nonzero()[0].item()
    last_valid = attention_mask[0].nonzero()[-1].item()
    print(f"ğŸ¯ ç¬¬ä¸€ä¸ªæœ‰æ•ˆä½ç½®: {first_valid}")
    print(f"ğŸ¯ æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®: {last_valid}")
    
    # 6. è¿è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
    print(f"\nğŸš€ è¿è¡Œæ¨¡å‹å‰å‘ä¼ æ’­:")
    print("-" * 60)
    
    with torch.no_grad():
        # è·å–æ¨¡å‹å†…éƒ¨çš„hidden states
        model_outputs = model.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            return_dict=True,
            **{k: v for k, v in inputs.items() if k not in ['input_ids', 'attention_mask', 'pixel_values']}
        )
        
        hidden_states = model_outputs.last_hidden_state
        print(f"ğŸ§  Hidden states å½¢çŠ¶: {hidden_states.shape}")
        
        # è®¡ç®—poolingä½ç½®
        if attention_mask is not None:
            valid_lengths = attention_mask.sum(dim=1)
            last_positions = valid_lengths - 1
            last_positions = torch.clamp(last_positions, min=0, max=hidden_states.size(1)-1)
            
            print(f"ğŸ“ è®¡ç®—çš„valid_lengths: {valid_lengths}")
            print(f"ğŸ“ è®¡ç®—çš„last_positions: {last_positions}")
            print(f"ğŸ“ å®é™…ä½¿ç”¨çš„poolingä½ç½®: {last_positions.item()}")
            
            # æå–pooledç‰¹å¾
            pooled = hidden_states[torch.arange(hidden_states.size(0)), last_positions]
            print(f"ğŸ¯ Pooledç‰¹å¾å½¢çŠ¶: {pooled.shape}")
            
            # éªŒè¯ï¼šæ˜¾ç¤ºpoolingä½ç½®å‰åçš„attentionå€¼
            pos = last_positions.item()
            print(f"\nğŸ” Poolingä½ç½®éªŒè¯:")
            print(f"ä½ç½® {pos-2}: attention_mask = {attention_mask[0, pos-2].item() if pos >= 2 else 'N/A'}")
            print(f"ä½ç½® {pos-1}: attention_mask = {attention_mask[0, pos-1].item() if pos >= 1 else 'N/A'}")
            print(f"ä½ç½® {pos}: attention_mask = {attention_mask[0, pos].item()}")
            print(f"ä½ç½® {pos+1}: attention_mask = {attention_mask[0, pos+1].item() if pos < attention_mask.size(1)-1 else 'N/A'}")
            print(f"ä½ç½® {pos+2}: attention_mask = {attention_mask[0, pos+2].item() if pos < attention_mask.size(1)-2 else 'N/A'}")
            
        else:
            pooled = hidden_states[:, -1, :]
            print(f"âš ï¸  æ²¡æœ‰attention_maskï¼Œä½¿ç”¨æœ€åä½ç½®")
            print(f"ğŸ¯ Pooledç‰¹å¾å½¢çŠ¶: {pooled.shape}")
    
    # 7. è¿è¡Œå®Œæ•´çš„åˆ†ç±»æ¨¡å‹
    print(f"\nğŸ¯ è¿è¡Œå®Œæ•´åˆ†ç±»æ¨¡å‹:")
    print("-" * 60)
    
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            **{k: v for k, v in inputs.items() if k not in ['input_ids', 'attention_mask', 'pixel_values']}
        )
        
        print(f"ğŸ“Š Logits å½¢çŠ¶: {outputs.logits.shape}")
        print(f"ğŸ† é¢„æµ‹ç±»åˆ«: {outputs.logits.argmax(dim=-1).item()}")
        print(f"ğŸ“ˆ æœ€å¤§æ¦‚ç‡: {torch.softmax(outputs.logits, dim=-1).max().item():.4f}")
    
    # 8. æ€»ç»“
    print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print("="*80)
    print(f"âœ… æ–‡æœ¬tokensæ•°é‡: {text_length}")
    print(f"âœ… æ€»åºåˆ—é•¿åº¦: {attention_length}")
    print(f"âœ… æœ‰æ•ˆtokensæ•°é‡: {valid_tokens}")
    print(f"âœ… Visual tokensæ•°é‡: {valid_tokens - text_length}")
    print(f"âœ… Poolingä½ç½®: ç¬¬{last_positions.item()}ä¸ªä½ç½® (0-indexed)")
    print(f"âœ… æ˜¯å¦ä½¿ç”¨æœ€åæœ‰æ•ˆä½ç½®: {'æ˜¯' if last_positions.item() == valid_tokens - 1 else 'å¦'}")
    print("="*80)

def create_simple_test():
    """åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æµ‹è¯•ï¼Œä¸éœ€è¦å¤§æ¨¡å‹"""
    print("\nğŸ§ª ç®€åŒ–æµ‹è¯• (ä¸åŠ è½½å¤§æ¨¡å‹):")
    print("-" * 60)
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 2
    total_seq_len = 100
    text_len = 30
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„attention_mask
    # å‰70ä¸ªä½ç½®æœ‰æ•ˆï¼ˆvisual tokens + text tokensï¼‰ï¼Œå30ä¸ªæ˜¯padding
    attention_mask = torch.zeros(batch_size, total_seq_len)
    attention_mask[:, :70] = 1  # å‰70ä¸ªä½ç½®æœ‰æ•ˆ
    
    # æ¨¡æ‹Ÿhidden_states
    hidden_dim = 768
    hidden_states = torch.randn(batch_size, total_seq_len, hidden_dim)
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®:")
    print(f"  â€¢ batch_size: {batch_size}")
    print(f"  â€¢ total_seq_len: {total_seq_len}")
    print(f"  â€¢ text_len: {text_len}")
    print(f"  â€¢ hidden_dim: {hidden_dim}")
    
    # æµ‹è¯•poolingé€»è¾‘
    if attention_mask is not None:
        valid_lengths = attention_mask.sum(dim=1)
        last_positions = valid_lengths - 1
        last_positions = torch.clamp(last_positions, min=0, max=hidden_states.size(1)-1)
        
        print(f"\nğŸ¯ Poolingè®¡ç®—:")
        print(f"  â€¢ valid_lengths: {valid_lengths}")
        print(f"  â€¢ last_positions: {last_positions}")
        
        # æå–ç‰¹å¾
        pooled = hidden_states[torch.arange(hidden_states.size(0)), last_positions]
        print(f"  â€¢ pooled shape: {pooled.shape}")
        
        # éªŒè¯
        for i in range(batch_size):
            pos = last_positions[i].item()
            print(f"  â€¢ æ ·æœ¬{i}: ä»ä½ç½®{pos}æå–ç‰¹å¾ï¼Œattentionå€¼={attention_mask[i, pos].item()}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    try:
        test_token_lengths_and_pooling()
    except Exception as e:
        print(f"âŒ å®Œæ•´æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ”„ è¿è¡Œç®€åŒ–æµ‹è¯•...")
        create_simple_test() 