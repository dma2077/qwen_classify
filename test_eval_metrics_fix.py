#!/usr/bin/env python3
"""
æµ‹è¯•evalæŒ‡æ ‡è®°å½•ä¿®å¤
éªŒè¯evalæŒ‡æ ‡æ˜¯å¦èƒ½æ­£ç¡®æ˜¾ç¤ºåœ¨WandBä¸­
"""

import torch
import sys
import os
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_eval_metrics_fix():
    """æµ‹è¯•evalæŒ‡æ ‡è®°å½•ä¿®å¤"""
    
    print("ğŸ§ª æµ‹è¯•evalæŒ‡æ ‡è®°å½•ä¿®å¤...")
    print("=" * 50)
    
    # æ£€æŸ¥WandBæ˜¯å¦å¯ç”¨
    try:
        import wandb
        print(f"âœ… WandBå¯ç”¨: {wandb.__version__}")
    except ImportError:
        print("âŒ WandBä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    device = torch.device('cuda:0')
    
    # æµ‹è¯•1: æµ‹è¯•evalæŒ‡æ ‡è®°å½•
    print("\nğŸ“Š æµ‹è¯•1: æµ‹è¯•evalæŒ‡æ ‡è®°å½•")
    try:
        from training.utils.monitor import TrainingMonitor
        
        # åˆ›å»ºé…ç½®
        config = {
            'wandb': {
                'enabled': True,
                'project': 'test_eval_fix',
                'run_name': 'test_eval_run'
            },
            'output_dir': './test_output',
            'datasets': {
                'dataset_configs': {
                    'food101': {},
                    'cifar10': {}
                }
            }
        }
        
        # åˆ›å»ºmonitor
        monitor = TrainingMonitor('./test_output', config)
        
        # æµ‹è¯•trainingæŒ‡æ ‡è®°å½•
        print("  æµ‹è¯•trainingæŒ‡æ ‡è®°å½•...")
        training_data = {
            "training/loss": 0.5,
            "training/lr": 1e-4,
            "perf/mfu": 0.8
        }
        monitor.log_metrics(training_data, step=100, commit=True)
        print("  âœ… trainingæŒ‡æ ‡è®°å½•æˆåŠŸ")
        
        # æµ‹è¯•evalæŒ‡æ ‡è®°å½•
        print("  æµ‹è¯•evalæŒ‡æ ‡è®°å½•...")
        eval_data = {
            "eval/overall_loss": 0.3,
            "eval/overall_accuracy": 0.85,
            "eval/overall_samples": 1000,
            "eval/overall_correct": 850,
            "eval/food101_loss": 0.25,
            "eval/food101_accuracy": 0.88,
            "eval/cifar10_loss": 0.35,
            "eval/cifar10_accuracy": 0.82
        }
        monitor.log_metrics(eval_data, step=100, commit=True)  # ä½¿ç”¨ç›¸åŒçš„step
        print("  âœ… evalæŒ‡æ ‡è®°å½•æˆåŠŸ")
        
        # æµ‹è¯•è¿ç»­è®°å½•
        print("  æµ‹è¯•è¿ç»­è®°å½•...")
        for step in range(200, 210):
            # è®°å½•trainingæŒ‡æ ‡
            training_data = {
                "training/loss": 0.5 - step * 0.001,
                "training/lr": 1e-4,
                "perf/mfu": 0.8 + step * 0.001
            }
            monitor.log_metrics(training_data, step=step, commit=True)
            
            # å¦‚æœæ˜¯evalæ­¥éª¤ï¼Œè®°å½•evalæŒ‡æ ‡
            if step % 50 == 0:
                eval_data = {
                    "eval/overall_loss": 0.3 - step * 0.0001,
                    "eval/overall_accuracy": 0.85 + step * 0.0001,
                    "eval/overall_samples": 1000,
                    "eval/overall_correct": 850 + step
                }
                monitor.log_metrics(eval_data, step=step, commit=True)
                print(f"    âœ… Step {step}: evalæŒ‡æ ‡è®°å½•æˆåŠŸ")
            
            time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ
        
        print("  âœ… è¿ç»­è®°å½•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ evalæŒ‡æ ‡è®°å½•æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•2: æµ‹è¯•stepå€’é€€æ£€æµ‹å¯¹evalæŒ‡æ ‡çš„å®½æ¾å¤„ç†
    print("\nğŸ“Š æµ‹è¯•2: æµ‹è¯•stepå€’é€€æ£€æµ‹å¯¹evalæŒ‡æ ‡çš„å®½æ¾å¤„ç†")
    try:
        # æµ‹è¯•stepå€’é€€çš„trainingæŒ‡æ ‡ï¼ˆåº”è¯¥è¢«é˜»æ­¢ï¼‰
        print("  æµ‹è¯•stepå€’é€€çš„trainingæŒ‡æ ‡...")
        training_data = {
            "training/loss": 0.4,
            "training/lr": 1e-4,
            "perf/mfu": 0.8
        }
        monitor.log_metrics(training_data, step=50, commit=True)  # å€’é€€çš„step
        print("  âœ… stepå€’é€€çš„trainingæŒ‡æ ‡è¢«æ­£ç¡®å¤„ç†")
        
        # æµ‹è¯•stepå€’é€€çš„evalæŒ‡æ ‡ï¼ˆåº”è¯¥è¢«å…è®¸ï¼‰
        print("  æµ‹è¯•stepå€’é€€çš„evalæŒ‡æ ‡...")
        eval_data = {
            "eval/overall_loss": 0.2,
            "eval/overall_accuracy": 0.9,
            "eval/overall_samples": 1000,
            "eval/overall_correct": 900
        }
        monitor.log_metrics(eval_data, step=50, commit=True)  # å€’é€€çš„stepï¼Œä½†åŒ…å«evalæŒ‡æ ‡
        print("  âœ… stepå€’é€€çš„evalæŒ‡æ ‡è¢«å…è®¸è®°å½•")
        
    except Exception as e:
        print(f"âŒ stepå€’é€€æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•3: æµ‹è¯•æ··åˆæŒ‡æ ‡è®°å½•
    print("\nğŸ“Š æµ‹è¯•3: æµ‹è¯•æ··åˆæŒ‡æ ‡è®°å½•")
    try:
        # æµ‹è¯•åŒæ—¶åŒ…å«trainingå’ŒevalæŒ‡æ ‡çš„è®°å½•
        print("  æµ‹è¯•æ··åˆæŒ‡æ ‡è®°å½•...")
        mixed_data = {
            "training/loss": 0.45,
            "training/lr": 1e-4,
            "eval/overall_loss": 0.28,
            "eval/overall_accuracy": 0.87,
            "perf/mfu": 0.82
        }
        monitor.log_metrics(mixed_data, step=300, commit=True)
        print("  âœ… æ··åˆæŒ‡æ ‡è®°å½•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ··åˆæŒ‡æ ‡è®°å½•æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•4: éªŒè¯WandBä¸­çš„æŒ‡æ ‡åˆ†ç»„
    print("\nğŸ“Š æµ‹è¯•4: éªŒè¯WandBä¸­çš„æŒ‡æ ‡åˆ†ç»„")
    try:
        import wandb
        if wandb.run is not None:
            # æ£€æŸ¥WandBä¸­çš„æŒ‡æ ‡
            print("  æ£€æŸ¥WandBä¸­çš„æŒ‡æ ‡åˆ†ç»„...")
            
            # è®°å½•ä¸€äº›æµ‹è¯•æŒ‡æ ‡æ¥éªŒè¯åˆ†ç»„
            test_data = {
                "training/test_loss": 0.1,
                "eval/test_accuracy": 0.95,
                "perf/test_mfu": 0.9
            }
            monitor.log_metrics(test_data, step=400, commit=True)
            
            print("  âœ… æŒ‡æ ‡åˆ†ç»„éªŒè¯å®Œæˆ")
            print("  ğŸ“Š è¯·åœ¨WandBç•Œé¢ä¸­æ£€æŸ¥ä»¥ä¸‹æŒ‡æ ‡ç»„:")
            print("     â€¢ training/* - è®­ç»ƒç›¸å…³æŒ‡æ ‡")
            print("     â€¢ eval/* - è¯„ä¼°ç›¸å…³æŒ‡æ ‡") 
            print("     â€¢ perf/* - æ€§èƒ½ç›¸å…³æŒ‡æ ‡")
            
    except Exception as e:
        print(f"âŒ WandBæŒ‡æ ‡åˆ†ç»„éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æ¸…ç†
    try:
        if hasattr(monitor, 'use_wandb') and monitor.use_wandb:
            import wandb
            if wandb.run is not None:
                wandb.finish()
                print("  âœ… WandBè¿è¡Œå·²ç»“æŸ")
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†WandBå¤±è´¥: {e}")
    
    print("\nâœ… evalæŒ‡æ ‡è®°å½•ä¿®å¤æµ‹è¯•å®Œæˆ")
    print("\nğŸ“‹ ä¿®å¤æ€»ç»“:")
    print("  1. âœ… evalæ­¥éª¤æ—¶ä¹Ÿè®°å½•trainingæŒ‡æ ‡ï¼Œä½†ä½¿ç”¨commit=False")
    print("  2. âœ… evalæŒ‡æ ‡è®°å½•æ—¶ä½¿ç”¨commit=Trueï¼Œç¡®ä¿æ•°æ®åŒæ­¥")
    print("  3. âœ… stepå€’é€€æ£€æµ‹å¯¹evalæŒ‡æ ‡æ›´å®½æ¾ï¼Œå…è®¸ç›¸åŒstepçš„evalè®°å½•")
    print("  4. âœ… æ”¯æŒæ··åˆæŒ‡æ ‡è®°å½•ï¼ˆtraining + eval + perfï¼‰")
    print("  5. âœ… ç¡®ä¿evalæŒ‡æ ‡åœ¨WandBä¸­æ­£ç¡®åˆ†ç»„æ˜¾ç¤º")

if __name__ == "__main__":
    test_eval_metrics_fix() 