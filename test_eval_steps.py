#!/usr/bin/env python3
"""
æµ‹è¯•æ¯æ¬¡evaléƒ½èƒ½æ­£ç¡®è®°å½•
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_eval_steps():
    """æµ‹è¯•æ¯æ¬¡evaléƒ½èƒ½æ­£ç¡®è®°å½•"""
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'output_dir': '/tmp/test_eval_steps',
        'wandb': {
            'enabled': True,
            'project': 'qwen_classification_test',
            'run_name': 'test_eval_steps',
            'tags': ['test', 'eval', 'steps'],
            'notes': 'Testing eval steps recording'
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    try:
        from training.utils.monitor import TrainingMonitor
        
        # åˆ›å»ºmonitor
        monitor = TrainingMonitor(config['output_dir'], config)
        
        print("âœ… Monitoråˆ›å»ºæˆåŠŸ")
        
        # ç­‰å¾…WandBåˆå§‹åŒ–
        time.sleep(2)
        
        # æ¨¡æ‹Ÿæ¯30æ­¥evalä¸€æ¬¡
        eval_steps = [30, 60, 90, 120]
        
        for step in eval_steps:
            print(f"\n{'='*50}")
            print(f"ğŸ“Š æ¨¡æ‹Ÿç¬¬{step}æ­¥çš„eval")
            print(f"{'='*50}")
            
            # æ¨¡æ‹ŸtrainingæŒ‡æ ‡
            training_metrics = {
                "training/loss": 0.5 - step * 0.001,  # æ¨¡æ‹Ÿlossä¸‹é™
                "training/lr": 1e-5,
                "training/epoch": step // 100,
                "training/grad_norm": 1.0 + step * 0.01
            }
            
            # æ¨¡æ‹ŸevalæŒ‡æ ‡
            eval_metrics = {
                "eval/overall_loss": 0.3 - step * 0.0005,  # æ¨¡æ‹Ÿeval lossä¸‹é™
                "eval/overall_accuracy": 0.7 + step * 0.002,  # æ¨¡æ‹Ÿaccuracyä¸Šå‡
                "eval/overall_samples": 1000,
                "eval/overall_correct": int(700 + step * 2)
            }
            
            # åˆå¹¶æŒ‡æ ‡
            combined_metrics = training_metrics.copy()
            combined_metrics.update(eval_metrics)
            
            print(f"ğŸ“Š å‡†å¤‡è®°å½•æŒ‡æ ‡ (step={step}):")
            print(f"   ğŸƒ trainingæŒ‡æ ‡: {list(training_metrics.keys())}")
            print(f"   ğŸ“Š evalæŒ‡æ ‡: {list(eval_metrics.keys())}")
            print(f"   ğŸ”¢ æ€»æŒ‡æ ‡æ•°é‡: {len(combined_metrics)}")
            
            # è®°å½•åˆ°WandB
            monitor.log_metrics(combined_metrics, step=step, commit=True)
            
            print(f"âœ… ç¬¬{step}æ­¥æŒ‡æ ‡è®°å½•å®Œæˆ")
            
            # ç­‰å¾…ä¸€ä¸‹è®©WandBåŒæ­¥
            time.sleep(2)
        
        # ç»“æŸWandB
        monitor.finish_training()
        
        print(f"\n{'='*50}")
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("ğŸ”— è¯·æ£€æŸ¥WandBç•Œé¢ï¼Œåº”è¯¥èƒ½çœ‹åˆ°ä»¥ä¸‹evalæ­¥éª¤:")
        for step in eval_steps:
            print(f"   ğŸ“Š Step {step}: eval/overall_loss, eval/overall_accuracy")
        print(f"{'='*50}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_eval_steps() 