#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•FLOPsæµ‹é‡ä¿®å¤
éªŒè¯æ‰€æœ‰profileré”™è¯¯æ˜¯å¦å·²è§£å†³
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_flops_simple():
    """ç®€å•æµ‹è¯•FLOPsæµ‹é‡"""
    
    print("ğŸ§ª ç®€å•æµ‹è¯•FLOPsæµ‹é‡ä¿®å¤...")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    device = torch.device('cuda:0')
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleTestModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = torch.nn.Linear(512, 256)
            self.linear2 = torch.nn.Linear(256, 101)
            self.relu = torch.nn.ReLU()
            
        def forward(self, input_ids, attention_mask, pixel_values, labels):
            # æ¨¡æ‹Ÿå¤šæ¨¡æ€è¾“å…¥å¤„ç†
            batch_size = input_ids.size(0)
            seq_len = input_ids.size(1)
            
            # å¤„ç†æ–‡æœ¬è¾“å…¥
            text_features = torch.randn(batch_size, seq_len, 512, device=input_ids.device)
            text_output = self.linear1(text_features)
            text_output = self.relu(text_output)
            
            # å¤„ç†å›¾åƒè¾“å…¥ - ä¿®å¤ç»´åº¦é—®é¢˜
            image_features = torch.randn(batch_size, 3, 224, 224, device=pixel_values.device)
            # å°†å›¾åƒç‰¹å¾å±•å¹³å¹¶æŠ•å½±åˆ°æ­£ç¡®çš„ç»´åº¦
            image_flat = image_features.view(batch_size, -1)  # [batch_size, 3*224*224]
            image_projected = torch.nn.functional.linear(image_flat, torch.randn(256, image_flat.size(1), device=image_flat.device))  # [batch_size, 256]
            image_output = image_projected.unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, 256]
            
            # èåˆç‰¹å¾
            combined = text_output + image_output
            logits = self.linear2(combined)
            
            # è®¡ç®—æŸå¤± - ä¿®å¤batch sizeä¸åŒ¹é…é—®é¢˜
            # logitså½¢çŠ¶: [batch_size, seq_len, 101] -> [batch_size * seq_len, 101]
            # labelså½¢çŠ¶: [batch_size] -> [batch_size * seq_len]
            logits_flat = logits.view(-1, 101)  # [batch_size * seq_len, 101]
            labels_flat = labels.unsqueeze(1).expand(-1, seq_len).contiguous().view(-1)  # [batch_size * seq_len]
            loss = torch.nn.functional.cross_entropy(logits_flat, labels_flat)
            
            # è¿”å›ä¸€ä¸ªç±»ä¼¼transformersè¾“å‡ºçš„å¯¹è±¡ï¼ˆä¸åŒ…å«last_hidden_stateï¼‰
            class Outputs:
                def __init__(self, loss, logits):
                    self.loss = loss
                    self.logits = logits
                    # ä¸åŒ…å«last_hidden_stateï¼Œæ¨¡æ‹Ÿä½ çš„æ¨¡å‹è¾“å‡º
            
            return Outputs(loss, logits)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTestModel().to(device)
    print(f"âœ… åˆ›å»ºæµ‹è¯•æ¨¡å‹: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
    
    # åˆ›å»ºæµ‹è¯•batchï¼ˆåŒ…å«attention_maskï¼‰
    batch_size = 8
    seq_length = 512
    
    test_batch = {
        'input_ids': torch.randint(0, 1000, (batch_size, seq_length), device=device),
        'attention_mask': torch.ones(batch_size, seq_length, device=device),  # å…¨1çš„attention_mask
        'pixel_values': torch.randn(batch_size, 3, 224, 224, device=device),
        'labels': torch.randint(0, 101, (batch_size,), device=device)
    }
    
    print(f"âœ… åˆ›å»ºæµ‹è¯•batch: batch_size={batch_size}, seq_length={seq_length}")
    
    # æµ‹è¯•åºåˆ—é•¿åº¦è·å–
    print("\nğŸ“Š æµ‹è¯•åºåˆ—é•¿åº¦è·å–...")
    try:
        from training.utils.monitor import _get_actual_sequence_length
        seq_len = _get_actual_sequence_length(model, test_batch)
        print(f"âœ… åºåˆ—é•¿åº¦è·å–æˆåŠŸ: {seq_len}")
    except Exception as e:
        print(f"âŒ åºåˆ—é•¿åº¦è·å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•ä¼°ç®—æ–¹æ³•
    print("\nğŸ“Š æµ‹è¯•ä¼°ç®—æ–¹æ³•...")
    try:
        from training.utils.monitor import _estimate_flops_fallback
        estimated_flops = _estimate_flops_fallback(model, test_batch, seq_length)
        print(f"âœ… ä¼°ç®—æ–¹æ³•æˆåŠŸ: {estimated_flops:.2e}")
    except Exception as e:
        print(f"âŒ ä¼°ç®—æ–¹æ³•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•å®Œæ•´FLOPsæµ‹é‡
    print("\nğŸ“Š æµ‹è¯•å®Œæ•´FLOPsæµ‹é‡...")
    try:
        from training.utils.monitor import profile_model_flops
        total_flops = profile_model_flops(model, test_batch)
        print(f"âœ… å®Œæ•´FLOPsæµ‹é‡æˆåŠŸ: {total_flops:.2e}")
    except Exception as e:
        print(f"âŒ å®Œæ•´FLOPsæµ‹é‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… FLOPsæµ‹é‡ä¿®å¤æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_flops_simple() 