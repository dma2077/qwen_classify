#!/usr/bin/env python3
"""
æµ‹è¯•trainingå’Œevalé¢‘ç‡ä¸åŒçš„æƒ…å†µ
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_training_eval_frequency():
    """æµ‹è¯•trainingå’Œevalé¢‘ç‡ä¸åŒçš„æƒ…å†µ"""
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'output_dir': '/tmp/test_frequency',
        'wandb': {
            'enabled': True,
            'project': 'qwen_classification_test',
            'run_name': 'test_frequency',
            'tags': ['test', 'frequency'],
            'notes': 'Testing training and eval frequency'
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    try:
        from training.utils.monitor import TrainingMonitor
        
        # åˆ›å»ºmonitor
        monitor = TrainingMonitor(config['output_dir'], config)
        
        print("âœ… Monitoråˆ›å»ºæˆåŠŸ")
        
        # ç­‰å¾…WandBåˆå§‹åŒ–
        time.sleep(2)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼šæ¯5æ­¥è®°å½•trainingï¼Œæ¯30æ­¥eval
        total_steps = 100
        eval_interval = 30
        
        for step in range(1, total_steps + 1):
            print(f"\n{'='*40}")
            print(f"ğŸ“Š Step {step}")
            print(f"{'='*40}")
            
            # æ¨¡æ‹ŸtrainingæŒ‡æ ‡ï¼ˆæ¯ä¸ªæ­¥éª¤éƒ½æœ‰ï¼‰
            training_metrics = {
                "training/loss": 0.5 - step * 0.001,  # æ¨¡æ‹Ÿlossä¸‹é™
                "training/lr": 1e-5,
                "training/epoch": step // 50,
                "training/grad_norm": 1.0 + step * 0.01
            }
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯evalæ­¥éª¤
            is_eval_step = (step % eval_interval == 0)
            
            if is_eval_step:
                print(f"ğŸ¯ è¿™æ˜¯evalæ­¥éª¤ (step={step})")
                
                # æ¨¡æ‹ŸevalæŒ‡æ ‡
                eval_metrics = {
                    "eval/overall_loss": 0.3 - step * 0.0005,  # æ¨¡æ‹Ÿeval lossä¸‹é™
                    "eval/overall_accuracy": 0.7 + step * 0.002,  # æ¨¡æ‹Ÿaccuracyä¸Šå‡
                    "eval/overall_samples": 1000,
                    "eval/overall_correct": int(700 + step * 2)
                }
                
                # å…ˆè®°å½•trainingæŒ‡æ ‡ï¼ˆcommit=Falseï¼‰
                monitor.log_metrics(training_metrics, step=step, commit=False)
                print(f"   âœ… å·²è®°å½•trainingæŒ‡æ ‡ (commit=False)")
                
                # å†è®°å½•evalæŒ‡æ ‡ï¼ˆcommit=Trueï¼‰
                monitor.log_metrics(eval_metrics, step=step, commit=True)
                print(f"   âœ… å·²è®°å½•evalæŒ‡æ ‡ (commit=True)")
                print(f"   ğŸ“Š evalæŒ‡æ ‡: {list(eval_metrics.keys())}")
                
            else:
                print(f"ğŸƒ è¿™æ˜¯æ™®é€štrainingæ­¥éª¤ (step={step})")
                
                # åªè®°å½•trainingæŒ‡æ ‡
                monitor.log_metrics(training_metrics, step=step, commit=True)
                print(f"   âœ… å·²è®°å½•trainingæŒ‡æ ‡ (commit=True)")
            
            print(f"   ğŸ“ˆ trainingæŒ‡æ ‡: {list(training_metrics.keys())}")
            
            # ç­‰å¾…ä¸€ä¸‹è®©WandBåŒæ­¥
            time.sleep(0.5)
        
        # ç»“æŸWandB
        monitor.finish_training()
        
        print(f"\n{'='*50}")
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("ğŸ”— è¯·æ£€æŸ¥WandBç•Œé¢:")
        print("   ğŸ“Š trainingæŒ‡æ ‡åº”è¯¥åœ¨æ¯ä¸ªæ­¥éª¤éƒ½æœ‰è®°å½•")
        print("   ğŸ¯ evalæŒ‡æ ‡åº”è¯¥åªåœ¨æ­¥éª¤ 30, 60, 90 æœ‰è®°å½•")
        print(f"{'='*50}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_training_eval_frequency() 