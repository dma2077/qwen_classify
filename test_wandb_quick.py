#!/usr/bin/env python3
"""
å¿«é€ŸWandBæµ‹è¯•è„šæœ¬
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from training.utils.monitor import TrainingMonitor

def quick_test():
    """å¿«é€Ÿæµ‹è¯•WandBåŠŸèƒ½"""
    print("ğŸš€ å¿«é€ŸWandBæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        'model': {
            'pretrained_name': 'test_model',
            'num_labels': 10
        },
        'training': {
            'num_epochs': 1,
            'learning_rate': 1e-4
        },
        'wandb': {
            'enabled': True,
            'project': 'quick_test',
            'run_name': 'test_run'
        },
        'monitor': {
            'freq': {
                'training_log_freq': 1,
                'perf_log_freq': 1,
                'eval_log_freq': 1
            }
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./test_output"
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = TrainingMonitor(output_dir, test_config)
    
    if not monitor.use_wandb:
        print("âŒ WandBæœªå¯ç”¨")
        return
    
    print("âœ… WandBå·²å¯ç”¨")
    
    # æµ‹è¯•è®°å½•
    for step in range(1, 4):
        data = {
            "training/loss": 1.0 - step * 0.2,
            "training/lr": 1e-4,
            "training/epoch": step * 0.1,
            "training/grad_norm": 1.0 + step * 0.1
        }
        
        monitor.log_metrics(data, step=step, commit=True)
        print(f"âœ… Step {step} è®°å½•å®Œæˆ")
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    quick_test() 