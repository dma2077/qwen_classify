#!/usr/bin/env python3
"""
è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
"""

import os
import sys
import time
import yaml
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NCCL_NTHREADS'] = '64'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

def benchmark_dataloader():
    """åŸºå‡†æµ‹è¯•æ•°æ®åŠ è½½å™¨æ€§èƒ½"""
    print("âš¡ å¼€å§‹æ•°æ®åŠ è½½å™¨æ€§èƒ½æµ‹è¯•...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‡†å¤‡é…ç½®
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from data.dataloader import create_dataloaders
    
    start_time = time.time()
    train_loader, val_loader = create_dataloaders(config)
    creation_time = time.time() - start_time
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»º: {creation_time:.2f}s")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_loader.dataset)}")
    print(f"ğŸ“Š æ‰¹æ¬¡æ•°: {len(train_loader)}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦
    print("ğŸ”¥ æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦...")
    
    batch_times = []
    total_start = time.time()
    
    for i, batch in enumerate(train_loader):
        if i >= 10:  # åªæµ‹è¯•å‰10ä¸ªbatch
            break
        
        batch_start = time.time()
        # ç®€å•å¤„ç†batchæ•°æ®
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                _ = value.shape
        batch_time = time.time() - batch_start
        batch_times.append(batch_time)
        
        if i < 5:  # åªæ‰“å°å‰5ä¸ª
            print(f"  Batch {i+1}: {batch_time:.3f}s")
    
    total_time = time.time() - total_start
    avg_batch_time = sum(batch_times) / len(batch_times)
    
    print(f"ğŸ“Š å¹³å‡batchæ—¶é—´: {avg_batch_time:.3f}s")
    print(f"ğŸ“Š æ€»æ—¶é—´ (10 batch): {total_time:.1f}s")
    print(f"ğŸ“Š é¢„ä¼°10æ­¥æ—¶é—´: {avg_batch_time * 10:.1f}s")
    
    # æ€§èƒ½è¯„ä¼°
    if avg_batch_time > 3.0:
        print("âš ï¸  æ•°æ®åŠ è½½å¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜")
    elif avg_batch_time > 1.0:
        print("ğŸ”¶ æ•°æ®åŠ è½½æ€§èƒ½ä¸€èˆ¬")
    else:
        print("âœ… æ•°æ®åŠ è½½æ€§èƒ½è‰¯å¥½")
    
    return avg_batch_time

def benchmark_simple_forward():
    """åŸºå‡†æµ‹è¯•ç®€å•å‰å‘ä¼ æ’­"""
    print("\nâš¡ æµ‹è¯•ç®€å•æ¨¡å‹å‰å‘ä¼ æ’­...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"ğŸ”§ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ”§ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•tensor
    batch_size = 8
    seq_length = 512
    hidden_size = 768
    
    # æµ‹è¯•tensoråˆ›å»ºå’Œç§»åŠ¨æ—¶é—´
    start_time = time.time()
    test_tensor = torch.randn(batch_size, seq_length, hidden_size)
    if torch.cuda.is_available():
        test_tensor = test_tensor.to(device)
        torch.cuda.synchronize()
    creation_time = time.time() - start_time
    
    print(f"ğŸ“Š Tensoråˆ›å»ºå’Œç§»åŠ¨æ—¶é—´: {creation_time:.3f}s")
    
    # æµ‹è¯•ç®€å•è®¡ç®—
    start_time = time.time()
    for _ in range(10):
        result = torch.matmul(test_tensor, test_tensor.transpose(-1, -2))
        if torch.cuda.is_available():
            torch.cuda.synchronize()
    compute_time = (time.time() - start_time) / 10
    
    print(f"ğŸ“Š å¹³å‡è®¡ç®—æ—¶é—´: {compute_time:.3f}s")
    
    return creation_time, compute_time

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # æ•°æ®åŠ è½½å™¨åŸºå‡†æµ‹è¯•
    avg_batch_time = benchmark_dataloader()
    
    # ç®€å•è®¡ç®—åŸºå‡†æµ‹è¯•
    creation_time, compute_time = benchmark_simple_forward()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ€»ç»“:")
    print(f"  â€¢ æ•°æ®åŠ è½½: {avg_batch_time:.3f}s/batch")
    print(f"  â€¢ Tensoræ“ä½œ: {creation_time:.3f}s")
    print(f"  â€¢ è®¡ç®—æ“ä½œ: {compute_time:.3f}s")
    
    # ä¼°ç®—æ€»ä½“æ€§èƒ½
    estimated_step_time = avg_batch_time + creation_time + compute_time * 5  # ä¼°ç®—
    print(f"  â€¢ ä¼°ç®—æ­¥éª¤æ—¶é—´: {estimated_step_time:.1f}s")
    
    if estimated_step_time > 30:
        print("âŒ æ€§èƒ½å­˜åœ¨ä¸¥é‡é—®é¢˜")
    elif estimated_step_time > 15:
        print("âš ï¸  æ€§èƒ½éœ€è¦ä¼˜åŒ–")
    elif estimated_step_time > 7:
        print("ğŸ”¶ æ€§èƒ½ä¸€èˆ¬")
    else:
        print("âœ… æ€§èƒ½è‰¯å¥½") 