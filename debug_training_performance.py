import os
import time
import torch
import deepspeed
from tqdm import tqdm

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NCCL_NTHREADS'] = '64'
os.environ['MASTER_PORT'] = '29501'
os.environ['MASTER_ADDR'] = 'localhost'

def test_training_loop_performance():
    """æµ‹è¯•è®­ç»ƒå¾ªç¯çš„çº¯æ€§èƒ½"""
    
    print("ğŸ” å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    deepspeed.init_distributed()
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    device = torch.cuda.current_device()
    batch_size = 8
    seq_length = 1024
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    dummy_data = []
    for i in range(100):  # 100ä¸ªbatch
        batch = {
            "input_ids": torch.randint(0, 1000, (batch_size, seq_length)),
            "attention_mask": torch.ones((batch_size, seq_length)),
            "pixel_values": torch.randn((batch_size, 3, 224, 224)),
            "labels": torch.randint(0, 10, (batch_size,))
        }
        dummy_data.append(batch)
    
    print(f"ğŸ“Š å‡†å¤‡äº† {len(dummy_data)} ä¸ªbatchï¼Œæ¯ä¸ªbatchå¤§å°: {batch_size}")
    
    # æµ‹è¯•åŸºæœ¬æ•°æ®ä¼ è¾“æ€§èƒ½
    print("\nğŸ”¥ æµ‹è¯•1: çº¯æ•°æ®ä¼ è¾“æ€§èƒ½")
    start_time = time.time()
    
    for i, batch in enumerate(dummy_data[:20]):
        # æ¨¡æ‹Ÿæ•°æ®ä¼ è¾“
        inputs = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        pixel_values = batch["pixel_values"].to(device, non_blocking=True)
        labels = batch["labels"].to(device, non_blocking=True)
        
        if i % 10 == 0:
            print(f"  å¤„ç†batch {i+1}/20")
    
    data_transfer_time = time.time() - start_time
    print(f"âœ… æ•°æ®ä¼ è¾“æµ‹è¯•å®Œæˆ: {data_transfer_time:.2f}ç§’ (å¹³å‡ {data_transfer_time/20:.3f}ç§’/batch)")
    
    # æµ‹è¯•tqdmæ€§èƒ½
    print("\nğŸ”¥ æµ‹è¯•2: tqdmè¿›åº¦æ¡æ€§èƒ½")
    start_time = time.time()
    
    pbar = tqdm(total=20, desc="Testing Progress Bar")
    for i in range(20):
        # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
        time.sleep(0.01)
        pbar.update(1)
        pbar.set_postfix({
            'loss': f'{0.5:.4f}',
            'lr': f'{1e-4:.2e}',
            'step': i+1
        })
    pbar.close()
    
    tqdm_time = time.time() - start_time
    print(f"âœ… tqdmæµ‹è¯•å®Œæˆ: {tqdm_time:.2f}ç§’")
    
    # æµ‹è¯•ç®€å•çš„tensoræ“ä½œ
    print("\nğŸ”¥ æµ‹è¯•3: åŸºç¡€tensoræ“ä½œæ€§èƒ½")
    start_time = time.time()
    
    for i in range(20):
        # æ¨¡æ‹Ÿlossè®¡ç®—
        dummy_loss = torch.tensor(0.5, device=device)
        loss_item = dummy_loss.item()
        
        # æ¨¡æ‹Ÿæ¢¯åº¦normè®¡ç®—
        dummy_grad_norm = torch.tensor(1.0, device=device)
        grad_norm_item = dummy_grad_norm.item()
        
        if i % 10 == 0:
            print(f"  å¤„ç†tensoræ“ä½œ {i+1}/20")
    
    tensor_ops_time = time.time() - start_time
    print(f"âœ… Tensoræ“ä½œæµ‹è¯•å®Œæˆ: {tensor_ops_time:.2f}ç§’")
    
    # æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œï¼ˆå¦‚æœæœ‰å¤šGPUï¼‰
    print("\nğŸ”¥ æµ‹è¯•4: åˆ†å¸ƒå¼æ“ä½œæ€§èƒ½")
    start_time = time.time()
    
    import torch.distributed as dist
    if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:
        print(f"  æ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒ: {dist.get_world_size()} GPUs")
        for i in range(10):
            # æµ‹è¯•all_reduce
            test_tensor = torch.tensor(1.0, device=device)
            dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)
            result = test_tensor.item()
            
            if i % 5 == 0:
                print(f"  all_reduceæ“ä½œ {i+1}/10, result: {result}")
    else:
        print("  å•GPUç¯å¢ƒï¼Œè·³è¿‡åˆ†å¸ƒå¼æµ‹è¯•")
        time.sleep(0.1)  # æ¨¡æ‹Ÿä¸€äº›æ—¶é—´
    
    dist_ops_time = time.time() - start_time
    print(f"âœ… åˆ†å¸ƒå¼æ“ä½œæµ‹è¯•å®Œæˆ: {dist_ops_time:.2f}ç§’")
    
    # æ€»ç»“
    total_time = data_transfer_time + tqdm_time + tensor_ops_time + dist_ops_time
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“:")
    print(f"  â€¢ æ•°æ®ä¼ è¾“: {data_transfer_time:.2f}ç§’")
    print(f"  â€¢ tqdmè¿›åº¦æ¡: {tqdm_time:.2f}ç§’")
    print(f"  â€¢ Tensoræ“ä½œ: {tensor_ops_time:.2f}ç§’")
    print(f"  â€¢ åˆ†å¸ƒå¼æ“ä½œ: {dist_ops_time:.2f}ç§’")
    print(f"  â€¢ æ€»æ—¶é—´: {total_time:.2f}ç§’")
    print(f"  â€¢ å¹³å‡æ¯ä¸ªæ“ä½œ: {total_time/20:.3f}ç§’")

if __name__ == "__main__":
    test_training_loop_performance() 