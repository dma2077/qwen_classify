#!/usr/bin/env python3
"""
å®Œæ•´çš„WandBæ—¥å¿—è®°å½•æµ‹è¯•
éªŒè¯trainingã€evalå’ŒperfæŒ‡æ ‡éƒ½èƒ½æ­£ç¡®è®°å½•
"""

import os
import sys
import time
import torch
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from training.deepspeed_trainer import DeepSpeedTrainer
from training.utils.config_utils import prepare_config
from training.utils.monitor import TrainingMonitor

def test_wandb_logging_complete():
    """æµ‹è¯•å®Œæ•´çš„WandBæ—¥å¿—è®°å½•åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹å®Œæ•´WandBæ—¥å¿—è®°å½•æµ‹è¯•...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('./test_output', exist_ok=True)
    
    # åˆ›å»ºmonitorå®ä¾‹
    monitor = TrainingMonitor(
        output_dir='./test_output',
        config={'training': {'batch_size': 2}},
        flops_profile_freq=5
    )
    
    # æ¨¡æ‹Ÿä¸€äº›åŸºç¡€æ•°æ®
    monitor.actual_flops = 1e12  # 1 TFLOPs
    monitor.model_ref = "dummy_model"
    monitor.batch_size = 2
    monitor.seq_length = 512
    
    # æµ‹è¯•1: TrainingæŒ‡æ ‡è®°å½•
    print("\n" + "="*60)
    print("æµ‹è¯•1: TrainingæŒ‡æ ‡è®°å½•")
    print("="*60)
    
    training_data = {
        "training/loss": 0.3,
        "training/lr": 1e-5,
        "training/epoch": 0.5,
        "training/grad_norm": 0.1,
        "step": 10
    }
    
    print("ğŸ“ è®°å½•trainingæŒ‡æ ‡...")
    monitor.log_metrics(training_data, step=10, commit=True)
    
    # æµ‹è¯•2: PerfæŒ‡æ ‡è®°å½•
    print("\n" + "="*60)
    print("æµ‹è¯•2: PerfæŒ‡æ ‡è®°å½•")
    print("="*60)
    
    perf_data = {
        "perf/step_time": 0.05,
        "perf/steps_per_second": 20.0,
        "perf/mfu": 0.75,
        "perf/mfu_percent": 75.0,
        "perf/tokens_per_second": 1000.0,
        "perf/samples_per_second": 40.0,
        "perf/actual_flops": 1e12,
        "perf/actual_seq_length": 512.0,
        "perf/flops_per_second": 2e13,
        "step": 15
    }
    
    print("ğŸ“ è®°å½•perfæŒ‡æ ‡...")
    monitor.log_metrics(perf_data, step=15, commit=True)
    
    # æµ‹è¯•3: EvalæŒ‡æ ‡è®°å½•
    print("\n" + "="*60)
    print("æµ‹è¯•3: EvalæŒ‡æ ‡è®°å½•")
    print("="*60)
    
    eval_data = {
        "eval/overall_loss": 0.5,
        "eval/overall_accuracy": 0.85,
        "eval/overall_samples": 100,
        "eval/overall_correct": 85,
        "eval/food101_loss": 0.4,
        "eval/food101_accuracy": 0.9,
        "eval/food101_samples": 50,
        "eval/food101_correct": 45,
        "eval/cifar10_loss": 0.6,
        "eval/cifar10_accuracy": 0.8,
        "eval/cifar10_samples": 50,
        "eval/cifar10_correct": 40,
        "step": 20
    }
    
    print("ğŸ“ è®°å½•evalæŒ‡æ ‡...")
    monitor.log_metrics(eval_data, step=20, commit=True)
    
    # æµ‹è¯•4: åˆå¹¶æŒ‡æ ‡è®°å½•ï¼ˆæ¨¡æ‹Ÿevalæ­¥éª¤ï¼‰
    print("\n" + "="*60)
    print("æµ‹è¯•4: åˆå¹¶æŒ‡æ ‡è®°å½•ï¼ˆæ¨¡æ‹Ÿevalæ­¥éª¤ï¼‰")
    print("="*60)
    
    # å…ˆè®°å½•trainingæŒ‡æ ‡ï¼ˆä¸commitï¼‰
    print("ğŸ“ è®°å½•trainingæŒ‡æ ‡ï¼ˆä¸commitï¼‰...")
    monitor.log_metrics(training_data, step=30, commit=False)
    
    # å†è®°å½•evalæŒ‡æ ‡ï¼ˆcommitï¼‰
    print("ğŸ“ è®°å½•evalæŒ‡æ ‡ï¼ˆcommitï¼‰...")
    monitor.log_metrics(eval_data, step=30, commit=True)
    
    # æµ‹è¯•5: éªŒè¯trainerçš„æŒ‡æ ‡æ„å»ºæ–¹æ³•
    print("\n" + "="*60)
    print("æµ‹è¯•5: éªŒè¯trainerçš„æŒ‡æ ‡æ„å»ºæ–¹æ³•")
    print("="*60)
    
    # åˆ›å»ºtrainerå®ä¾‹
    trainer = DeepSpeedTrainer({'output_dir': './test_output'})
    trainer.monitor = monitor
    trainer.dist_ctx = type('obj', (object,), {
        'is_main_process': lambda: True,
        'world_size': 1
    })()
    
    # æµ‹è¯•_build_training_metricsæ–¹æ³•
    inputs = torch.randn(2, 10)
    attention_mask = torch.ones(2, 10)
    
    training_metrics = trainer._build_training_metrics(
        effective_step=40,
        epoch=1,
        aggregated_loss=0.25,
        current_lr=1e-5,
        grad_norm_value=0.08,
        inputs=inputs,
        attention_mask=attention_mask,
        step_time=0.06
    )
    
    print("ğŸ“‹ æ„å»ºçš„trainingæŒ‡æ ‡:")
    for key, value in training_metrics.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•_build_eval_metricsæ–¹æ³•
    eval_results = {
        'total_samples': 100,
        'total_correct': 85,
        'dataset_metrics': {
            'food101': {
                'loss': 0.4,
                'accuracy': 0.9,
                'samples': 50,
                'correct': 45
            },
            'cifar10': {
                'loss': 0.6,
                'accuracy': 0.8,
                'samples': 50,
                'correct': 40
            }
        }
    }
    
    eval_metrics = trainer._build_eval_metrics(0.5, 0.85, eval_results)
    
    print("\nğŸ“‹ æ„å»ºçš„evalæŒ‡æ ‡:")
    for key, value in eval_metrics.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•6: MFUè®¡ç®—
    print("\n" + "="*60)
    print("æµ‹è¯•6: MFUè®¡ç®—")
    print("="*60)
    
    # æµ‹è¯•ä¸åŒçš„step_timeå€¼
    test_cases = [
        (0.1, "æ­£å¸¸step_time"),
        (0.0, "é›¶step_time"),
        (None, "None step_time")
    ]
    
    for step_time, description in test_cases:
        print(f"ğŸ” æµ‹è¯• {description}...")
        mfu = trainer._calculate_mfu(50, inputs, attention_mask, step_time or 0.0)
        
        if mfu is not None:
            print(f"  âœ… MFUè®¡ç®—æˆåŠŸ: {mfu:.4f}")
        else:
            print(f"  âš ï¸ MFUè®¡ç®—è¿”å›None (é¢„æœŸè¡Œä¸º)")
    
    print("\n" + "="*60)
    print("âœ… å®Œæ•´WandBæ—¥å¿—è®°å½•æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("  â€¢ TrainingæŒ‡æ ‡è®°å½•: âœ…")
    print("  â€¢ PerfæŒ‡æ ‡è®°å½•: âœ…")
    print("  â€¢ EvalæŒ‡æ ‡è®°å½•: âœ…")
    print("  â€¢ åˆå¹¶æŒ‡æ ‡è®°å½•: âœ…")
    print("  â€¢ æŒ‡æ ‡æ„å»ºæ–¹æ³•: âœ…")
    print("  â€¢ MFUè®¡ç®—: âœ…")
    print("\nğŸ¯ å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œè¯´æ˜WandBæ—¥å¿—è®°å½•åŠŸèƒ½æ­£å¸¸ï¼")

def test_wandb_metrics_structure():
    """æµ‹è¯•WandBæŒ‡æ ‡ç»“æ„"""
    print("\n" + "="*60)
    print("æµ‹è¯•WandBæŒ‡æ ‡ç»“æ„")
    print("="*60)
    
    # åˆ›å»ºmonitorå®ä¾‹
    monitor = TrainingMonitor(
        output_dir='./test_output',
        config={'training': {'batch_size': 2}},
        flops_profile_freq=5
    )
    
    # æ¨¡æ‹Ÿå®Œæ•´çš„æŒ‡æ ‡ç»“æ„
    complete_metrics = {
        # TrainingæŒ‡æ ‡
        "training/loss": 0.3,
        "training/lr": 1e-5,
        "training/epoch": 0.5,
        "training/grad_norm": 0.1,
        
        # PerfæŒ‡æ ‡
        "perf/step_time": 0.05,
        "perf/steps_per_second": 20.0,
        "perf/mfu": 0.75,
        "perf/mfu_percent": 75.0,
        "perf/tokens_per_second": 1000.0,
        "perf/samples_per_second": 40.0,
        "perf/actual_flops": 1e12,
        "perf/actual_seq_length": 512.0,
        "perf/flops_per_second": 2e13,
        
        # EvalæŒ‡æ ‡
        "eval/overall_loss": 0.5,
        "eval/overall_accuracy": 0.85,
        "eval/overall_samples": 100,
        "eval/overall_correct": 85,
        "eval/food101_loss": 0.4,
        "eval/food101_accuracy": 0.9,
        "eval/food101_samples": 50,
        "eval/food101_correct": 45,
        "eval/cifar10_loss": 0.6,
        "eval/cifar10_accuracy": 0.8,
        "eval/cifar10_samples": 50,
        "eval/cifar10_correct": 40,
        
        # ç»Ÿä¸€stepå­—æ®µ
        "step": 100
    }
    
    print("ğŸ“‹ å®Œæ•´çš„æŒ‡æ ‡ç»“æ„:")
    training_count = 0
    perf_count = 0
    eval_count = 0
    
    for key, value in complete_metrics.items():
        if key.startswith('training/'):
            training_count += 1
            print(f"  ğŸƒ {key}: {value}")
        elif key.startswith('perf/'):
            perf_count += 1
            print(f"  âš¡ {key}: {value}")
        elif key.startswith('eval/'):
            eval_count += 1
            print(f"  ğŸ“Š {key}: {value}")
        else:
            print(f"  ğŸ”¢ {key}: {value}")
    
    print(f"\nğŸ“ˆ æŒ‡æ ‡ç»Ÿè®¡:")
    print(f"  â€¢ TrainingæŒ‡æ ‡: {training_count}ä¸ª")
    print(f"  â€¢ PerfæŒ‡æ ‡: {perf_count}ä¸ª")
    print(f"  â€¢ EvalæŒ‡æ ‡: {eval_count}ä¸ª")
    print(f"  â€¢ æ€»æŒ‡æ ‡: {len(complete_metrics)}ä¸ª")
    
    # éªŒè¯æŒ‡æ ‡åˆ†ç±»
    expected_training = ['training/loss', 'training/lr', 'training/epoch', 'training/grad_norm']
    expected_perf = ['perf/step_time', 'perf/steps_per_second', 'perf/mfu', 'perf/mfu_percent', 
                    'perf/tokens_per_second', 'perf/samples_per_second', 'perf/actual_flops', 
                    'perf/actual_seq_length', 'perf/flops_per_second']
    expected_eval = ['eval/overall_loss', 'eval/overall_accuracy', 'eval/overall_samples', 
                    'eval/overall_correct', 'eval/food101_loss', 'eval/food101_accuracy', 
                    'eval/food101_samples', 'eval/food101_correct', 'eval/cifar10_loss', 
                    'eval/cifar10_accuracy', 'eval/cifar10_samples', 'eval/cifar10_correct']
    
    actual_training = [k for k in complete_metrics.keys() if k.startswith('training/')]
    actual_perf = [k for k in complete_metrics.keys() if k.startswith('perf/')]
    actual_eval = [k for k in complete_metrics.keys() if k.startswith('eval/')]
    
    print(f"\nâœ… æŒ‡æ ‡åˆ†ç±»éªŒè¯:")
    print(f"  â€¢ TrainingæŒ‡æ ‡åŒ¹é…: {len(actual_training) == len(expected_training)}")
    print(f"  â€¢ PerfæŒ‡æ ‡åŒ¹é…: {len(actual_perf) == len(expected_perf)}")
    print(f"  â€¢ EvalæŒ‡æ ‡åŒ¹é…: {len(actual_eval) == len(expected_eval)}")

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    test_wandb_logging_complete()
    
    # è¿è¡ŒæŒ‡æ ‡ç»“æ„æµ‹è¯•
    test_wandb_metrics_structure()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œè¯´æ˜WandBæ—¥å¿—è®°å½•åŠŸèƒ½å·²ç»æ­£ç¡®å®ç°ã€‚")
    print("ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œå®é™…è®­ç»ƒæ¥éªŒè¯æ‰€æœ‰æŒ‡æ ‡éƒ½èƒ½æ­£ç¡®è®°å½•åˆ°WandBã€‚") 