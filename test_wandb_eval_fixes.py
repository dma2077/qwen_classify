#!/usr/bin/env python3
"""
æµ‹è¯•WandBç¦ç”¨å’Œè¯„ä¼°batch sizeä¿®å¤
"""

import os
import sys
import yaml
import json

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NCCL_NTHREADS'] = '64'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

def test_wandb_disabled():
    """æµ‹è¯•WandBæ˜¯å¦è¢«å®Œå…¨ç¦ç”¨"""
    print("ğŸš« æµ‹è¯•WandBç¦ç”¨çŠ¶æ€...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    try:
        from training.utils.monitor import TrainingMonitor
        
        # æµ‹è¯•TrainingMonitor
        monitor = TrainingMonitor("./temp_output", config)
        
        use_wandb = getattr(monitor, 'use_wandb', None)
        if use_wandb is False:
            print("âœ… TrainingMonitorä¸­WandBå·²ç¦ç”¨")
            training_disabled = True
        else:
            print(f"âš ï¸ TrainingMonitorä¸­WandBçŠ¶æ€: {use_wandb}")
            training_disabled = False
        
        # æµ‹è¯•DummyMonitor
        from training.utils.monitor import DummyMonitor
        dummy_monitor = DummyMonitor("./temp_output", config)
        
        dummy_use_wandb = getattr(dummy_monitor, 'use_wandb', None)
        if dummy_use_wandb is False:
            print("âœ… DummyMonitorä¸­WandBå·²ç¦ç”¨")
            dummy_disabled = True
        else:
            print(f"âš ï¸ DummyMonitorä¸­WandBçŠ¶æ€: {dummy_use_wandb}")
            dummy_disabled = False
        
        return training_disabled and dummy_disabled
        
    except Exception as e:
        print(f"âŒ WandBç¦ç”¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_eval_batch_size():
    """æµ‹è¯•è¯„ä¼°batch sizeä¿®å¤"""
    print("\nğŸ“Š æµ‹è¯•è¯„ä¼°batch sizeä¿®å¤...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‡†å¤‡é…ç½®
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    try:
        # è·å–DeepSpeedé…ç½®
        if 'deepspeed' in config:
            if isinstance(config['deepspeed'], str):
                with open(config['deepspeed'], 'r') as f:
                    deepspeed_config = json.load(f)
            else:
                deepspeed_config = config['deepspeed']
            
            train_micro_batch = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
            train_batch_size = deepspeed_config.get('train_batch_size', 1)
            gradient_accumulation = deepspeed_config.get('gradient_accumulation_steps', 1)
            
            print(f"DeepSpeedé…ç½®:")
            print(f"  â€¢ train_micro_batch_size_per_gpu: {train_micro_batch}")
            print(f"  â€¢ train_batch_size: {train_batch_size}")
            print(f"  â€¢ gradient_accumulation_steps: {gradient_accumulation}")
            
            # æµ‹è¯•æ•°æ®åŠ è½½å™¨
            from data.dataloader import create_dataloaders
             train_loader, val_loader = create_dataloaders(config)
             
             train_actual_batch = train_loader.batch_size
             val_actual_batch = val_loader.batch_size
             
             print(f"å®é™…DataLoader batch size:")
             print(f"  â€¢ è®­ç»ƒDataLoader batch size: {train_actual_batch}")
             print(f"  â€¢ è¯„ä¼°DataLoader batch size: {val_actual_batch}")
             
             # æ£€æŸ¥ç†è®ºè®¡ç®—
             world_size = 8  # å‡è®¾8å¡ï¼Œå®é™…è¿è¡Œæ—¶ä¼šä»åˆ†å¸ƒå¼è·å–
             expected_eval_batch = train_micro_batch * world_size  # gradient_accumulation_steps=1æ—¶çš„ç­‰æ•ˆ
             theoretical_total_batch = train_micro_batch * world_size * gradient_accumulation
             
             print(f"ç†è®ºè®¡ç®—:")
             print(f"  â€¢ æœŸæœ›è¯„ä¼°batch size: {expected_eval_batch} (micro_batch Ã— num_gpus)")
             print(f"  â€¢ ç†è®ºè®­ç»ƒæ€»batch size: {theoretical_total_batch} (åŒ…å«gradient accumulation)")
             print(f"  â€¢ é…ç½®çš„train_batch_size: {train_batch_size}")
             
             # æ£€æŸ¥ä¿®å¤é€»è¾‘
             # è®­ç»ƒDataLoaderåº”è¯¥ä½¿ç”¨micro_batch_size_per_gpu
             # è¯„ä¼°DataLoaderåº”è¯¥ä½¿ç”¨micro_batch_size_per_gpu Ã— num_gpus
             
             train_correct = (train_actual_batch == train_micro_batch)
             # æ³¨æ„ï¼šåœ¨å•æœºæµ‹è¯•æ—¶world_sizeå¯èƒ½æ˜¯1ï¼Œæ‰€ä»¥eval_batch_sizeå¯èƒ½ç­‰äºtrain_micro_batch
             eval_correct = (val_actual_batch >= train_micro_batch)  # è‡³å°‘ä¸å°äºmicro batch
             
             if train_correct and eval_correct:
                 print("âœ… batch sizeé€»è¾‘ä¿®å¤æ­£ç¡®")
                 print(f"   - è®­ç»ƒä½¿ç”¨micro batch: {train_actual_batch}")
                 print(f"   - è¯„ä¼°ä½¿ç”¨é€‚å½“å¤§å°: {val_actual_batch}")
                 return True
             else:
                 print("âš ï¸ batch sizeé€»è¾‘éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
                 print(f"   - è®­ç»ƒbatchæ­£ç¡®: {train_correct}")
                 print(f"   - è¯„ä¼°batchåˆç†: {eval_correct}")
                 return False
        else:
            print("âš ï¸ æœªä½¿ç”¨DeepSpeedé…ç½®")
            return True
            
    except Exception as e:
        print(f"âŒ batch sizeæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡æ”¹è¿›"""
    print("\nğŸ’¾ æµ‹è¯•å†…å­˜æ•ˆç‡...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‡†å¤‡é…ç½®
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    try:
        import torch
        
        if torch.cuda.is_available():
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            print(f"åˆå§‹GPUå†…å­˜: {initial_memory / 1e9:.2f} GB")
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            from data.dataloader import create_dataloaders
            train_loader, val_loader = create_dataloaders(config)
            
            # æµ‹è¯•å•ä¸ªbatchçš„å†…å­˜ä½¿ç”¨
            for batch in val_loader:
                batch_memory = torch.cuda.memory_allocated()
                print(f"åŠ è½½batchåGPUå†…å­˜: {batch_memory / 1e9:.2f} GB")
                print(f"batchå†…å­˜å¢é•¿: {(batch_memory - initial_memory) / 1e9:.2f} GB")
                
                # æ£€æŸ¥batchå¤§å°
                batch_size = batch["input_ids"].size(0)
                print(f"å®é™…batchå¤§å°: {batch_size}")
                
                # ä¼°ç®—å†…å­˜æ•ˆç‡
                memory_per_sample = (batch_memory - initial_memory) / batch_size
                print(f"æ¯æ ·æœ¬å†…å­˜: {memory_per_sample / 1e6:.2f} MB")
                
                break
            
            torch.cuda.empty_cache()
            
            if memory_per_sample < 1e9:  # å°äº1GBæ¯æ ·æœ¬
                print("âœ… å†…å­˜ä½¿ç”¨æ•ˆç‡è‰¯å¥½")
                return True
            else:
                print("âš ï¸ å†…å­˜ä½¿ç”¨ä»ç„¶è¾ƒé«˜")
                return False
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            return True
            
    except Exception as e:
        print(f"âŒ å†…å­˜æ•ˆç‡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_trainer_initialization():
    """æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–"""
    print("\nğŸš€ æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–...")
    
    # åŠ è½½é…ç½®
    config_file = "configs/fast_eval_config.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‡†å¤‡é…ç½®
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    try:
        import time
        
        start_time = time.time()
        from training.deepspeed_trainer import DeepSpeedTrainer
        trainer = DeepSpeedTrainer(config)
        init_time = time.time() - start_time
        
        print(f"è®­ç»ƒå™¨åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}s")
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶çŠ¶æ€
        mfu_stats = getattr(trainer, 'mfu_stats', None)
        monitor_wandb = getattr(trainer.monitor, 'use_wandb', None)
        
        print(f"ç»„ä»¶çŠ¶æ€:")
        print(f"  â€¢ mfu_stats: {mfu_stats}")
        print(f"  â€¢ monitor use_wandb: {monitor_wandb}")
        
        if init_time < 60 and mfu_stats is None and monitor_wandb is False:
            print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸä¸”ä¼˜åŒ–ç”Ÿæ•ˆ")
            return True
        else:
            print("âš ï¸ è®­ç»ƒå™¨åˆå§‹åŒ–å­˜åœ¨é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒå™¨åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹WandBç¦ç”¨å’Œè¯„ä¼°ä¿®å¤éªŒè¯")
    print("=" * 70)
    
    # æµ‹è¯•1: WandBç¦ç”¨
    wandb_ok = test_wandb_disabled()
    
    # æµ‹è¯•2: è¯„ä¼°batch sizeä¿®å¤
    batch_ok = test_eval_batch_size()
    
    # æµ‹è¯•3: å†…å­˜æ•ˆç‡
    memory_ok = test_memory_efficiency()
    
    # æµ‹è¯•4: è®­ç»ƒå™¨åˆå§‹åŒ–
    trainer_ok = test_trainer_initialization()
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  â€¢ WandBç¦ç”¨: {'âœ… æˆåŠŸ' if wandb_ok else 'âŒ å¤±è´¥'}")
    print(f"  â€¢ è¯„ä¼°batch sizeä¿®å¤: {'âœ… æˆåŠŸ' if batch_ok else 'âŒ å¤±è´¥'}")
    print(f"  â€¢ å†…å­˜æ•ˆç‡: {'âœ… è‰¯å¥½' if memory_ok else 'âš ï¸ ä¸€èˆ¬'}")
    print(f"  â€¢ è®­ç»ƒå™¨åˆå§‹åŒ–: {'âœ… æ­£å¸¸' if trainer_ok else 'âŒ å¼‚å¸¸'}")
    
    if wandb_ok and batch_ok and trainer_ok:
        print("\nğŸ‰ æ‰€æœ‰å…³é”®ä¿®å¤éªŒè¯æˆåŠŸï¼")
        print("   - WandBå·²å®Œå…¨ç¦ç”¨ï¼Œä¸å†æœ‰æ—¥å¿—å¼€é”€")
        print("   - è¯„ä¼°batch sizeå·²ä¿®å¤ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸")
        print("   - è®­ç»ƒæ€§èƒ½åº”è¯¥æ˜¾è‘—æå‡")
        sys.exit(0)
    else:
        print("\nâš ï¸ éƒ¨åˆ†ä¿®å¤éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        sys.exit(1) 