from torch.utils.data import DataLoader, DistributedSampler
from transformers import AutoProcessor
from .dataset import MyFoodDataset, MultiDatasetLoader
from .collator import create_collate_fn
import json
import torch.distributed as dist

def create_dataloaders(config):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒå¤šæ•°æ®é›†é…ç½®"""
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    pretrained_model_name = config['model']['pretrained_name']
    num_workers = config['training'].get('num_workers', 0)
    
    # è·å–å¤šæ•°æ®é›†é…ç½®
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    shuffle_datasets = config.get('datasets', {}).get('shuffle_datasets', True)
    
    # å‡†å¤‡è¯„ä¼°æ¯”ä¾‹å­—å…¸
    eval_ratios = {}
    for dataset_name, dataset_config in dataset_configs.items():
        eval_ratios[dataset_name] = dataset_config.get('eval_ratio', 0.2)
    
    # ä»DeepSpeedé…ç½®ä¸­è¯»å–æ‰¹æ¬¡å¤§å°
    if 'deepspeed' in config:
        if isinstance(config['deepspeed'], str):
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            with open(config['deepspeed'], 'r') as f:
                deepspeed_config = json.load(f)
        else:
            # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
            deepspeed_config = config['deepspeed']
        
        # ğŸ”¥ ä¿®å¤batch sizeé€»è¾‘ï¼š
        # - è®­ç»ƒDataLoaderä½¿ç”¨ï¼štrain_micro_batch_size_per_gpuï¼ˆDeepSpeedä¼šå¤„ç†gradient accumulationï¼‰
        # - è¯„ä¼°æ—¶å¸Œæœ›ä½¿ç”¨ï¼šmicro_batch_size_per_gpu Ã— num_gpusï¼ˆgradient_accumulation_steps=1çš„ç­‰æ•ˆï¼‰
        
        micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
        total_batch_size = deepspeed_config.get('train_batch_size', 1)
        gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
        # è®­ç»ƒDataLoaderä½¿ç”¨micro batch sizeï¼ˆDeepSpeedä¼šè‡ªåŠ¨å¤„ç†gradient accumulationï¼‰
        train_batch_size = micro_batch_size_per_gpu
        
        # è®¡ç®—GPUæ•°é‡ï¼šä¼˜å…ˆä»åˆ†å¸ƒå¼è·å–ï¼Œå¦åˆ™ä»DeepSpeedé…ç½®åæ¨
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            num_gpus = dist.get_world_size()
            print(f"ğŸ”§ ä»åˆ†å¸ƒå¼ç¯å¢ƒè·å–GPUæ•°é‡: {num_gpus}")
        else:
            # å¦‚æœåˆ†å¸ƒå¼æœªåˆå§‹åŒ–ï¼Œä»DeepSpeedé…ç½®åæ¨
            # train_batch_size = micro_batch_size_per_gpu Ã— num_gpus Ã— gradient_accumulation_steps
            # æ‰€ä»¥ num_gpus = train_batch_size / (micro_batch_size_per_gpu Ã— gradient_accumulation_steps)
            calculated_num_gpus = total_batch_size // (micro_batch_size_per_gpu * gradient_accumulation_steps)
            num_gpus = max(1, calculated_num_gpus)  # è‡³å°‘ä¸º1
            print(f"ğŸ”§ ä»DeepSpeedé…ç½®è®¡ç®—GPUæ•°é‡: {num_gpus}")
            print(f"   è®¡ç®—å…¬å¼: {total_batch_size} / ({micro_batch_size_per_gpu} Ã— {gradient_accumulation_steps}) = {num_gpus}")
        
        # è¯„ä¼°batch size = micro_batch_size_per_gpu Ã— num_gpusï¼ˆç›¸å½“äºgradient_accumulation_steps=1æ—¶çš„æ€»batch sizeï¼‰
        eval_batch_size = micro_batch_size_per_gpu * num_gpus
        
        # éªŒè¯è®¡ç®—æ˜¯å¦æ­£ç¡®
        expected_total_batch = micro_batch_size_per_gpu * num_gpus * gradient_accumulation_steps
        if expected_total_batch != total_batch_size:
            print(f"âš ï¸ batch sizeé…ç½®æ£€æŸ¥:")
            print(f"   micro_batch_size_per_gpu: {micro_batch_size_per_gpu}")
            print(f"   num_gpus: {num_gpus}")
            print(f"   gradient_accumulation_steps: {gradient_accumulation_steps}")
            print(f"   è®¡ç®—çš„æ€»batch size: {expected_total_batch}")
            print(f"   é…ç½®çš„train_batch_size: {total_batch_size}")
    else:
        batch_size = config['training'].get('batch_size', 8)
    
    # å‡†å¤‡ processor
    processor = AutoProcessor.from_pretrained(pretrained_model_name)
    
    # åˆ¤æ–­ä½¿ç”¨å•æ–‡ä»¶è¿˜æ˜¯å¤šæ–‡ä»¶æ¨¡å¼
    data_config = config.get('data', {})
    
    if 'train_jsonl_list' in data_config and 'val_jsonl_list' in data_config:
        # å¤šæ–‡ä»¶æ¨¡å¼
        train_jsonl_list = data_config['train_jsonl_list']
        val_jsonl_list = data_config['val_jsonl_list']
        
        # è·å–è¯„ä¼°é…ç½®
        eval_config = config.get('training', {}).get('evaluation', {})
        use_partial_eval = eval_config.get('partial_eval_during_training', True)
        
        # æ„é€ è®­ç»ƒæ•°æ®é›†
        train_dataset = MultiDatasetLoader(
            jsonl_file_list=train_jsonl_list,
            dataset_configs=dataset_configs,
            shuffle_datasets=shuffle_datasets,
            eval_ratios=eval_ratios,
            is_eval=False,
            use_partial_eval=False  # è®­ç»ƒæ—¶æ€»æ˜¯ä½¿ç”¨å…¨éƒ¨æ•°æ®
        )
        
        # æ„é€ éªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨éƒ¨åˆ†è¯„ä¼°ï¼‰
        val_dataset = MultiDatasetLoader(
            jsonl_file_list=val_jsonl_list,
            dataset_configs=dataset_configs,
            shuffle_datasets=False,  # éªŒè¯æ—¶ä¸shuffle
            eval_ratios=eval_ratios,
            is_eval=True,
            use_partial_eval=use_partial_eval
        )
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºå®Œæ•´è¯„ä¼°
        val_dataset._original_file_list = val_jsonl_list
        
    else:
        # å•æ–‡ä»¶æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        train_jsonl = data_config.get('train_jsonl')
        val_jsonl = data_config.get('val_jsonl')
        
        if not train_jsonl or not val_jsonl:
            raise ValueError("è¯·åœ¨é…ç½®ä¸­æä¾› train_jsonl_list/val_jsonl_list æˆ– train_jsonl/val_jsonl")
        
        # æ„é€ è®­ç»ƒæ•°æ®é›†ï¼Œä¼ é€’æ•°æ®é›†é…ç½®
        train_dataset = MyFoodDataset(train_jsonl, dataset_configs=dataset_configs)
        
        # æ„é€ éªŒè¯æ•°æ®é›†ï¼Œä¼ é€’æ•°æ®é›†é…ç½®
        val_dataset = MyFoodDataset(val_jsonl, dataset_configs=dataset_configs)
    
    train_collate_fn = create_collate_fn(processor)
    val_collate_fn = create_collate_fn(processor)
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    use_distributed = dist.is_available() and dist.is_initialized()
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°å…³é”®ä¿¡æ¯
    is_main_process = not use_distributed or dist.get_rank() == 0
    
    if is_main_process:
        # æ‰“å°æ‰¹æ¬¡å¤§å°é…ç½®ä¿¡æ¯
        if 'deepspeed' in config:
            print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°é…ç½®:")
            print(f"  â€¢ è®­ç»ƒæ‰¹æ¬¡å¤§å°: {train_batch_size} (per GPU)")
            print(f"  â€¢ è¯„ä¼°æ‰¹æ¬¡å¤§å°: {eval_batch_size} (total)")
        else:
            print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°é…ç½®: {batch_size}")
        
        # æ‰“å°æ•°æ®é›†é…ç½®ä¿¡æ¯
        if dataset_configs:
            print(f"ğŸ“Š æ•°æ®é›†é…ç½®:")
            for dataset_name, config_info in dataset_configs.items():
                num_classes = config_info.get('num_classes', 'N/A')
                eval_ratio = config_info.get('eval_ratio', 'N/A')
                print(f"  â€¢ {dataset_name}: {num_classes} classes, eval_ratio={eval_ratio}")
    
    # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆå¦‚æœä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼‰
    if use_distributed:
        world_size = dist.get_world_size()
        rank = dist.get_rank()
        
        train_sampler = DistributedSampler(
            train_dataset,
            shuffle=True,
            drop_last=True  # ç¡®ä¿æ‰€æœ‰GPUå¤„ç†ç›¸åŒæ•°é‡çš„æ‰¹æ¬¡
        )
        val_sampler = DistributedSampler(
            val_dataset,
            shuffle=False,
            drop_last=False
        )
        shuffle_train = False  # åˆ†å¸ƒå¼é‡‡æ ·å™¨å·²ç»å¤„ç†äº†shuffle
        shuffle_val = False
    else:
        train_sampler = None
        val_sampler = None
        shuffle_train = True
        shuffle_val = False
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_batch_size if 'deepspeed' in config else batch_size,
        shuffle=shuffle_train,
        sampler=train_sampler,
        num_workers=num_workers,
        collate_fn=train_collate_fn,
        pin_memory=True,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_dataset,
        batch_size=eval_batch_size if 'deepspeed' in config else batch_size,
        shuffle=shuffle_val,
        sampler=val_sampler,
        num_workers=num_workers,
        collate_fn=val_collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    return train_loader, val_loader

def create_full_eval_dataloader(config, model_processor=None):
    """åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºè®­ç»ƒç»“æŸåçš„å®Œæ•´è¯„ä¼°ï¼‰"""
    # è·å–é…ç½®
    data_config = config.get('data', {})
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    
    if 'val_jsonl_list' not in data_config:
        return None
    
    val_jsonl_list = data_config['val_jsonl_list']
    
    # å‡†å¤‡è¯„ä¼°æ¯”ä¾‹å­—å…¸ï¼ˆå®Œæ•´è¯„ä¼°æ—¶è®¾ä¸º1.0ï¼‰
    eval_ratios = {name: 1.0 for name in dataset_configs.keys()}
    
    # åˆ›å»ºå®Œæ•´éªŒè¯æ•°æ®é›†
    full_val_dataset = MultiDatasetLoader(
        jsonl_file_list=val_jsonl_list,
        dataset_configs=dataset_configs,
        shuffle_datasets=False,  # å®Œæ•´è¯„ä¼°æ—¶ä¸shuffle
        eval_ratios=eval_ratios,
        is_eval=True,
        use_partial_eval=False  # ä½¿ç”¨å®Œæ•´æ•°æ®
    )
    
    # è·å–processor
    if model_processor is None:
        pretrained_model_name = config['model']['pretrained_name']
        processor = AutoProcessor.from_pretrained(pretrained_model_name)
    else:
        processor = model_processor
    
    val_collate_fn = create_collate_fn(processor)
    
    # è·å–æ‰¹æ¬¡å¤§å°
    if 'deepspeed' in config:
        if isinstance(config['deepspeed'], str):
            with open(config['deepspeed'], 'r') as f:
                deepspeed_config = json.load(f)
        else:
            deepspeed_config = config['deepspeed']
        # ğŸ”¥ ä¿®å¤ï¼šè¯„ä¼°æ—¶ä½¿ç”¨æ€»çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼Œè€Œä¸æ˜¯æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°
        batch_size = deepspeed_config.get('train_batch_size', 1)
    else:
        batch_size = config['training'].get('batch_size', 8)
    
    # æ£€æŸ¥åˆ†å¸ƒå¼è®¾ç½®
    use_distributed = dist.is_available() and dist.is_initialized()
    
    if use_distributed:
        val_sampler = DistributedSampler(
            full_val_dataset,
            shuffle=False,
            drop_last=False
        )
        shuffle_val = False
    else:
        val_sampler = None
        shuffle_val = False
    
    # åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨
    full_val_loader = DataLoader(
        full_val_dataset,
        batch_size=batch_size,
        shuffle=shuffle_val,
        sampler=val_sampler,
        num_workers=config['training'].get('num_workers', 0),
        collate_fn=val_collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    return full_val_loader