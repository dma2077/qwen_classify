from torch.utils.data import DataLoader, DistributedSampler
from transformers import AutoProcessor
from .dataset import MyFoodDataset, MultiDatasetLoader
from .collator import create_collate_fn
import json
import torch.distributed as dist

def create_dataloaders(config):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒå¤šæ•°æ®é›†é…ç½®"""
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    pretrained_model_name = config['model']['pretrained_name']
    num_workers = config['training'].get('num_workers', 0)
    
    # è·å–å¤šæ•°æ®é›†é…ç½®
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    shuffle_datasets = config.get('datasets', {}).get('shuffle_datasets', True)
    
    # å‡†å¤‡è¯„ä¼°æ¯”ä¾‹å­—å…¸
    eval_ratios = {}
    for dataset_name, dataset_config in dataset_configs.items():
        eval_ratios[dataset_name] = dataset_config.get('eval_ratio', 0.2)
    
    # ä»DeepSpeedé…ç½®ä¸­è¯»å–æ‰¹æ¬¡å¤§å°
    if 'deepspeed' in config:
        if isinstance(config['deepspeed'], str):
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            with open(config['deepspeed'], 'r') as f:
                deepspeed_config = json.load(f)
        else:
            # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
            deepspeed_config = config['deepspeed']
        
        # ä½¿ç”¨micro_batch_size_per_gpuä½œä¸ºDataLoaderçš„batch_size
        batch_size = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
    else:
        batch_size = config['training'].get('batch_size', 8)
    
    # å‡†å¤‡ processor
    processor = AutoProcessor.from_pretrained(pretrained_model_name)
    
    # åˆ¤æ–­ä½¿ç”¨å•æ–‡ä»¶è¿˜æ˜¯å¤šæ–‡ä»¶æ¨¡å¼
    data_config = config.get('data', {})
    
    if 'train_jsonl_list' in data_config and 'val_jsonl_list' in data_config:
        # å¤šæ–‡ä»¶æ¨¡å¼
        train_jsonl_list = data_config['train_jsonl_list']
        val_jsonl_list = data_config['val_jsonl_list']
        
        # è·å–è¯„ä¼°é…ç½®
        eval_config = config.get('training', {}).get('evaluation', {})
        use_partial_eval = eval_config.get('partial_eval_during_training', True)
        
        # æ„é€ è®­ç»ƒæ•°æ®é›†
        train_dataset = MultiDatasetLoader(
            jsonl_file_list=train_jsonl_list,
            dataset_configs=dataset_configs,
            shuffle_datasets=shuffle_datasets,
            eval_ratios=eval_ratios,
            is_eval=False,
            use_partial_eval=False  # è®­ç»ƒæ—¶æ€»æ˜¯ä½¿ç”¨å…¨éƒ¨æ•°æ®
        )
        
        # æ„é€ éªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨éƒ¨åˆ†è¯„ä¼°ï¼‰
        val_dataset = MultiDatasetLoader(
            jsonl_file_list=val_jsonl_list,
            dataset_configs=dataset_configs,
            shuffle_datasets=False,  # éªŒè¯æ—¶ä¸shuffle
            eval_ratios=eval_ratios,
            is_eval=True,
            use_partial_eval=use_partial_eval
        )
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºå®Œæ•´è¯„ä¼°
        val_dataset._original_file_list = val_jsonl_list
        
    else:
        # å•æ–‡ä»¶æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        train_jsonl = data_config.get('train_jsonl')
        val_jsonl = data_config.get('val_jsonl')
        
        if not train_jsonl or not val_jsonl:
            raise ValueError("è¯·åœ¨é…ç½®ä¸­æä¾› train_jsonl_list/val_jsonl_list æˆ– train_jsonl/val_jsonl")
        
        # æ„é€ è®­ç»ƒæ•°æ®é›†ï¼Œä¼ é€’æ•°æ®é›†é…ç½®
        train_dataset = MyFoodDataset(train_jsonl, dataset_configs=dataset_configs)
        
        # æ„é€ éªŒè¯æ•°æ®é›†ï¼Œä¼ é€’æ•°æ®é›†é…ç½®
        val_dataset = MyFoodDataset(val_jsonl, dataset_configs=dataset_configs)
    
    train_collate_fn = create_collate_fn(processor)
    val_collate_fn = create_collate_fn(processor)
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    use_distributed = dist.is_available() and dist.is_initialized()
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°åˆ†å¸ƒå¼ä¿¡æ¯
    is_main_process = not use_distributed or dist.get_rank() == 0
    
    if is_main_process:
        print(f"\nåˆ†å¸ƒå¼æ£€æŸ¥:")
        print(f"  â€¢ dist.is_available(): {dist.is_available()}")
        print(f"  â€¢ dist.is_initialized(): {dist.is_initialized()}")
        print(f"  â€¢ ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ: {use_distributed}")
        
        # æ‰“å°æ•°æ®é›†é…ç½®ä¿¡æ¯
        if dataset_configs:
            print(f"\nğŸ“Š æ•°æ®é›†é…ç½®:")
            for dataset_name, config_info in dataset_configs.items():
                num_classes = config_info.get('num_classes', 'N/A')
                eval_ratio = config_info.get('eval_ratio', 'N/A')
                description = config_info.get('description', 'No description')
                print(f"  â€¢ {dataset_name}: {num_classes} classes, eval_ratio={eval_ratio} - {description}")
        
        # æ‰“å°è¯„ä¼°é…ç½®
        if 'train_jsonl_list' in data_config:
            eval_config = config.get('training', {}).get('evaluation', {})
            print(f"\nğŸ” è¯„ä¼°é…ç½®:")
            print(f"  â€¢ è®­ç»ƒè¿‡ç¨‹ä¸­éƒ¨åˆ†è¯„ä¼°: {eval_config.get('partial_eval_during_training', True)}")
            print(f"  â€¢ è®­ç»ƒç»“æŸåå®Œæ•´è¯„ä¼°: {eval_config.get('full_eval_at_end', True)}")
            print(f"  â€¢ ä»…è¯„ä¼°æœ€ä½³æ¨¡å‹: {eval_config.get('eval_best_model_only', True)}")
    
    # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆå¦‚æœä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼‰
    if use_distributed:
        world_size = dist.get_world_size()
        rank = dist.get_rank()
        
        if is_main_process:
            print(f"  â€¢ ä¸–ç•Œå¤§å°: {world_size}")
            print(f"  â€¢ å½“å‰è¿›ç¨‹: {rank}")
        
        train_sampler = DistributedSampler(
            train_dataset,
            shuffle=True,
            drop_last=True  # ç¡®ä¿æ‰€æœ‰GPUå¤„ç†ç›¸åŒæ•°é‡çš„æ‰¹æ¬¡
        )
        val_sampler = DistributedSampler(
            val_dataset,
            shuffle=False,
            drop_last=False
        )
        shuffle_train = False  # åˆ†å¸ƒå¼é‡‡æ ·å™¨å·²ç»å¤„ç†äº†shuffle
        shuffle_val = False
        
        if is_main_process:
            print(f"  â€¢ æ¯ä¸ªGPUå°†å¤„ç†è®­ç»ƒæ ·æœ¬æ•°: {len(train_sampler)}")
            print(f"  â€¢ æ¯ä¸ªGPUå°†å¤„ç†éªŒè¯æ ·æœ¬æ•°: {len(val_sampler)}")
    else:
        train_sampler = None
        val_sampler = None
        shuffle_train = True
        shuffle_val = False
        if is_main_process:
            print(f"  â€¢ æœªä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=shuffle_train,
        sampler=train_sampler,
        num_workers=num_workers,
        collate_fn=train_collate_fn,
        pin_memory=True,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=shuffle_val,
        sampler=val_sampler,
        num_workers=num_workers,
        collate_fn=val_collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    return train_loader, val_loader

def create_full_eval_dataloader(config, model_processor=None):
    """åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºè®­ç»ƒç»“æŸåçš„å®Œæ•´è¯„ä¼°ï¼‰"""
    # è·å–é…ç½®
    data_config = config.get('data', {})
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    
    if 'val_jsonl_list' not in data_config:
        return None
    
    val_jsonl_list = data_config['val_jsonl_list']
    
    # å‡†å¤‡è¯„ä¼°æ¯”ä¾‹å­—å…¸ï¼ˆå®Œæ•´è¯„ä¼°æ—¶è®¾ä¸º1.0ï¼‰
    eval_ratios = {name: 1.0 for name in dataset_configs.keys()}
    
    # åˆ›å»ºå®Œæ•´éªŒè¯æ•°æ®é›†
    full_val_dataset = MultiDatasetLoader(
        jsonl_file_list=val_jsonl_list,
        dataset_configs=dataset_configs,
        shuffle_datasets=False,  # å®Œæ•´è¯„ä¼°æ—¶ä¸shuffle
        eval_ratios=eval_ratios,
        is_eval=True,
        use_partial_eval=False  # ä½¿ç”¨å®Œæ•´æ•°æ®
    )
    
    # è·å–processor
    if model_processor is None:
        pretrained_model_name = config['model']['pretrained_name']
        processor = AutoProcessor.from_pretrained(pretrained_model_name)
    else:
        processor = model_processor
    
    val_collate_fn = create_collate_fn(processor)
    
    # è·å–æ‰¹æ¬¡å¤§å°
    if 'deepspeed' in config:
        if isinstance(config['deepspeed'], str):
            with open(config['deepspeed'], 'r') as f:
                deepspeed_config = json.load(f)
        else:
            deepspeed_config = config['deepspeed']
        batch_size = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
    else:
        batch_size = config['training'].get('batch_size', 8)
    
    # æ£€æŸ¥åˆ†å¸ƒå¼è®¾ç½®
    use_distributed = dist.is_available() and dist.is_initialized()
    
    if use_distributed:
        val_sampler = DistributedSampler(
            full_val_dataset,
            shuffle=False,
            drop_last=False
        )
        shuffle_val = False
    else:
        val_sampler = None
        shuffle_val = False
    
    # åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨
    full_val_loader = DataLoader(
        full_val_dataset,
        batch_size=batch_size,
        shuffle=shuffle_val,
        sampler=val_sampler,
        num_workers=config['training'].get('num_workers', 0),
        collate_fn=val_collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    return full_val_loader