import os
import random
from torch.utils.data import Dataset
from PIL import Image

class BaseDataset(Dataset):
    def __init__(self, data_list):
        self.data_list = data_list

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item_data = self.data_list[idx]
        # æ”¯æŒæ–°æ ¼å¼ï¼š(image_path, messages, label, dataset_name, num_classes)
        if len(item_data) == 5:
            image_path, messages, label, dataset_name, num_classes = item_data
        else:
            # å…¼å®¹æ—§æ ¼å¼ï¼š(image_path, messages, label)
            image_path, messages, label = item_data
            dataset_name = "unknown"
            num_classes = None
            
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Image file not found: {image_path}")
            
            # åŠ è½½å¹¶éªŒè¯å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            
            # ç¡®ä¿å›¾åƒä¸ä¸ºç©ºä¸”æœ‰åˆç†çš„å°ºå¯¸
            if image.size[0] == 0 or image.size[1] == 0:
                raise ValueError(f"Invalid image size: {image.size}")
            
            # æ£€æŸ¥å›¾åƒæ¨¡å¼
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            result = {
                "image": image,
                "messages": messages,
                "label": label,
                "dataset_name": dataset_name,
            }
            
            # åªæœ‰åœ¨æœ‰æ•ˆæ—¶æ‰æ·»åŠ num_classes
            if num_classes is not None:
                result["num_classes"] = num_classes
            
            return result
        except Exception as e:
            print(f"Error loading image at index {idx}, path {image_path}: {e}")
            print(f"Image exists: {os.path.exists(image_path) if image_path else 'Path is None'}")
            raise

class MultiDatasetLoader(BaseDataset):
    """
    å¤šæ•°æ®é›†åŠ è½½å™¨ï¼Œæ”¯æŒä»å¤šä¸ªæ–‡ä»¶è¯»å–æ•°æ®ï¼Œshuffleï¼Œä»¥åŠéƒ¨åˆ†è¯„ä¼°
    """
    def __init__(self, jsonl_file_list, dataset_configs=None, shuffle_datasets=True, 
                 eval_ratios=None, is_eval=False, use_partial_eval=True):
        """
        Args:
            jsonl_file_list: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            dataset_configs: æ•°æ®é›†é…ç½®å­—å…¸
            shuffle_datasets: æ˜¯å¦shuffleæ‰€æœ‰æ•°æ®
            eval_ratios: å„æ•°æ®é›†çš„è¯„ä¼°æ¯”ä¾‹å­—å…¸ {dataset_name: ratio}
            is_eval: æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
            use_partial_eval: è¯„ä¼°æ—¶æ˜¯å¦ä½¿ç”¨éƒ¨åˆ†æ•°æ®
        """
        self.dataset_configs = dataset_configs or {}
        self.shuffle_datasets = shuffle_datasets
        self.eval_ratios = eval_ratios or {}
        self.is_eval = is_eval
        self.use_partial_eval = use_partial_eval
        self._original_file_list = jsonl_file_list  # ä¿å­˜åŸå§‹æ–‡ä»¶åˆ—è¡¨
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_data = []
        dataset_stats = {}
        
        for jsonl_file in jsonl_file_list:
            if not os.path.exists(jsonl_file):
                print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {jsonl_file}")
                continue
                
            dataset_data = self._load_single_file(jsonl_file)
            all_data.extend(dataset_data)
            
            # ç»Ÿè®¡å„æ•°æ®é›†çš„æ•°æ®é‡
            for _, _, _, dataset_name, _ in dataset_data:
                if dataset_name not in dataset_stats:
                    dataset_stats[dataset_name] = 0
                dataset_stats[dataset_name] += 1
        
        # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š å¤šæ•°æ®é›†åŠ è½½ç»Ÿè®¡ ({'è¯„ä¼°æ¨¡å¼' if is_eval else 'è®­ç»ƒæ¨¡å¼'}):")
        total_samples = len(all_data)
        for dataset_name, count in dataset_stats.items():
            percentage = count / total_samples * 100 if total_samples > 0 else 0
            eval_ratio = self.eval_ratios.get(dataset_name, 1.0)
            if is_eval and use_partial_eval:
                actual_count = int(count * eval_ratio)
                print(f"  â€¢ {dataset_name}: {count:,} â†’ {actual_count:,} ({eval_ratio:.1%}) samples ({percentage:.1f}%)")
            else:
                print(f"  â€¢ {dataset_name}: {count:,} samples ({percentage:.1f}%)")
        print(f"ğŸ“Š æ€»è®¡: {total_samples:,} samples")
        
        # å¦‚æœæ˜¯è¯„ä¼°æ¨¡å¼ä¸”ä½¿ç”¨éƒ¨åˆ†è¯„ä¼°ï¼ŒæŒ‰æ¯”ä¾‹é‡‡æ ·
        if is_eval and use_partial_eval:
            all_data = self._apply_eval_sampling(all_data)
            print(f"ğŸ“Š éƒ¨åˆ†è¯„ä¼°åæ€»è®¡: {len(all_data):,} samples")
        
        # Shuffleæ•°æ®
        if shuffle_datasets:
            random.shuffle(all_data)
            print("ğŸ”€ æ•°æ®å·²shuffle")
        
        super().__init__(all_data)
    
    def _load_single_file(self, jsonl_file):
        """åŠ è½½å•ä¸ªjsonlæ–‡ä»¶"""
        import json
        data_list = []
        
        with open(jsonl_file, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())
                    if not item:  # è·³è¿‡ç©ºè¡Œ
                        continue
                        
                    img_path = item.get("image_path")
                    label = int(item.get("label"))
                    dataset_name = item.get("dataset_name", "unknown")
                    
                    # ä»é…ç½®ä¸­è·å–è¯¥æ•°æ®é›†çš„ç±»åˆ«æ•°
                    dataset_config = self.dataset_configs.get(dataset_name, {})
                    num_classes = dataset_config.get("num_classes", None)
                    
                    # æ„é€  HF chat-format messages (Qwen2.5-VLæ ¼å¼)
                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {"type": "image", "image": img_path},
                                {"type": "text", "text": f"This is an image of {dataset_name}, what dish is it?"},
                            ],
                        }
                    ]
                    data_list.append((img_path, messages, label, dataset_name, num_classes))
                    
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æé”™è¯¯ {jsonl_file}:{line_num}: {e}")
                except Exception as e:
                    print(f"âš ï¸ æ•°æ®å¤„ç†é”™è¯¯ {jsonl_file}:{line_num}: {e}")
        
        return data_list
    
    def _apply_eval_sampling(self, all_data):
        """å¯¹è¯„ä¼°æ•°æ®æŒ‰æ•°æ®é›†æ¯”ä¾‹é‡‡æ ·"""
        # æŒ‰æ•°æ®é›†åˆ†ç»„
        dataset_groups = {}
        for item in all_data:
            dataset_name = item[3]  # dataset_nameä½ç½®
            if dataset_name not in dataset_groups:
                dataset_groups[dataset_name] = []
            dataset_groups[dataset_name].append(item)
        
        # æŒ‰æ¯”ä¾‹é‡‡æ ·
        sampled_data = []
        for dataset_name, items in dataset_groups.items():
            eval_ratio = self.eval_ratios.get(dataset_name, 1.0)
            sample_count = int(len(items) * eval_ratio)
            
            if sample_count < len(items):
                # éšæœºé‡‡æ ·
                sampled_items = random.sample(items, sample_count)
            else:
                sampled_items = items
            
            sampled_data.extend(sampled_items)
        
        return sampled_data
    
    def get_full_dataset(self):
        """è¿”å›å®Œæ•´æ•°æ®é›†ï¼ˆä¸è¿›è¡Œéƒ¨åˆ†é‡‡æ ·ï¼‰"""
        return MultiDatasetLoader(
            jsonl_file_list=self._original_file_list,
            dataset_configs=self.dataset_configs,
            shuffle_datasets=False,  # å®Œæ•´è¯„ä¼°æ—¶ä¸shuffle
            eval_ratios=self.eval_ratios,
            is_eval=True,
            use_partial_eval=False  # ä¸ä½¿ç”¨éƒ¨åˆ†è¯„ä¼°
        )

class MyFoodDataset(BaseDataset):
    def __init__(self, split_file, dataset_configs=None):
        """
        Args:
            split_file: jsonlæ–‡ä»¶è·¯å¾„
            dataset_configs: æ•°æ®é›†é…ç½®å­—å…¸ï¼Œæ ¼å¼ä¸º {dataset_name: {"num_classes": int, ...}}
        """
        self.dataset_configs = dataset_configs or {}
        data_list = []
        import json
        # æ¯è¡Œæ˜¯ JSON æ ¼å¼ï¼ŒåŒ…å«"image_path"ã€"label"å’Œå¯é€‰çš„"dataset_name"é”®
        with open(split_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                img_path = item.get("image_path")
                label = int(item.get("label"))
                dataset_name = item.get("dataset_name", "unknown")
                
                # ä»é…ç½®ä¸­è·å–è¯¥æ•°æ®é›†çš„ç±»åˆ«æ•°
                dataset_config = self.dataset_configs.get(dataset_name, {})
                num_classes = dataset_config.get("num_classes", None)
                
                # æ„é€  HF chat-format messages (Qwen2.5-VLæ ¼å¼)
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": img_path},
                            {"type": "text", "text": f"This is an image of {dataset_name}, what dish is it?"},
                        ],
                    }
                ]
                data_list.append((img_path, messages, label, dataset_name, num_classes))
        super().__init__(data_list)
