#!/usr/bin/env python3
"""
å®Œæ•´æŒ‡æ ‡æ˜¾ç¤ºæµ‹è¯• - ç¡®ä¿trainingã€perfã€evalæŒ‡æ ‡éƒ½èƒ½æ­£å¸¸æ˜¾ç¤º
"""

import os
import time
import json
import torch
import wandb
from typing import Dict

# æ¨¡æ‹Ÿé…ç½®
config = {
    'output_dir': './test_output',
    'wandb': {
        'enabled': True,
        'project': 'qwen_classify_test',
        'run_name': 'all_metrics_test',
        'tags': ['test', 'all_metrics']
    },
    'monitor': {
        'freq': {
            'all_freq': 1  # æ¯æ­¥éƒ½è®°å½•æ‰€æœ‰æŒ‡æ ‡ï¼Œç¡®ä¿èƒ½çœ‹åˆ°
        }
    },
    'model': {
        'max_sequence_length': 512
    },
    'deepspeed': {
        'train_batch_size': 32
    }
}

class SimpleTrainingMonitor:
    """ç®€åŒ–çš„è®­ç»ƒç›‘æ§å™¨ï¼Œç¡®ä¿æ‰€æœ‰æŒ‡æ ‡éƒ½èƒ½æ˜¾ç¤º"""
    
    def __init__(self, output_dir: str, config: Dict):
        self.output_dir = output_dir
        self.config = config
        self.start_time = time.time()
        self.step_start_time = time.time()
        
        # åˆå§‹åŒ–wandb
        self._init_wandb()
        
        # è®¾ç½®ç›‘æ§é¢‘ç‡ - æ¯æ­¥éƒ½è®°å½•
        self.freq = {
            'training_log_freq': 1,
            'perf_log_freq': 1,
            'gpu_log_freq': 1,
        }
        
        # æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°
        self.batch_size = 32
        self.seq_length = 512
        
    def _init_wandb(self):
        """åˆå§‹åŒ–wandb"""
        try:
            wandb.init(
                project=self.config['wandb']['project'],
                name=self.config['wandb']['run_name'],
                tags=self.config['wandb']['tags'],
                config=self.config
            )
            
            # å®šä¹‰æ‰€æœ‰æŒ‡æ ‡ç»„
            wandb.define_metric("step")
            wandb.define_metric("training/*", step_metric="step")
            wandb.define_metric("perf/*", step_metric="step")
            wandb.define_metric("eval/*", step_metric="step")
            
            print("âœ… WandBåˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸ“Š é¡¹ç›®: {wandb.run.project}")
            print(f"ğŸ”— è¿è¡Œ: {wandb.run.name}")
            print(f"ğŸš€ æŸ¥çœ‹åœ°å€: {wandb.run.url}")
            
        except Exception as e:
            print(f"âŒ WandBåˆå§‹åŒ–å¤±è´¥: {e}")
            return
    
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float):
        """è®°å½•è®­ç»ƒæ­¥éª¤ - æ¯æ­¥éƒ½è®°å½•æ‰€æœ‰æŒ‡æ ‡"""
        current_time = time.time()
        step_time = current_time - self.step_start_time
        
        # å‡†å¤‡æ‰€æœ‰æŒ‡æ ‡æ•°æ®
        wandb_data = {
            # ç»Ÿä¸€çš„stepå­—æ®µ
            "step": int(step),
            
            # Trainingç»„æŒ‡æ ‡
            "training/loss": float(loss),
            "training/lr": float(learning_rate),
            "training/epoch": float(epoch),
            "training/grad_norm": float(grad_norm),
            
            # Perfç»„æŒ‡æ ‡
            "perf/step_time": float(step_time),
            "perf/steps_per_second": float(1.0 / step_time) if step_time > 0 else 0.0,
            "perf/tokens_per_second": float(self.batch_size * self.seq_length / step_time) if step_time > 0 else 0.0,
            "perf/samples_per_second": float(self.batch_size / step_time) if step_time > 0 else 0.0,
        }
        
        # æ·»åŠ GPUæŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            try:
                memory_allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
                memory_reserved = torch.cuda.memory_reserved() / (1024**3)    # GB
                memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                memory_utilization = (memory_allocated / memory_total) * 100
                
                wandb_data.update({
                    "perf/gpu_memory_allocated_gb": float(memory_allocated),
                    "perf/gpu_memory_reserved_gb": float(memory_reserved),
                    "perf/gpu_memory_utilization_percent": float(memory_utilization),
                })
            except Exception as e:
                print(f"âš ï¸ GPUæŒ‡æ ‡è·å–å¤±è´¥: {e}")
        
        # è®°å½•åˆ°wandb
        try:
            wandb.log(wandb_data, step=int(step), commit=True)
            print(f"âœ… Step {step}: å·²è®°å½• {len(wandb_data)} ä¸ªæŒ‡æ ‡")
            print(f"   Training: {[k for k in wandb_data.keys() if k.startswith('training/')]}")
            print(f"   Perf: {[k for k in wandb_data.keys() if k.startswith('perf/')]}")
        except Exception as e:
            print(f"âŒ è®°å½•æŒ‡æ ‡å¤±è´¥: {e}")
        
        self.step_start_time = current_time
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        eval_data = {
            "step": int(step),
            "eval/overall_loss": float(eval_loss),
            "eval/overall_accuracy": float(eval_accuracy),
        }
        
        try:
            wandb.log(eval_data, step=int(step), commit=True)
            print(f"âœ… Eval Step {step}: å·²è®°å½•è¯„ä¼°æŒ‡æ ‡")
            print(f"   Eval: {list(eval_data.keys())}")
        except Exception as e:
            print(f"âŒ è®°å½•è¯„ä¼°æŒ‡æ ‡å¤±è´¥: {e}")
    
    def finish(self):
        """ç»“æŸè®­ç»ƒ"""
        if wandb.run is not None:
            total_time = time.time() - self.start_time
            wandb.log({"training/total_time": total_time}, commit=True)
            wandb.finish()
            print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´æŒ‡æ ‡æ˜¾ç¤ºæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = SimpleTrainingMonitor('./test_output', config)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    total_steps = 20
    eval_steps = 5
    
    print(f"ğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: {total_steps}, è¯„ä¼°é—´éš”: {eval_steps}")
    print("=" * 60)
    
    for step in range(1, total_steps + 1):
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        epoch = step / 10.0
        loss = 2.0 - (step * 0.05) + (torch.rand(1).item() * 0.1)  # é€’å‡çš„æŸå¤±
        grad_norm = 0.5 + (torch.rand(1).item() * 0.3)  # éšæœºæ¢¯åº¦èŒƒæ•°
        learning_rate = 1e-5 * (0.95 ** (step // 10))  # é€’å‡çš„å­¦ä¹ ç‡
        
        # è®°å½•è®­ç»ƒæ­¥éª¤
        monitor.log_step(step, epoch, loss, grad_norm, learning_rate)
        
        # æ¯eval_stepsæ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
        if step % eval_steps == 0:
            # æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®
            eval_loss = loss * 0.8 + (torch.rand(1).item() * 0.1)  # è¯„ä¼°æŸå¤±ç•¥ä½äºè®­ç»ƒæŸå¤±
            eval_accuracy = 0.3 + (step * 0.02) + (torch.rand(1).item() * 0.05)  # é€’å¢çš„å‡†ç¡®ç‡
            eval_accuracy = min(eval_accuracy, 0.95)  # é™åˆ¶åœ¨95%ä»¥å†…
            
            monitor.log_evaluation(step, eval_loss, eval_accuracy)
        
        # çŸ­æš‚æš‚åœï¼Œæ¨¡æ‹Ÿå®é™…è®­ç»ƒ
        time.sleep(0.1)
    
    # ç»“æŸè®­ç»ƒ
    monitor.finish()
    
    print("=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š è¯·åœ¨WandBç•Œé¢ä¸­æŸ¥çœ‹ä»¥ä¸‹æŒ‡æ ‡ç»„ï¼š")
    print("   â€¢ training/* - è®­ç»ƒæŒ‡æ ‡")
    print("   â€¢ perf/* - æ€§èƒ½æŒ‡æ ‡") 
    print("   â€¢ eval/* - è¯„ä¼°æŒ‡æ ‡")
    print("   â€¢ system/* - ç³»ç»ŸæŒ‡æ ‡ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰")
    print("ğŸ”— å¦‚æœæŒ‡æ ‡æ²¡æœ‰ç«‹å³æ˜¾ç¤ºï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–ç­‰å¾…å‡ ç§’é’Ÿ")

if __name__ == "__main__":
    main() 