#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen2.5-VLå¤šæ•°æ®é›†åˆ†ç±»å¤šGPUè®­ç»ƒè„šæœ¬
"""

import os
import sys
import argparse
import yaml
import torch
import deepspeed
import random
import numpy as np

# Add project root to Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

def set_random_seeds(seed=42):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

from data.dataloader import create_dataloaders
from models.qwen2_5_vl_classify import Qwen2_5_VLForImageClassification
from optimizer.optimizer import create_optimizer
from training.deepspeed_trainer import DeepSpeedTrainer
from training.lr_scheduler import create_lr_scheduler

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Qwen2.5-VLé£Ÿç‰©åˆ†ç±»å¤šGPUè®­ç»ƒ")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--local_rank", type=int, default=-1, help="æœ¬åœ°è¿›ç¨‹æ’å")
    parser.add_argument("--resume_from", type=str, help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    
    # æ”¯æŒDeepSpeedå‚æ•° (åŒ…å« --deepspeed_config)
    parser = deepspeed.add_config_arguments(parser)
    return parser.parse_args()

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def setup_model(config):
    """è®¾ç½®æ¨¡å‹"""
    # è·å–æŸå¤±å‡½æ•°é…ç½®
    loss_config = config.get('loss', {'type': 'cross_entropy'})
    
    # è·å–å¤šæ•°æ®é›†é…ç½®
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    enable_logits_masking = config.get('datasets', {}).get('enable_logits_masking', True)
    
    # æ‰“å°æŸå¤±å‡½æ•°ä¿¡æ¯ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    try:
        import torch.distributed as dist
        is_distributed = dist.is_available() and dist.is_initialized()
        if is_distributed:
            current_rank = dist.get_rank()
        else:
            current_rank = 0
        should_print = not is_distributed or current_rank == 0
    except:
        should_print = True
    
    if should_print:
        print(f"ğŸ¯ ä½¿ç”¨æŸå¤±å‡½æ•°: {loss_config.get('type', 'cross_entropy')}")
        if loss_config.get('type') != 'cross_entropy':
            print(f"  æŸå¤±å‡½æ•°å‚æ•°: {loss_config}")
        
        # æ‰“å°å¤šæ•°æ®é›†é…ç½®ä¿¡æ¯
        if dataset_configs:
            print(f"ğŸ—‚ï¸ å¤šæ•°æ®é›†æ¨¡å¼å·²å¯ç”¨:")
            print(f"  â€¢ æ•°æ®é›†æ•°é‡: {len(dataset_configs)}")
            print(f"  â€¢ Logits Masking: {'å¯ç”¨' if enable_logits_masking else 'ç¦ç”¨'}")
            for dataset_name, dataset_config in dataset_configs.items():
                num_classes = dataset_config.get('num_classes', 'N/A')
                print(f"  â€¢ {dataset_name}: {num_classes} classes")
    
    model = Qwen2_5_VLForImageClassification(
        pretrained_model_name=config['model']['pretrained_name'],
        num_labels=config['model']['num_labels'],
        loss_config=loss_config,
        dataset_configs=dataset_configs,
        enable_logits_masking=enable_logits_masking
    )
    
    # å¦‚æœæœ‰é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ŒåŠ è½½å®ƒ
    if config.get('pretrained_checkpoint'):
        print(f"åŠ è½½é¢„è®­ç»ƒæ£€æŸ¥ç‚¹: {config['pretrained_checkpoint']}")
        checkpoint = torch.load(config['pretrained_checkpoint'], map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    
    return model

def print_training_info(config, train_loader, val_loader):
    """æ‰“å°è®­ç»ƒä¿¡æ¯"""
    # è·å–DeepSpeedé…ç½®
    deepspeed_config = config.get('deepspeed', {})
    if isinstance(deepspeed_config, str):
        import json
        with open(deepspeed_config, 'r') as f:
            deepspeed_config = json.load(f)
    
    gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
    micro_batch_size = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
    train_batch_size = deepspeed_config.get('train_batch_size', 32)
    
    # è®¡ç®—æœ‰æ•ˆæ­¥æ•°
    dataloader_steps_per_epoch = len(train_loader)
    effective_steps_per_epoch = dataloader_steps_per_epoch // gradient_accumulation_steps
    total_effective_steps = effective_steps_per_epoch * config['training']['num_epochs']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ•°æ®é›†æ¨¡å¼
    is_multi_dataset = 'train_jsonl_list' in config.get('data', {})
    dataset_configs = config.get('datasets', {}).get('dataset_configs', {})
    
    print("=" * 80)
    if is_multi_dataset:
        print("ğŸš€ Qwen2.5-VLå¤šæ•°æ®é›†åˆ†ç±»å¤šGPUè®­ç»ƒ")
    else:
        print("ğŸš€ Qwen2.5-VLåˆ†ç±»å¤šGPUè®­ç»ƒ")
    print("=" * 80)
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_loader.dataset):,}")
    print(f"ğŸ“Š éªŒè¯æ•°æ®é›†å¤§å°: {len(val_loader.dataset):,}")
    print(f"ğŸ¯ åˆ†ç±»å¤´å¤§å°: {config['model']['num_labels']}")
    
    if is_multi_dataset and dataset_configs:
        print(f"ğŸ—‚ï¸ å¤šæ•°æ®é›†é…ç½®:")
        for dataset_name, dataset_config in dataset_configs.items():
            num_classes = dataset_config.get('num_classes', 'N/A')
            eval_ratio = dataset_config.get('eval_ratio', 'N/A')
            print(f"  â€¢ {dataset_name}: {num_classes} classes (eval: {eval_ratio})")
    
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {config['training']['num_epochs']}")
    print()
    print("ğŸ“¦ æ‰¹æ¬¡é…ç½®:")
    print(f"  â€¢ æ¯GPUå¾®æ‰¹æ¬¡å¤§å°: {micro_batch_size}")
    print(f"  â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"  â€¢ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print()
    print("ğŸ“ˆ æ­¥æ•°ç»Ÿè®¡:")
    print(f"  â€¢ DataLoaderæ­¥æ•°æ¯epoch: {dataloader_steps_per_epoch:,}")
    print(f"  â€¢ æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch: {effective_steps_per_epoch:,}")
    print(f"  â€¢ æ€»æœ‰æ•ˆè®­ç»ƒæ­¥æ•°: {total_effective_steps:,}")
    print()
    print("ğŸ“ è®­ç»ƒé…ç½®:")
    print(f"  â€¢ æ—¥å¿—æ­¥æ•°: {config['logging_steps']}")
    print(f"  â€¢ ä¿å­˜æ­¥æ•°: {config['save_steps']}")
    print(f"  â€¢ è¯„ä¼°æ­¥æ•°: {config['eval_steps']}")
    print(f"  â€¢ è¾“å‡ºç›®å½•: {config['output_dir']}")
    
    # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹è¿½è¸ªé…ç½®
    best_model_config = config.get('training', {}).get('best_model_tracking', {})
    if best_model_config.get('enabled', True):
        print(f"ğŸ† æœ€ä½³æ¨¡å‹è¿½è¸ª:")
        print(f"  â€¢ è¿½è¸ªæŒ‡æ ‡: {best_model_config.get('metric', 'overall_accuracy')}")
        print(f"  â€¢ ä¼˜åŒ–æ¨¡å¼: {best_model_config.get('mode', 'max')}")
    
    # æ˜¾ç¤ºè¯„ä¼°é…ç½®
    eval_config = config.get('training', {}).get('evaluation', {})
    if is_multi_dataset:
        print(f"ğŸ” è¯„ä¼°é…ç½®:")
        print(f"  â€¢ è®­ç»ƒè¿‡ç¨‹ä¸­éƒ¨åˆ†è¯„ä¼°: {eval_config.get('partial_eval_during_training', True)}")
        print(f"  â€¢ è®­ç»ƒç»“æŸåå®Œæ•´è¯„ä¼°: {eval_config.get('full_eval_at_end', True)}")
        print(f"  â€¢ ä»…è¯„ä¼°æœ€ä½³æ¨¡å‹: {eval_config.get('eval_best_model_only', True)}")
    
    print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_random_seeds(42)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (DeepSpeedä¼šå¤„ç†è¿™ä¸ª)
    deepspeed.init_distributed()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„
    config['deepspeed'] = args.deepspeed_config
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config['training']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œå°†output_diræå‡åˆ°æ ¹å±‚çº§
    config['output_dir'] = output_dir
    
    # æå‰å‡†å¤‡é…ç½®å‚æ•°ï¼ˆå‚æ•°æ˜ å°„ç­‰ï¼‰
    from training.utils.config_utils import prepare_config
    config = prepare_config(config)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_model(config)
    
    # è®¾ç½®æ•°æ®åŠ è½½å™¨ï¼ˆç°åœ¨åˆ†å¸ƒå¼ç¯å¢ƒå·²ç»åˆå§‹åŒ–ï¼‰
    train_loader, val_loader = create_dataloaders(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆè¿™é‡Œä¼šè°ƒç”¨prepare_configï¼‰
    trainer = DeepSpeedTrainer(config)
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆåœ¨prepare_configä¹‹åï¼‰
    if args.local_rank <= 0:
        print_training_info(config, train_loader, val_loader)
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config)
    
    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = create_lr_scheduler(optimizer, config, len(train_loader))
    
    # è®¾ç½®è®­ç»ƒå™¨
    trainer.setup_model(model, train_loader, val_loader, optimizer, lr_scheduler)
    
    # å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒ
    if args.resume_from:
        print(f"æ¢å¤è®­ç»ƒä»: {args.resume_from}")
        trainer.load_checkpoint(args.resume_from)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main() 