import torch
import time
from typing import Dict, Tuple
from tqdm import tqdm
from collections import defaultdict

def evaluate_model(model, val_loader, device) -> Tuple[float, float]:
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹æ­£ç¡®èšåˆæ‰€æœ‰GPUçš„ç»“æœ"""
    import torch.distributed as dist
    
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    # æ£€æŸ¥åˆ†å¸ƒå¼çŠ¶æ€
    is_distributed = dist.is_available() and dist.is_initialized()
    if is_distributed:
        current_rank = dist.get_rank()
    else:
        current_rank = 0
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    show_progress = not is_distributed or current_rank == 0
    eval_pbar = tqdm(val_loader, desc="Evaluating", leave=False, disable=not show_progress)
    
    batch_count = 0  # ç”¨äºè®¡ç®—å¹³å‡æŸå¤±
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(eval_pbar):
            try:
                batch_count += 1
                
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                inputs = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                pixel_values = batch["pixel_values"].to(device)
                labels = batch["labels"].to(device)
                
                # å‰å‘ä¼ æ’­
                forward_kwargs = {
                    "input_ids": inputs,
                    "attention_mask": attention_mask,
                    "pixel_values": pixel_values,
                    "labels": labels
                }
                
                # æ£€æŸ¥å¹¶æ·»åŠ image_grid_thwå‚æ•°
                if "image_grid_thw" in batch:
                    forward_kwargs["image_grid_thw"] = batch["image_grid_thw"].to(device)
                
                outputs = model(**forward_kwargs)
                
                # è®¡ç®—æŸå¤±
                loss = outputs.loss
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = torch.argmax(outputs.logits, dim=-1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
                
                # æ¯10æ­¥æˆ–æœ€åä¸€æ­¥æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                if show_progress and (batch_idx % 10 == 0 or batch_idx == len(val_loader) - 1):
                    current_accuracy = correct / total if total > 0 else 0
                    current_avg_loss = total_loss / batch_count
                    eval_pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{current_avg_loss:.4f}',
                        'accuracy': f'{current_accuracy:.4f}',
                        'samples': f'{total}'
                    })
                
            except Exception as e:
                if show_progress:
                    print(f"âŒ è¯„ä¼°æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    eval_pbar.set_postfix({'error': f'batch_{batch_idx}_failed'})
                continue
    
    # å…³é—­è¿›åº¦æ¡
    eval_pbar.close()
    
    # åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹èšåˆæ‰€æœ‰GPUçš„ç»“æœ
    if is_distributed:
        # è½¬æ¢ä¸ºtensorè¿›è¡Œèšåˆ
        total_loss_tensor = torch.tensor(total_loss, dtype=torch.float32, device=device)
        correct_tensor = torch.tensor(correct, dtype=torch.long, device=device) 
        total_tensor = torch.tensor(total, dtype=torch.long, device=device)
        batch_count_tensor = torch.tensor(batch_count, dtype=torch.long, device=device)
        
        # èšåˆæ‰€æœ‰GPUçš„ç»“æœ
        dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(batch_count_tensor, op=dist.ReduceOp.SUM)
        
        # è®¡ç®—å…¨å±€å¹³å‡å€¼
        global_avg_loss = total_loss_tensor.item() / batch_count_tensor.item() if batch_count_tensor.item() > 0 else 0
        global_accuracy = correct_tensor.item() / total_tensor.item() if total_tensor.item() > 0 else 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°å…¨å±€ç»“æœ
        if current_rank == 0:
            print("\n" + "="*80)
            print("ğŸ“Š éªŒè¯é›†è¯„ä¼°ç»“æœ (å…¨å±€èšåˆ)")
            print("="*80)
            print(f"ğŸ“ˆ Average Loss:     {global_avg_loss:.6f}")
            print(f"ğŸ¯ Accuracy:         {global_accuracy:.4f} ({global_accuracy*100:.2f}%)")
            print(f"ğŸ“Š Total Samples:    {total_tensor.item():,}")
            print(f"âœ… Correct Samples:  {correct_tensor.item():,}")
            print(f"âŒ Wrong Samples:    {total_tensor.item() - correct_tensor.item():,}")
            print("="*80 + "\n")
        
        return global_avg_loss, global_accuracy
    else:
        # å•GPUæ¨¡å¼
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        accuracy = correct / total if total > 0 else 0
        print("\n" + "="*80)
        print("ğŸ“Š éªŒè¯é›†è¯„ä¼°ç»“æœ")
        print("="*80)
        print(f"ğŸ“ˆ Average Loss:     {avg_loss:.6f}")
        print(f"ğŸ¯ Accuracy:         {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"ğŸ“Š Total Samples:    {total:,}")
        print(f"âœ… Correct Samples:  {correct:,}")
        print(f"âŒ Wrong Samples:    {total - correct:,}")
        print("="*80 + "\n")
        return avg_loss, accuracy

def evaluate_multi_dataset(model, val_loader, device, dataset_configs=None) -> Dict:
    """
    å¤šæ•°æ®é›†è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒæŒ‰æ•°æ®é›†åˆ†åˆ«ç»Ÿè®¡æŒ‡æ ‡
    
    Args:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        dataset_configs: æ•°æ®é›†é…ç½®å­—å…¸
        
    Returns:
        evaluation results dictionary containing overall and per-dataset metrics
    """
    import torch.distributed as dist
    
    model.eval()
    
    # æ£€æŸ¥åˆ†å¸ƒå¼çŠ¶æ€
    is_distributed = dist.is_available() and dist.is_initialized()
    if is_distributed:
        current_rank = dist.get_rank()
    else:
        current_rank = 0
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    show_progress = not is_distributed or current_rank == 0
    eval_pbar = tqdm(val_loader, desc="Multi-Dataset Evaluation", leave=False, disable=not show_progress)
    
    # æ•´ä½“ç»Ÿè®¡
    total_loss = 0
    correct = 0
    total = 0
    batch_count = 0
    
    # æŒ‰æ•°æ®é›†ç»Ÿè®¡
    dataset_stats = defaultdict(lambda: {
        'total_loss': 0.0,
        'correct': 0,
        'total': 0,
        'batch_count': 0
    })
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(eval_pbar):
            try:
                batch_count += 1
                
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                inputs = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                pixel_values = batch["pixel_values"].to(device)
                labels = batch["labels"].to(device)
                
                # è·å–æ•°æ®é›†ä¿¡æ¯
                dataset_names = batch.get("dataset_names", [])
                num_classes_list = batch.get("num_classes_list", [])
                
                # å‰å‘ä¼ æ’­
                forward_kwargs = {
                    "input_ids": inputs,
                    "attention_mask": attention_mask,
                    "pixel_values": pixel_values,
                    "labels": labels
                }
                
                # æ£€æŸ¥å¹¶æ·»åŠ image_grid_thwå‚æ•°
                if "image_grid_thw" in batch:
                    forward_kwargs["image_grid_thw"] = batch["image_grid_thw"].to(device)
                
                # æ·»åŠ å¤šæ•°æ®é›†æ”¯æŒçš„å‚æ•°
                if dataset_names:
                    forward_kwargs["dataset_names"] = dataset_names
                if num_classes_list:
                    forward_kwargs["num_classes_list"] = num_classes_list
                
                outputs = model(**forward_kwargs)
                
                # è®¡ç®—æŸå¤±å’Œé¢„æµ‹
                loss = outputs.loss
                predictions = torch.argmax(outputs.logits, dim=-1)
                
                # æ›´æ–°æ•´ä½“ç»Ÿè®¡
                total_loss += loss.item()
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
                
                # æ›´æ–°å„æ•°æ®é›†ç»Ÿè®¡
                for i, dataset_name in enumerate(dataset_names):
                    if i >= len(labels) or i >= len(predictions):
                        continue
                        
                    label = labels[i].item()
                    pred = predictions[i].item()
                    
                    dataset_stats[dataset_name]['total_loss'] += loss.item() / len(dataset_names)
                    dataset_stats[dataset_name]['total'] += 1
                    dataset_stats[dataset_name]['batch_count'] += 1
                    
                    if pred == label:
                        dataset_stats[dataset_name]['correct'] += 1
                
                # æ¯10æ­¥æˆ–æœ€åä¸€æ­¥æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                if show_progress and (batch_idx % 10 == 0 or batch_idx == len(val_loader) - 1):
                    current_accuracy = correct / total if total > 0 else 0
                    current_avg_loss = total_loss / batch_count
                    eval_pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{current_avg_loss:.4f}',
                        'accuracy': f'{current_accuracy:.4f}',
                        'datasets': f'{len(dataset_stats)}'
                    })
                
            except Exception as e:
                if show_progress:
                    print(f"âŒ å¤šæ•°æ®é›†è¯„ä¼°æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    eval_pbar.set_postfix({'error': f'batch_{batch_idx}_failed'})
                continue
    
    # å…³é—­è¿›åº¦æ¡
    eval_pbar.close()
    
    # è®¡ç®—ç»“æœ
    overall_avg_loss = total_loss / batch_count if batch_count > 0 else 0
    overall_accuracy = correct / total if total > 0 else 0
    
    # è®¡ç®—å„æ•°æ®é›†çš„æŒ‡æ ‡
    dataset_metrics = {}
    for dataset_name, stats in dataset_stats.items():
        if stats['total'] > 0:
            avg_loss = stats['total_loss'] / stats['batch_count'] if stats['batch_count'] > 0 else 0
            accuracy = stats['correct'] / stats['total']
            dataset_metrics[dataset_name] = {
                'loss': avg_loss,
                'accuracy': accuracy,
                'samples': stats['total'],
                'correct': stats['correct']
            }
    
    # åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹èšåˆç»“æœ
    if is_distributed:
        # TODO: æ·»åŠ åˆ†å¸ƒå¼èšåˆé€»è¾‘
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç±»ä¼¼å•æ•°æ®é›†è¯„ä¼°çš„åˆ†å¸ƒå¼èšåˆä»£ç 
        pass
    
    # è¾“å‡ºç»“æœ
    if show_progress:
        print("\n" + "="*80)
        print("ğŸ“Š å¤šæ•°æ®é›†è¯„ä¼°ç»“æœ")
        print("="*80)
        print(f"ğŸ“ˆ Overall Loss:     {overall_avg_loss:.6f}")
        print(f"ğŸ¯ Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
        print(f"ğŸ“Š Total Samples:    {total:,}")
        print(f"âœ… Total Correct:    {correct:,}")
        print()
        
        for dataset_name, metrics in dataset_metrics.items():
            print(f"ğŸ“‚ {dataset_name}:")
            print(f"  â€¢ Loss:     {metrics['loss']:.6f}")
            print(f"  â€¢ Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            print(f"  â€¢ Samples:  {metrics['samples']:,} (Correct: {metrics['correct']:,})")
            
        print("="*80 + "\n")
    
    return {
        'overall_loss': overall_avg_loss,
        'overall_accuracy': overall_accuracy,
        'dataset_metrics': dataset_metrics,
        'total_samples': total,
        'total_correct': correct
    } 