import time
import json
import os
import torch
import psutil
from typing import Dict, List, Optional

# æ·»åŠ wandbæ”¯æŒ
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not installed. pip install wandb to enable logging.")

def make_json_serializable(obj):
    """ç¡®ä¿å¯¹è±¡å¯ä»¥JSONåºåˆ—åŒ–"""
    if isinstance(obj, torch.Tensor):
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif torch.is_tensor(obj):
        # å¤„ç†å…¶ä»–torch tensorç±»å‹
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif hasattr(obj, 'dtype') and 'torch' in str(type(obj)):
        # å¤„ç†torchçš„æ ‡é‡ç±»å‹
        if 'float' in str(obj.dtype):
            return float(obj)
        elif 'int' in str(obj.dtype):
            return int(obj)
        else:
            return float(obj)
    elif isinstance(obj, (float, int)):
        return obj
    elif isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    else:
        return obj

def get_gpu_peak_flops():
    """è·å–GPUå³°å€¼FLOPsæ€§èƒ½"""
    try:
        if not torch.cuda.is_available():
            return 312e12  # é»˜è®¤å€¼
        
        # è·å–GPUåç§°
        gpu_name = torch.cuda.get_device_name(0).upper()
        
        # ä¸åŒGPUçš„å³°å€¼æ€§èƒ½ (TFLOPs for FP16/BF16)
        gpu_peak_flops = {
            # NVIDIA A100ç³»åˆ—
            'A100': 312e12,    # A100 80GB
            'A100-SXM': 312e12,
            'A100-PCIE': 312e12,
            # NVIDIA A800ç³»åˆ— (é’ˆå¯¹ä¸­å›½å¸‚åœºçš„A100å˜ä½“)
            'A800': 280e12,    # A800 80GB (ç¨ä½äºA100)
            'A800-SXM': 280e12,
            'A800-PCIE': 280e12,
            # NVIDIA H100ç³»åˆ—  
            'H100': 989e12,    # H100 80GB
            'H100-SXM': 989e12,
            'H100-PCIE': 756e12,
            # NVIDIA H800ç³»åˆ— (é’ˆå¯¹ä¸­å›½å¸‚åœºçš„H100å˜ä½“)
            'H800': 850e12,    # H800 80GB (ç¨ä½äºH100)
            'H800-SXM': 850e12,
            'H800-PCIE': 700e12,
            # NVIDIA V100ç³»åˆ—
            'V100': 112e12,    # V100 32GB
            'V100-SXM': 112e12,
            'V100-PCIE': 112e12,
            # NVIDIA RTXç³»åˆ—
            'RTX 4090': 165e12,
            'RTX 4080': 112e12,
            'RTX 3090': 71e12,
            'RTX 3080': 58e12,
            # NVIDIA T4
            'T4': 65e12,
            # NVIDIA L4
            'L4': 121e12,
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„GPU
        for gpu_model, peak_flops in gpu_peak_flops.items():
            if gpu_model in gpu_name:
                # åªåœ¨ç¬¬ä¸€æ¬¡è¯†åˆ«æ—¶æ‰“å°ï¼ˆé¿å…é¢‘ç¹è¾“å‡ºï¼‰
                if not hasattr(get_gpu_peak_flops, '_gpu_identified'):
                    print(f"âœ… è¯†åˆ«GPU: {gpu_name} -> {gpu_model} ({peak_flops/1e12:.0f} TFLOPs)")
                    get_gpu_peak_flops._gpu_identified = True
                return peak_flops
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„GPUï¼Œä½¿ç”¨é»˜è®¤å€¼
        print(f"æœªè¯†åˆ«çš„GPUç±»å‹: {gpu_name}ï¼Œä½¿ç”¨é»˜è®¤å³°å€¼æ€§èƒ½ (A100: 312 TFLOPs)")
        return 312e12  # é»˜è®¤ä½¿ç”¨A100çš„æ€§èƒ½
        
    except Exception as e:
        print(f"è·å–GPUå³°å€¼æ€§èƒ½é”™è¯¯: {e}")
        return 312e12  # é»˜è®¤å€¼

def calculate_mfu(model, batch_size: int, seq_length: int, step_time: float, actual_flops: float = None) -> float:
    """è®¡ç®—MFU (Model FLOPs Utilization)
    
    MFU = å®é™…FLOPs/s / GPUå³°å€¼FLOPs/s
    
    å‚æ•°:
        model: æ¨¡å‹å®ä¾‹
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_length: å®é™…åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokens + text tokensï¼‰
        step_time: æ­¥éª¤è€—æ—¶ï¼ˆç§’ï¼‰
        actual_flops: å®é™…æµ‹é‡çš„FLOPsï¼ˆåŒ…å«å‰å‘+åå‘ä¼ æ’­ï¼‰
    
    è¿”å›:
        mfu: Model FLOPs Utilization (0-1ä¹‹é—´çš„å€¼)
    """
    try:
        if actual_flops is None:
            # å¦‚æœæ²¡æœ‰æä¾›å®é™…FLOPsï¼Œè¿”å›0
            return 0.0
        
        # è®¡ç®—å®é™…FLOPs/s
        actual_flops_per_second = actual_flops / step_time
        
        # åŠ¨æ€è·å–GPUå³°å€¼æ€§èƒ½
        peak_flops_per_second = get_gpu_peak_flops()
        
        # è®¡ç®—MFU
        mfu = actual_flops_per_second / peak_flops_per_second
        return min(mfu, 1.0)  # é™åˆ¶åœ¨100%ä»¥å†…
        
    except Exception as e:
        print(f"MFUè®¡ç®—é”™è¯¯: {e}")
        return 0.0

def profile_model_flops(model, batch_example: Dict) -> float:
    """ä½¿ç”¨PyTorch profilerè·å–æ¨¡å‹å®é™…FLOPsï¼ˆåŒ…æ‹¬å‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
        model.train()
        
        # è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰
        actual_seq_length = _get_actual_sequence_length(model, batch_example)
        
        # åˆ†åˆ«æµ‹é‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„FLOPs
        forward_flops = _profile_forward_flops(model, batch_example)
        backward_flops = _profile_backward_flops(model, batch_example)
        
        total_flops = forward_flops + backward_flops
        
        if total_flops > 0:
            print(f"æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
            print(f"å®é™…åºåˆ—é•¿åº¦(åŒ…å«visual tokens): {actual_seq_length}")
            print(f"å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
            print(f"åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
            print(f"æ€»FLOPs: {total_flops:.2e}")
        else:
            print("æ— æ³•é€šè¿‡profileræµ‹é‡FLOPsï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            total_flops = _estimate_flops_fallback(model, batch_example, actual_seq_length)
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPs profilingé”™è¯¯: {e}")
        # å°è¯•è·å–åºåˆ—é•¿åº¦ç”¨äºä¼°ç®—
        try:
            actual_seq_length = _get_actual_sequence_length(model, batch_example)
            return _estimate_flops_fallback(model, batch_example, actual_seq_length)
        except:
            return _estimate_flops_fallback(model, batch_example)

def _profile_forward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡å‰å‘ä¼ æ’­çš„FLOPs"""
    try:
        model.eval()  # ä½¿ç”¨evalæ¨¡å¼é¿å…dropoutç­‰å½±å“FLOPsè®¡ç®—
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                with torch.no_grad():
                    # ä»…æ‰§è¡Œå‰å‘ä¼ æ’­
                    outputs = model(**batch_example)
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            return 0.0
        
    except Exception as e:
        print(f"å‰å‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _profile_backward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡åå‘ä¼ æ’­çš„FLOPs"""
    try:
        model.train()  # è®­ç»ƒæ¨¡å¼
        
        # å…ˆæ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆä¸åœ¨profilerä¸­ï¼‰
        outputs = model(**batch_example)
        loss = outputs.loss
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            # æµ‹é‡åå‘ä¼ æ’­çš„FLOPs
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                # ä»…æ‰§è¡Œåå‘ä¼ æ’­
                loss.backward()
            
            # æ¸…ç†æ¢¯åº¦
            model.zero_grad()
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            model.zero_grad()  # ç¡®ä¿æ¸…ç†æ¢¯åº¦
            return 0.0
        
    except Exception as e:
        print(f"åå‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _get_actual_sequence_length(model, batch_example: Dict) -> int:
    """è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰"""
    try:
        # ä¸´æ—¶è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼ä»¥è·å–è¾“å‡ºshape
        model.eval()
        
        with torch.no_grad():
            # æ‰§è¡Œå‰å‘ä¼ æ’­è·å–è¾“å‡º
            outputs = model(**batch_example)
            # è·å–å®é™…çš„åºåˆ—é•¿åº¦
            actual_seq_length = outputs.last_hidden_state.size(1)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()
        
        return actual_seq_length
        
    except Exception as e:
        print(f"è·å–å®é™…åºåˆ—é•¿åº¦é”™è¯¯: {e}")
        # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨æ–‡æœ¬é•¿åº¦ä½œä¸ºè¿‘ä¼¼
        return batch_example['input_ids'].size(1)

def _estimate_visual_tokens_count(batch_example: Dict) -> int:
    """ä¼°ç®—è§†è§‰tokensçš„æ•°é‡"""
    try:
        # å¯¹äºQwen2.5-VLï¼Œvisual tokensæ•°é‡é€šå¸¸åŸºäºå›¾åƒåˆ†è¾¨ç‡
        # å¦‚æœæœ‰image_grid_thwå‚æ•°ï¼Œä½¿ç”¨å®ƒæ¥è®¡ç®—
        if 'image_grid_thw' in batch_example:
            # image_grid_thwçš„å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, 3]ï¼Œå…¶ä¸­3ä»£è¡¨ [tiles, height, width]
            grid_thw = batch_example['image_grid_thw']
            if grid_thw.dim() == 2 and grid_thw.size(1) == 3:
                # è®¡ç®—æ¯ä¸ªå›¾åƒçš„visual tokensæ•°é‡ (tiles * height * width)
                visual_tokens_per_image = grid_thw[:, 0] * grid_thw[:, 1] * grid_thw[:, 2]
                # è¿”å›batchä¸­ç¬¬ä¸€ä¸ªå›¾åƒçš„visual tokensæ•°é‡ï¼ˆå‡è®¾batchå†…æ‰€æœ‰å›¾åƒç›¸åŒï¼‰
                return int(visual_tokens_per_image[0].item())
        
        # å¦‚æœæ²¡æœ‰image_grid_thwï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
        # å¯¹äºæ ‡å‡†çš„VLæ¨¡å‹ï¼Œä¸€èˆ¬ä¸€å¼ å›¾ç‰‡ä¼šäº§ç”Ÿå‡ ç™¾åˆ°å‡ åƒä¸ªvisual tokens
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¿å®ˆçš„ä¼°ç®—å€¼
        if 'pixel_values' in batch_example and batch_example['pixel_values'] is not None:
            # åŸºäºåƒç´ å€¼çš„å½¢çŠ¶è¿›è¡Œä¼°ç®—
            pixel_values = batch_example['pixel_values']
            if pixel_values.dim() >= 3:
                # ä¸€èˆ¬æ¥è¯´ï¼Œvisual tokensæ•°é‡ä¸å›¾åƒåˆ†è¾¨ç‡ç›¸å…³
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„ä¼°ç®—å…¬å¼
                return 576  # å¸¸è§çš„vision transformer patchæ•°é‡ (24*24)
        
        return 0  # å¦‚æœæ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œè¿”å›0
        
    except Exception as e:
        print(f"ä¼°ç®—visual tokensæ•°é‡é”™è¯¯: {e}")
        return 0

def _estimate_flops_fallback(model, batch_example: Dict, actual_seq_length: int = None) -> float:
    """å¤‡é€‰çš„FLOPsä¼°ç®—æ–¹æ³•ï¼ˆå‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # è·å–æ¨¡å‹å‚æ•°æ•°é‡
        if hasattr(model, 'module'):
            param_count = sum(p.numel() for p in model.module.parameters())
        else:
            param_count = sum(p.numel() for p in model.parameters())
        
        # è·å–batch size
        batch_size = batch_example['input_ids'].size(0)
        
        # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¼°ç®—
        if actual_seq_length is not None:
            seq_length = actual_seq_length
        else:
            # ä¼°ç®—åºåˆ—é•¿åº¦ = æ–‡æœ¬tokens + visual tokens
            text_length = batch_example['input_ids'].size(1)
            visual_tokens = _estimate_visual_tokens_count(batch_example)
            seq_length = text_length + visual_tokens
        
        # æ›´å‡†ç¡®çš„FLOPsä¼°ç®—ï¼ˆåŸºäºTransformeræ¶æ„ï¼‰
        # å‰å‘ä¼ æ’­FLOPsä¼°ç®—
        forward_flops = _estimate_forward_flops(param_count, batch_size, seq_length)
        
        # åå‘ä¼ æ’­FLOPsä¼°ç®—ï¼ˆé€šå¸¸æ˜¯å‰å‘ä¼ æ’­çš„2å€ï¼‰
        backward_flops = 2 * forward_flops
        
        total_flops = forward_flops + backward_flops
        
        print(f"ä½¿ç”¨ä¼°ç®—æ–¹æ³•:")
        print(f"  å‚æ•°æ•°é‡: {param_count:.2e}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
        if actual_seq_length is not None:
            print(f"  å®é™…åºåˆ—é•¿åº¦: {seq_length}")
        else:
            estimated_visual = _estimate_visual_tokens_count(batch_example)
            print(f"  ä¼°ç®—visual tokens: {estimated_visual}")
            print(f"  ä¼°ç®—æ€»åºåˆ—é•¿åº¦: {seq_length}")
        print(f"  å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
        print(f"  åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
        print(f"  æ€»FLOPs: {total_flops:.2e}")
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPsä¼°ç®—é”™è¯¯: {e}")
        return 0.0

def _estimate_forward_flops(param_count: int, batch_size: int, seq_length: int) -> float:
    """ä¼°ç®—å‰å‘ä¼ æ’­çš„FLOPs"""
    # å¯¹äºTransformeræ¨¡å‹ï¼Œå‰å‘ä¼ æ’­çš„FLOPsä¸»è¦æ¥è‡ªï¼š
    # 1. çŸ©é˜µä¹˜æ³•ï¼ˆçº¿æ€§å±‚ï¼‰
    # 2. æ³¨æ„åŠ›æœºåˆ¶
    # 3. æ¿€æ´»å‡½æ•°ç­‰
    
    # ç®€åŒ–ä¼°ç®—ï¼šæ¯ä¸ªå‚æ•°åœ¨å‰å‘ä¼ æ’­ä¸­å¤§çº¦å‚ä¸2æ¬¡ä¹˜æ³•è¿ç®—
    # å¯¹äºmultimodalæ¨¡å‹ï¼Œè€ƒè™‘è§†è§‰å’Œæ–‡æœ¬çš„äº¤äº’ï¼Œä½¿ç”¨ç¨é«˜çš„ç³»æ•°
    flops_per_token = 2.5 * param_count
    total_flops = flops_per_token * batch_size * seq_length
    
    return float(total_flops)

def get_gpu_stats():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    gpu_stats = {}
    try:
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                memory_reserved = torch.cuda.memory_reserved(i) / (1024**3)    # GB
                memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                
                # è·å–GPUåˆ©ç”¨ç‡ (è¿‘ä¼¼å€¼ï¼ŒåŸºäºå†…å­˜ä½¿ç”¨)
                memory_utilization = (memory_allocated / memory_total) * 100
                
                gpu_stats[f'gpu_{i}'] = {
                    'memory_allocated_gb': memory_allocated,
                    'memory_reserved_gb': memory_reserved,
                    'memory_total_gb': memory_total,
                    'memory_utilization_percent': memory_utilization
                }
                
                # å°è¯•è·å–GPUæ¸©åº¦å’ŒåŠŸè€— (å¦‚æœå¯ç”¨)
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # W
                    gpu_stats[f'gpu_{i}']['temperature_c'] = temp
                    gpu_stats[f'gpu_{i}']['power_usage_w'] = power
                except:
                    pass  # pynvmlä¸å¯ç”¨æ—¶è·³è¿‡
                    
    except Exception as e:
        print(f"GPUçŠ¶æ€è·å–é”™è¯¯: {e}")
    
    return gpu_stats

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨ï¼ˆæ”¯æŒwandbï¼‰"""
    
    def __init__(self, output_dir: str, config: Dict = None, log_file: str = "training_log.json"):
        self.output_dir = output_dir
        self.log_file = os.path.join(output_dir, log_file)
        self.step_logs = []
        self.epoch_logs = []
        self.start_time = None
        self.step_start_time = None
        self.config = config or {}
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–wandb
        self.use_wandb = False
        self._init_wandb()
        
        # MFUè®¡ç®—ç›¸å…³å‚æ•°
        self.model_ref = None
        self.seq_length = config.get('model', {}).get('max_sequence_length', 512)
        self.batch_size = config.get('train_batch_size', 32)
        self.actual_flops = None  # å­˜å‚¨å®é™…æµ‹é‡çš„FLOPs
        self.actual_seq_length = None  # å­˜å‚¨å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokensï¼‰
    
    def _is_main_process(self):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹"""
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                return dist.get_rank() == 0
            return True  # éåˆ†å¸ƒå¼è®­ç»ƒæ—¶é»˜è®¤ä¸ºä¸»è¿›ç¨‹
        except ImportError:
            return True
    
    def _init_wandb(self):
        """åˆå§‹åŒ–wandbï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰"""
        if not WANDB_AVAILABLE:
            return
        
        wandb_config = self.config.get('wandb', {})
        if not wandb_config.get('enabled', False):
            print("wandb logging disabled in config")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¦‚æœæ˜¯åˆ™åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–wandb
        is_main_process = True
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                # åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰rank 0è¿›ç¨‹åˆå§‹åŒ–wandb
                is_main_process = dist.get_rank() == 0
                if not is_main_process:
                    print(f"è¿›ç¨‹ rank {dist.get_rank()}: è·³è¿‡wandbåˆå§‹åŒ–ï¼ˆéä¸»è¿›ç¨‹ï¼‰")
                    return
        except ImportError:
            # å¦‚æœtorch.distributedä¸å¯ç”¨ï¼Œé»˜è®¤ä¸ºä¸»è¿›ç¨‹
            pass
        
        if not is_main_process:
            return
        
        try:
            # åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–wandb
            wandb.init(
                project=wandb_config.get('project', 'qwen_classification'),
                name=wandb_config.get('run_name'),
                tags=wandb_config.get('tags', []),
                notes=wandb_config.get('notes'),
                config=self.config,
                resume="allow"  # å…è®¸æ¢å¤
            )
            
            # è®°å½•æ¨¡å‹å’Œè®­ç»ƒé…ç½®
            try:
                if 'model' in self.config:
                    wandb.config.update({
                        'model_name': self.config['model'].get('pretrained_name', 'unknown'),
                        'num_labels': self.config['model'].get('num_labels', 'unknown')
                    })
                
                if 'training' in self.config:
                    wandb.config.update({
                        'learning_rate': self.config['training'].get('learning_rate', 'unknown'),
                        'num_epochs': self.config['training'].get('num_epochs', 'unknown'),
                        'batch_size': self.config.get('train_batch_size', 'unknown')
                    })
            except Exception as config_error:
                print(f"âš ï¸  wandbé…ç½®æ›´æ–°å¤±è´¥: {config_error}")
            
            self.use_wandb = True
            print("âœ… wandb initialized successfully")
            
            # æ˜¾ç¤ºwandbé“¾æ¥ä¿¡æ¯
            try:
                if wandb.run is not None:
                    print(f"ğŸ“Š wandb project: {wandb.run.project}")
                    print(f"ğŸ”— wandb run: {wandb.run.name}")
                    print(f"ğŸš€ View run at: {wandb.run.url}")
                    
                    # æ„å»ºé¡¹ç›®é“¾æ¥
                    if hasattr(wandb.run, 'entity') and hasattr(wandb.run, 'project'):
                        project_url = f"https://wandb.ai/{wandb.run.entity}/{wandb.run.project}"
                        print(f"â­ View project at: {project_url}")
            except Exception as display_error:
                print(f"âš ï¸  wandbé“¾æ¥æ˜¾ç¤ºå¤±è´¥: {display_error}")
            
        except Exception as e:
            print(f"âŒ Failed to initialize wandb: {e}")
            self.use_wandb = False
    
    def set_model_ref(self, model):
        """è®¾ç½®æ¨¡å‹å¼•ç”¨ï¼Œç”¨äºMFUè®¡ç®—"""
        self.model_ref = model
    
    def profile_model_flops(self, batch_example: Dict):
        """æµ‹é‡æ¨¡å‹çš„å®é™…FLOPs"""
        if self.model_ref is None:
            print("æ¨¡å‹å¼•ç”¨æœªè®¾ç½®ï¼Œæ— æ³•æµ‹é‡FLOPs")
            return
        
        print("æ­£åœ¨æµ‹é‡æ¨¡å‹å®é™…FLOPs...")
        self.actual_flops = profile_model_flops(self.model_ref, batch_example)
        
        # åŒæ—¶è·å–å®é™…åºåˆ—é•¿åº¦
        try:
            self.actual_seq_length = _get_actual_sequence_length(self.model_ref, batch_example)
            print(f"âœ… å®é™…åºåˆ—é•¿åº¦: {self.actual_seq_length}")
        except Exception as e:
            print(f"âŒ è·å–åºåˆ—é•¿åº¦å¤±è´¥: {e}")
        
        if self.actual_flops > 0:
            print(f"âœ… æ¨¡å‹å®é™…FLOPs: {self.actual_flops:.2e}")
            
            # æ˜¾ç¤ºMFUè®¡ç®—ç›¸å…³ä¿¡æ¯
            self._show_mfu_calculation_info()
        else:
            print("âŒ FLOPsæµ‹é‡å¤±è´¥ï¼ŒMFUè®¡ç®—å°†è¢«ç¦ç”¨")
    
    def _show_mfu_calculation_info(self):
        """æ˜¾ç¤ºMFUè®¡ç®—çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            if self.actual_flops is None or self.actual_flops <= 0:
                return
            
            print(f"\nğŸ“Š MFUè®¡ç®—é…ç½®ä¿¡æ¯:")
            print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"  â€¢ å®é™…åºåˆ—é•¿åº¦: {self.actual_seq_length}")
            print(f"  â€¢ å®é™…FLOPs: {self.actual_flops:.2e}")
            
            # è·å–GPUå³°å€¼æ€§èƒ½
            peak_flops = get_gpu_peak_flops()
            print(f"  â€¢ GPUå³°å€¼æ€§èƒ½: {peak_flops:.2e} FLOPs/s")
            
            # ä¼°ç®—ä¸€ä¸ªæ ·æœ¬çš„MFU (å‡è®¾1ç§’çš„step time)
            sample_step_time = 1.0
            sample_mfu = calculate_mfu(self.model_ref, self.batch_size, self.actual_seq_length, sample_step_time, self.actual_flops)
            print(f"  â€¢ ç†è®ºæœ€å¤§MFU (1ç§’/æ­¥): {sample_mfu:.4f} ({sample_mfu*100:.2f}%)")
            
            # è®¡ç®—è¾¾åˆ°ç›®æ ‡MFUæ‰€éœ€çš„æ­¥éª¤æ—¶é—´
            target_mfus = [0.1, 0.2, 0.3, 0.5]
            print(f"  â€¢ è¾¾åˆ°ç›®æ ‡MFUæ‰€éœ€çš„æ­¥éª¤æ—¶é—´:")
            for target_mfu in target_mfus:
                required_time = self.actual_flops / (target_mfu * peak_flops)
                print(f"    - {target_mfu*100:.0f}% MFU: {required_time:.3f}ç§’/æ­¥")
                
        except Exception as e:
            print(f"æ˜¾ç¤ºMFUä¿¡æ¯é”™è¯¯: {e}")
    
    def set_actual_flops(self, flops: float, seq_length: int = None):
        """è®¾ç½®å®é™…FLOPsï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„åŒæ­¥ï¼‰"""
        self.actual_flops = flops
        if seq_length is not None:
            self.actual_seq_length = seq_length
    
    def _calculate_actual_seq_length(self, attention_mask):
        """åŠ¨æ€è®¡ç®—å½“å‰batchçš„å®é™…åºåˆ—é•¿åº¦"""
        try:
            if attention_mask is None:
                return self.seq_length
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦
            valid_lengths = attention_mask.sum(dim=1)  # [batch_size]
            
            # ä½¿ç”¨æ‰¹æ¬¡ä¸­çš„å¹³å‡æœ‰æ•ˆé•¿åº¦ï¼ˆæˆ–æœ€å¤§é•¿åº¦ï¼‰
            # è¿™é‡Œä½¿ç”¨å¹³å‡å€¼ï¼Œå› ä¸ºMFUé€šå¸¸å…³å¿ƒçš„æ˜¯æ•´ä½“ååé‡
            avg_seq_length = valid_lengths.float().mean().item()
            
            return int(avg_seq_length)
            
        except Exception as e:
            print(f"è®¡ç®—å®é™…åºåˆ—é•¿åº¦é”™è¯¯: {e}")
            return self.actual_seq_length if self.actual_seq_length is not None else self.seq_length
    
    def start_training(self):
        """å¼€å§‹è®­ç»ƒ"""
        self.start_time = time.time()
        self.step_start_time = time.time()
        
        if self.use_wandb and self._is_main_process():
            wandb.log({"training/started": True, "training/start_time": self.start_time})
    
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float, attention_mask=None, real_time_flops=None):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        current_time = time.time()
        step_time = current_time - self.step_start_time
        
        # å¦‚æœæä¾›äº†å®æ—¶FLOPsï¼Œæ›´æ–°å½“å‰FLOPså€¼
        if real_time_flops is not None and real_time_flops > 0:
            self.actual_flops = real_time_flops
        
        log_entry = {
            'step': int(step),
            'epoch': int(epoch),
            'loss': float(loss),
            'grad_norm': float(grad_norm),
            'learning_rate': float(learning_rate),
            'step_time': float(step_time),
            'timestamp': float(current_time)
        }
        
        # å¦‚æœæœ‰å®æ—¶FLOPsï¼Œä¹Ÿè®°å½•åˆ°æ—¥å¿—ä¸­
        if real_time_flops is not None:
            log_entry['real_time_flops'] = float(real_time_flops)
        
        self.step_logs.append(log_entry)
        
        # è®°å½•åˆ°wandbï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if self.use_wandb and self._is_main_process():
            # Trainingç»„ - åŒ…å«lossã€grad_normã€lr
            wandb.log({
                "training/loss": float(loss),
                "training/grad_norm": float(grad_norm),
                "training/lr": float(learning_rate),
                "training/epoch": float(epoch),
                "global_step": int(step)
            }, step=int(step))
            
            # Perfç»„ - åŒ…å«MFUï¼ˆä½¿ç”¨å®æ—¶æ•°æ®ï¼‰
            if self.model_ref is not None and self.actual_flops is not None:
                # ä¼˜å…ˆä½¿ç”¨å½“å‰batchçš„å®é™…åºåˆ—é•¿åº¦
                if attention_mask is not None:
                    # åŠ¨æ€è®¡ç®—å½“å‰batchçš„å®é™…åºåˆ—é•¿åº¦
                    current_seq_length = self._calculate_actual_seq_length(attention_mask)
                elif self.actual_seq_length is not None:
                    # ä½¿ç”¨ä¹‹å‰æµ‹é‡çš„åºåˆ—é•¿åº¦
                    current_seq_length = self.actual_seq_length
                else:
                    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
                    current_seq_length = self.seq_length
                
                # ä½¿ç”¨æœ€æ–°çš„FLOPså€¼è®¡ç®—MFU
                current_flops = real_time_flops if real_time_flops is not None else self.actual_flops
                mfu = calculate_mfu(self.model_ref, self.batch_size, current_seq_length, step_time, current_flops)
                
                perf_logs = {
                    "perf/mfu": float(mfu),
                    "perf/step_time": float(step_time),
                    "perf/tokens_per_second": float(self.batch_size * current_seq_length / step_time),
                    "perf/actual_flops": float(current_flops),
                    "perf/actual_seq_length": float(current_seq_length)
                }
                
                # å¦‚æœæœ‰å®æ—¶FLOPsï¼Œæ ‡è®°å‡ºæ¥
                if real_time_flops is not None:
                    perf_logs["perf/real_time_measurement"] = 1.0
                    perf_logs["perf/flops_per_second"] = float(current_flops / step_time)
                else:
                    perf_logs["perf/real_time_measurement"] = 0.0
                
                wandb.log(perf_logs, step=int(step))
            
            # Systemç»„ - GPUçŠ¶æ€ (æ¯10æ­¥è®°å½•ä¸€æ¬¡) - å·²ç¦ç”¨å•GPUæŒ‡æ ‡ï¼Œé¿å…å†—ä½™
            # æ³¨é‡Šæ‰å•ä¸ªGPUæŒ‡æ ‡ï¼Œå‡å°‘WandBä¸­çš„å†—ä½™ä¿¡æ¯
            # if step % 10 == 0:
            #     gpu_stats = get_gpu_stats()
            #     if gpu_stats:
            #         system_logs = {}
            #         for gpu_id, stats in gpu_stats.items():
            #             # åªè®°å½•GPUå†…å­˜åˆ†é…å’Œåˆ©ç”¨ç‡
            #             system_logs[f"system/{gpu_id}_memory_allocated_percent"] = stats['memory_utilization_percent']
            #             system_logs[f"system/{gpu_id}_memory_allocated_gb"] = stats['memory_allocated_gb']
            #         
            #         if system_logs:  # åªæœ‰å½“æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è®°å½•
            #             wandb.log(system_logs, step=int(step))
        
        self.step_start_time = current_time
        
        # å®šæœŸä¿å­˜æœ¬åœ°æ—¥å¿—
        if step % 100 == 0:
            self.save_logs()
    
    def log_epoch(self, epoch: int, avg_loss: float, elapsed_time: float):
        """è®°å½•epochç»Ÿè®¡"""
        log_entry = {
            'epoch': int(epoch),
            'avg_loss': float(avg_loss),
            'elapsed_time': float(elapsed_time),
            'timestamp': float(time.time())
        }
        
        self.epoch_logs.append(log_entry)
        
        # è®°å½•åˆ°wandbï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if self.use_wandb and self._is_main_process():
            wandb.log({
                "training/epoch_avg_loss": float(avg_loss),
                "training/epoch_time": float(elapsed_time),
                "training/epoch_number": int(epoch)
            }, step=int(epoch))
        
        self.save_logs()
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float):
        """è®°å½•è¯„ä¼°ç»“æœ - åœ¨trainingç»„ä¸­æ˜¾ç¤ºaccuracy"""
        if self.use_wandb and self._is_main_process():
            wandb.log({
                "training/eval_loss": float(eval_loss),
                "training/eval_accuracy": float(eval_accuracy),
                "global_step": int(step)
            }, step=int(step))
    
    def save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            logs = {
                'step_logs': self.step_logs,
                'epoch_logs': self.epoch_logs,
                'total_training_time': time.time() - self.start_time if self.start_time else 0
            }
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥JSONåºåˆ—åŒ–
            serializable_logs = make_json_serializable(logs)
            
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_logs, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")
    
    def finish_training(self):
        """ç»“æŸè®­ç»ƒ"""
        if self.use_wandb and self._is_main_process():
            wandb.log({"training/finished": True, "training/total_time": time.time() - self.start_time})
            wandb.finish()
            print("ğŸ“Š wandb run finished")
    
    def get_latest_metrics(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡"""
        if self.step_logs:
            return self.step_logs[-1]
        return None
    
    def get_avg_metrics(self, last_n_steps: int = 100) -> Dict:
        """è·å–æœ€è¿‘Næ­¥çš„å¹³å‡æŒ‡æ ‡"""
        if not self.step_logs:
            return {}
        
        recent_logs = self.step_logs[-last_n_steps:]
        
        if not recent_logs:
            return {}
        
        avg_loss = sum(log['loss'] for log in recent_logs) / len(recent_logs)
        avg_grad_norm = sum(log['grad_norm'] for log in recent_logs) / len(recent_logs)
        avg_step_time = sum(log['step_time'] for log in recent_logs) / len(recent_logs)
        
        return {
            'avg_loss': avg_loss,
            'avg_grad_norm': avg_grad_norm,
            'avg_step_time': avg_step_time,
            'num_steps': len(recent_logs)
        } 