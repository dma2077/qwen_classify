import time
import json
import os
import torch
import psutil
from typing import Dict, List, Optional

# æ·»åŠ wandbæ”¯æŒ
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not installed. pip install wandb to enable logging.")

def make_json_serializable(obj):
    """ç¡®ä¿å¯¹è±¡å¯ä»¥JSONåºåˆ—åŒ–"""
    if isinstance(obj, torch.Tensor):
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif torch.is_tensor(obj):
        # å¤„ç†å…¶ä»–torch tensorç±»å‹
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif hasattr(obj, 'dtype') and 'torch' in str(type(obj)):
        # å¤„ç†torchçš„æ ‡é‡ç±»å‹
        if 'float' in str(obj.dtype):
            return float(obj)
        elif 'int' in str(obj.dtype):
            return int(obj)
        else:
            return float(obj)
    elif isinstance(obj, (float, int)):
        return obj
    elif isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    else:
        return obj

def get_gpu_peak_flops():
    """è·å–GPUå³°å€¼FLOPsæ€§èƒ½"""
    try:
        if not torch.cuda.is_available():
            return 312e12  # é»˜è®¤å€¼
        
        # è·å–GPUåç§°
        gpu_name = torch.cuda.get_device_name(0).upper()
        
        # ä¸åŒGPUçš„å³°å€¼æ€§èƒ½ (TFLOPs for FP16/BF16)
        gpu_peak_flops = {
            # NVIDIA A100ç³»åˆ—
            'A100': 312e12,    # A100 80GB
            'A100-SXM': 312e12,
            'A100-PCIE': 312e12,
            # NVIDIA H100ç³»åˆ—  
            'H100': 989e12,    # H100 80GB
            'H100-SXM': 989e12,
            'H100-PCIE': 756e12,
            # NVIDIA V100ç³»åˆ—
            'V100': 112e12,    # V100 32GB
            'V100-SXM': 112e12,
            'V100-PCIE': 112e12,
            # NVIDIA RTXç³»åˆ—
            'RTX 4090': 165e12,
            'RTX 4080': 112e12,
            'RTX 3090': 71e12,
            'RTX 3080': 58e12,
            # NVIDIA T4
            'T4': 65e12,
            # NVIDIA L4
            'L4': 121e12,
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„GPU
        for gpu_model, peak_flops in gpu_peak_flops.items():
            if gpu_model in gpu_name:
                return peak_flops
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„GPUï¼Œä½¿ç”¨é»˜è®¤å€¼
        print(f"æœªè¯†åˆ«çš„GPUç±»å‹: {gpu_name}ï¼Œä½¿ç”¨é»˜è®¤å³°å€¼æ€§èƒ½")
        return 312e12  # é»˜è®¤ä½¿ç”¨A100çš„æ€§èƒ½
        
    except Exception as e:
        print(f"è·å–GPUå³°å€¼æ€§èƒ½é”™è¯¯: {e}")
        return 312e12  # é»˜è®¤å€¼

def calculate_mfu(model, batch_size: int, seq_length: int, step_time: float, actual_flops: float = None) -> float:
    """è®¡ç®—MFU (Model FLOPs Utilization)
    
    MFU = å®é™…FLOPs/s / GPUå³°å€¼FLOPs/s
    """
    try:
        if actual_flops is None:
            # å¦‚æœæ²¡æœ‰æä¾›å®é™…FLOPsï¼Œè¿”å›0
            return 0.0
        
        # è®¡ç®—å®é™…FLOPs/s
        actual_flops_per_second = actual_flops / step_time
        
        # åŠ¨æ€è·å–GPUå³°å€¼æ€§èƒ½
        peak_flops_per_second = get_gpu_peak_flops()
        
        # è®¡ç®—MFU
        mfu = actual_flops_per_second / peak_flops_per_second
        return min(mfu, 1.0)  # é™åˆ¶åœ¨100%ä»¥å†…
        
    except Exception as e:
        print(f"MFUè®¡ç®—é”™è¯¯: {e}")
        return 0.0

def profile_model_flops(model, batch_example: Dict) -> float:
    """ä½¿ç”¨PyTorch profilerè·å–æ¨¡å‹å®é™…FLOPsï¼ˆåŒ…æ‹¬å‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
        model.train()
        
        # è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰
        actual_seq_length = _get_actual_sequence_length(model, batch_example)
        
        # åˆ†åˆ«æµ‹é‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„FLOPs
        forward_flops = _profile_forward_flops(model, batch_example)
        backward_flops = _profile_backward_flops(model, batch_example)
        
        total_flops = forward_flops + backward_flops
        
        if total_flops > 0:
            print(f"æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
            print(f"å®é™…åºåˆ—é•¿åº¦(åŒ…å«visual tokens): {actual_seq_length}")
            print(f"å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
            print(f"åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
            print(f"æ€»FLOPs: {total_flops:.2e}")
        else:
            print("æ— æ³•é€šè¿‡profileræµ‹é‡FLOPsï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            total_flops = _estimate_flops_fallback(model, batch_example, actual_seq_length)
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPs profilingé”™è¯¯: {e}")
        # å°è¯•è·å–åºåˆ—é•¿åº¦ç”¨äºä¼°ç®—
        try:
            actual_seq_length = _get_actual_sequence_length(model, batch_example)
            return _estimate_flops_fallback(model, batch_example, actual_seq_length)
        except:
            return _estimate_flops_fallback(model, batch_example)

def _profile_forward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡å‰å‘ä¼ æ’­çš„FLOPs"""
    try:
        model.eval()  # ä½¿ç”¨evalæ¨¡å¼é¿å…dropoutç­‰å½±å“FLOPsè®¡ç®—
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                with torch.no_grad():
                    # ä»…æ‰§è¡Œå‰å‘ä¼ æ’­
                    outputs = model(**batch_example)
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            return 0.0
        
    except Exception as e:
        print(f"å‰å‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _profile_backward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡åå‘ä¼ æ’­çš„FLOPs"""
    try:
        model.train()  # è®­ç»ƒæ¨¡å¼
        
        # å…ˆæ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆä¸åœ¨profilerä¸­ï¼‰
        outputs = model(**batch_example)
        loss = outputs.loss
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            # æµ‹é‡åå‘ä¼ æ’­çš„FLOPs
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                # ä»…æ‰§è¡Œåå‘ä¼ æ’­
                loss.backward()
            
            # æ¸…ç†æ¢¯åº¦
            model.zero_grad()
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            model.zero_grad()  # ç¡®ä¿æ¸…ç†æ¢¯åº¦
            return 0.0
        
    except Exception as e:
        print(f"åå‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _get_actual_sequence_length(model, batch_example: Dict) -> int:
    """è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰"""
    try:
        # ä¸´æ—¶è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼ä»¥è·å–è¾“å‡ºshape
        model.eval()
        
        with torch.no_grad():
            # æ‰§è¡Œå‰å‘ä¼ æ’­è·å–è¾“å‡º
            outputs = model(**batch_example)
            # è·å–å®é™…çš„åºåˆ—é•¿åº¦
            actual_seq_length = outputs.last_hidden_state.size(1)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()
        
        return actual_seq_length
        
    except Exception as e:
        print(f"è·å–å®é™…åºåˆ—é•¿åº¦é”™è¯¯: {e}")
        # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨æ–‡æœ¬é•¿åº¦ä½œä¸ºè¿‘ä¼¼
        return batch_example['input_ids'].size(1)

def _estimate_visual_tokens_count(batch_example: Dict) -> int:
    """ä¼°ç®—è§†è§‰tokensçš„æ•°é‡"""
    try:
        # å¯¹äºQwen2.5-VLï¼Œvisual tokensæ•°é‡é€šå¸¸åŸºäºå›¾åƒåˆ†è¾¨ç‡
        # å¦‚æœæœ‰image_grid_thwå‚æ•°ï¼Œä½¿ç”¨å®ƒæ¥è®¡ç®—
        if 'image_grid_thw' in batch_example:
            # image_grid_thwçš„å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, 3]ï¼Œå…¶ä¸­3ä»£è¡¨ [tiles, height, width]
            grid_thw = batch_example['image_grid_thw']
            if grid_thw.dim() == 2 and grid_thw.size(1) == 3:
                # è®¡ç®—æ¯ä¸ªå›¾åƒçš„visual tokensæ•°é‡ (tiles * height * width)
                visual_tokens_per_image = grid_thw[:, 0] * grid_thw[:, 1] * grid_thw[:, 2]
                # è¿”å›batchä¸­ç¬¬ä¸€ä¸ªå›¾åƒçš„visual tokensæ•°é‡ï¼ˆå‡è®¾batchå†…æ‰€æœ‰å›¾åƒç›¸åŒï¼‰
                return int(visual_tokens_per_image[0].item())
        
        # å¦‚æœæ²¡æœ‰image_grid_thwï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
        # å¯¹äºæ ‡å‡†çš„VLæ¨¡å‹ï¼Œä¸€èˆ¬ä¸€å¼ å›¾ç‰‡ä¼šäº§ç”Ÿå‡ ç™¾åˆ°å‡ åƒä¸ªvisual tokens
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¿å®ˆçš„ä¼°ç®—å€¼
        if 'pixel_values' in batch_example and batch_example['pixel_values'] is not None:
            # åŸºäºåƒç´ å€¼çš„å½¢çŠ¶è¿›è¡Œä¼°ç®—
            pixel_values = batch_example['pixel_values']
            if pixel_values.dim() >= 3:
                # ä¸€èˆ¬æ¥è¯´ï¼Œvisual tokensæ•°é‡ä¸å›¾åƒåˆ†è¾¨ç‡ç›¸å…³
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„ä¼°ç®—å…¬å¼
                return 576  # å¸¸è§çš„vision transformer patchæ•°é‡ (24*24)
        
        return 0  # å¦‚æœæ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œè¿”å›0
        
    except Exception as e:
        print(f"ä¼°ç®—visual tokensæ•°é‡é”™è¯¯: {e}")
        return 0

def _estimate_flops_fallback(model, batch_example: Dict, actual_seq_length: int = None) -> float:
    """å¤‡é€‰çš„FLOPsä¼°ç®—æ–¹æ³•ï¼ˆå‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # è·å–æ¨¡å‹å‚æ•°æ•°é‡
        if hasattr(model, 'module'):
            param_count = sum(p.numel() for p in model.module.parameters())
        else:
            param_count = sum(p.numel() for p in model.parameters())
        
        # è·å–batch size
        batch_size = batch_example['input_ids'].size(0)
        
        # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¼°ç®—
        if actual_seq_length is not None:
            seq_length = actual_seq_length
        else:
            # ä¼°ç®—åºåˆ—é•¿åº¦ = æ–‡æœ¬tokens + visual tokens
            text_length = batch_example['input_ids'].size(1)
            visual_tokens = _estimate_visual_tokens_count(batch_example)
            seq_length = text_length + visual_tokens
        
        # æ›´å‡†ç¡®çš„FLOPsä¼°ç®—ï¼ˆåŸºäºTransformeræ¶æ„ï¼‰
        # å‰å‘ä¼ æ’­FLOPsä¼°ç®—
        forward_flops = _estimate_forward_flops(param_count, batch_size, seq_length)
        
        # åå‘ä¼ æ’­FLOPsä¼°ç®—ï¼ˆé€šå¸¸æ˜¯å‰å‘ä¼ æ’­çš„2å€ï¼‰
        backward_flops = 2 * forward_flops
        
        total_flops = forward_flops + backward_flops
        
        print(f"ä½¿ç”¨ä¼°ç®—æ–¹æ³•:")
        print(f"  å‚æ•°æ•°é‡: {param_count:.2e}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
        if actual_seq_length is not None:
            print(f"  å®é™…åºåˆ—é•¿åº¦: {seq_length}")
        else:
            estimated_visual = _estimate_visual_tokens_count(batch_example)
            print(f"  ä¼°ç®—visual tokens: {estimated_visual}")
            print(f"  ä¼°ç®—æ€»åºåˆ—é•¿åº¦: {seq_length}")
        print(f"  å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
        print(f"  åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
        print(f"  æ€»FLOPs: {total_flops:.2e}")
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPsä¼°ç®—é”™è¯¯: {e}")
        return 0.0

def _estimate_forward_flops(param_count: int, batch_size: int, seq_length: int) -> float:
    """ä¼°ç®—å‰å‘ä¼ æ’­çš„FLOPs"""
    # å¯¹äºTransformeræ¨¡å‹ï¼Œå‰å‘ä¼ æ’­çš„FLOPsä¸»è¦æ¥è‡ªï¼š
    # 1. çŸ©é˜µä¹˜æ³•ï¼ˆçº¿æ€§å±‚ï¼‰
    # 2. æ³¨æ„åŠ›æœºåˆ¶
    # 3. æ¿€æ´»å‡½æ•°ç­‰
    
    # ç®€åŒ–ä¼°ç®—ï¼šæ¯ä¸ªå‚æ•°åœ¨å‰å‘ä¼ æ’­ä¸­å¤§çº¦å‚ä¸2æ¬¡ä¹˜æ³•è¿ç®—
    # å¯¹äºmultimodalæ¨¡å‹ï¼Œè€ƒè™‘è§†è§‰å’Œæ–‡æœ¬çš„äº¤äº’ï¼Œä½¿ç”¨ç¨é«˜çš„ç³»æ•°
    flops_per_token = 2.5 * param_count
    total_flops = flops_per_token * batch_size * seq_length
    
    return float(total_flops)

def get_gpu_stats():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    gpu_stats = {}
    try:
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                memory_reserved = torch.cuda.memory_reserved(i) / (1024**3)    # GB
                memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                
                # è·å–GPUåˆ©ç”¨ç‡ (è¿‘ä¼¼å€¼ï¼ŒåŸºäºå†…å­˜ä½¿ç”¨)
                memory_utilization = (memory_allocated / memory_total) * 100
                
                gpu_stats[f'gpu_{i}'] = {
                    'memory_allocated_gb': memory_allocated,
                    'memory_reserved_gb': memory_reserved,
                    'memory_total_gb': memory_total,
                    'memory_utilization_percent': memory_utilization
                }
                
                # å°è¯•è·å–GPUæ¸©åº¦å’ŒåŠŸè€— (å¦‚æœå¯ç”¨)
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # W
                    gpu_stats[f'gpu_{i}']['temperature_c'] = temp
                    gpu_stats[f'gpu_{i}']['power_usage_w'] = power
                except:
                    pass  # pynvmlä¸å¯ç”¨æ—¶è·³è¿‡
                    
    except Exception as e:
        print(f"GPUçŠ¶æ€è·å–é”™è¯¯: {e}")
    
    return gpu_stats

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨ï¼ˆæ”¯æŒwandbï¼‰"""
    
    def __init__(self, output_dir: str, config: Dict = None, log_file: str = "training_log.json"):
        self.output_dir = output_dir
        self.log_file = os.path.join(output_dir, log_file)
        self.step_logs = []
        self.epoch_logs = []
        self.start_time = None
        self.step_start_time = None
        self.config = config or {}
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–wandb
        self.use_wandb = False
        self._init_wandb()
        
        # MFUè®¡ç®—ç›¸å…³å‚æ•°
        self.model_ref = None
        self.seq_length = config.get('model', {}).get('max_sequence_length', 512)
        self.batch_size = config.get('train_batch_size', 32)
        self.actual_flops = None  # å­˜å‚¨å®é™…æµ‹é‡çš„FLOPs
        self.actual_seq_length = None  # å­˜å‚¨å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokensï¼‰
    
    def _init_wandb(self):
        """åˆå§‹åŒ–wandb"""
        if not WANDB_AVAILABLE:
            return
        
        wandb_config = self.config.get('wandb', {})
        if not wandb_config.get('enabled', False):
            print("wandb logging disabled in config")
            return
        
        try:
            # åˆå§‹åŒ–wandb
            wandb.init(
                project=wandb_config.get('project', 'qwen_classification'),
                name=wandb_config.get('run_name'),
                tags=wandb_config.get('tags', []),
                notes=wandb_config.get('notes'),
                config=self.config,
                resume="allow"  # å…è®¸æ¢å¤
            )
            
            # è®°å½•æ¨¡å‹å’Œè®­ç»ƒé…ç½®
            if 'model' in self.config:
                wandb.config.update({
                    'model_name': self.config['model'].get('pretrained_name', 'unknown'),
                    'num_labels': self.config['model'].get('num_labels', 'unknown')
                })
            
            if 'training' in self.config:
                wandb.config.update({
                    'learning_rate': self.config['training'].get('learning_rate', 'unknown'),
                    'num_epochs': self.config['training'].get('num_epochs', 'unknown'),
                    'batch_size': self.config.get('train_batch_size', 'unknown')
                })
            
            self.use_wandb = True
            print("âœ… wandb initialized successfully")
            
        except Exception as e:
            print(f"âŒ Failed to initialize wandb: {e}")
            self.use_wandb = False
    
    def set_model_ref(self, model):
        """è®¾ç½®æ¨¡å‹å¼•ç”¨ï¼Œç”¨äºMFUè®¡ç®—"""
        self.model_ref = model
    
    def profile_model_flops(self, batch_example: Dict):
        """æµ‹é‡æ¨¡å‹çš„å®é™…FLOPs"""
        if self.model_ref is None:
            print("æ¨¡å‹å¼•ç”¨æœªè®¾ç½®ï¼Œæ— æ³•æµ‹é‡FLOPs")
            return
        
        print("æ­£åœ¨æµ‹é‡æ¨¡å‹å®é™…FLOPs...")
        self.actual_flops = profile_model_flops(self.model_ref, batch_example)
        
        # åŒæ—¶è·å–å®é™…åºåˆ—é•¿åº¦
        try:
            self.actual_seq_length = _get_actual_sequence_length(self.model_ref, batch_example)
            print(f"âœ… å®é™…åºåˆ—é•¿åº¦: {self.actual_seq_length}")
        except Exception as e:
            print(f"âŒ è·å–åºåˆ—é•¿åº¦å¤±è´¥: {e}")
        
        if self.actual_flops > 0:
            print(f"âœ… æ¨¡å‹å®é™…FLOPs: {self.actual_flops:.2e}")
        else:
            print("âŒ FLOPsæµ‹é‡å¤±è´¥ï¼ŒMFUè®¡ç®—å°†è¢«ç¦ç”¨")
    
    def set_actual_flops(self, flops: float, seq_length: int = None):
        """è®¾ç½®å®é™…FLOPsï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„åŒæ­¥ï¼‰"""
        self.actual_flops = flops
        if seq_length is not None:
            self.actual_seq_length = seq_length
    
    def start_training(self):
        """å¼€å§‹è®­ç»ƒ"""
        self.start_time = time.time()
        self.step_start_time = time.time()
        
        if self.use_wandb:
            wandb.log({"training/started": True, "training/start_time": self.start_time})
    
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        current_time = time.time()
        step_time = current_time - self.step_start_time
        
        log_entry = {
            'step': int(step),
            'epoch': int(epoch),
            'loss': float(loss),
            'grad_norm': float(grad_norm),
            'learning_rate': float(learning_rate),
            'step_time': float(step_time),
            'timestamp': float(current_time)
        }
        
        self.step_logs.append(log_entry)
        
        # è®°å½•åˆ°wandb
        if self.use_wandb:
            # Trainingç»„ - åŒ…å«lossã€grad_normã€lr
            wandb.log({
                "training/loss": float(loss),
                "training/grad_norm": float(grad_norm),
                "training/lr": float(learning_rate),
                "training/epoch": float(epoch),
                "global_step": int(step)
            }, step=int(step))
            
            # Perfç»„ - åŒ…å«MFU
            if self.model_ref is not None and self.actual_flops is not None:
                # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                seq_length_for_mfu = self.actual_seq_length if self.actual_seq_length is not None else self.seq_length
                mfu = calculate_mfu(self.model_ref, self.batch_size, seq_length_for_mfu, step_time, self.actual_flops)
                wandb.log({
                    "perf/mfu": float(mfu),
                    "perf/step_time": float(step_time),
                    "perf/tokens_per_second": float(self.batch_size * seq_length_for_mfu / step_time),
                    "perf/actual_flops": float(self.actual_flops),
                    "perf/actual_seq_length": float(seq_length_for_mfu)
                }, step=int(step))
            
            # Systemç»„ - GPUçŠ¶æ€ (æ¯10æ­¥è®°å½•ä¸€æ¬¡)
            if step % 10 == 0:
                gpu_stats = get_gpu_stats()
                if gpu_stats:
                    system_logs = {}
                    for gpu_id, stats in gpu_stats.items():
                        system_logs[f"system/{gpu_id}_memory_allocated_gb"] = stats['memory_allocated_gb']
                        system_logs[f"system/{gpu_id}_memory_utilization_percent"] = stats['memory_utilization_percent']
                        if 'temperature_c' in stats:
                            system_logs[f"system/{gpu_id}_temperature_c"] = stats['temperature_c']
                        if 'power_usage_w' in stats:
                            system_logs[f"system/{gpu_id}_power_usage_w"] = stats['power_usage_w']
                    
                    wandb.log(system_logs, step=int(step))
        
        self.step_start_time = current_time
        
        # å®šæœŸä¿å­˜æœ¬åœ°æ—¥å¿—
        if step % 100 == 0:
            self.save_logs()
    
    def log_epoch(self, epoch: int, avg_loss: float, elapsed_time: float):
        """è®°å½•epochç»Ÿè®¡"""
        log_entry = {
            'epoch': int(epoch),
            'avg_loss': float(avg_loss),
            'elapsed_time': float(elapsed_time),
            'timestamp': float(time.time())
        }
        
        self.epoch_logs.append(log_entry)
        
        # è®°å½•åˆ°wandb
        if self.use_wandb:
            wandb.log({
                "training/epoch_avg_loss": float(avg_loss),
                "training/epoch_time": float(elapsed_time),
                "training/epoch_number": int(epoch)
            }, step=int(epoch))
        
        self.save_logs()
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float):
        """è®°å½•è¯„ä¼°ç»“æœ - åœ¨trainingç»„ä¸­æ˜¾ç¤ºaccuracy"""
        if self.use_wandb:
            wandb.log({
                "training/eval_loss": float(eval_loss),
                "training/eval_accuracy": float(eval_accuracy),
                "global_step": int(step)
            }, step=int(step))
    
    def save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            logs = {
                'step_logs': self.step_logs,
                'epoch_logs': self.epoch_logs,
                'total_training_time': time.time() - self.start_time if self.start_time else 0
            }
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥JSONåºåˆ—åŒ–
            serializable_logs = make_json_serializable(logs)
            
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_logs, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")
    
    def finish_training(self):
        """ç»“æŸè®­ç»ƒ"""
        if self.use_wandb:
            wandb.log({"training/finished": True, "training/total_time": time.time() - self.start_time})
            wandb.finish()
            print("ğŸ“Š wandb run finished")
    
    def get_latest_metrics(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡"""
        if self.step_logs:
            return self.step_logs[-1]
        return None
    
    def get_avg_metrics(self, last_n_steps: int = 100) -> Dict:
        """è·å–æœ€è¿‘Næ­¥çš„å¹³å‡æŒ‡æ ‡"""
        if not self.step_logs:
            return {}
        
        recent_logs = self.step_logs[-last_n_steps:]
        
        if not recent_logs:
            return {}
        
        avg_loss = sum(log['loss'] for log in recent_logs) / len(recent_logs)
        avg_grad_norm = sum(log['grad_norm'] for log in recent_logs) / len(recent_logs)
        avg_step_time = sum(log['step_time'] for log in recent_logs) / len(recent_logs)
        
        return {
            'avg_loss': avg_loss,
            'avg_grad_norm': avg_grad_norm,
            'avg_step_time': avg_step_time,
            'num_steps': len(recent_logs)
        } 