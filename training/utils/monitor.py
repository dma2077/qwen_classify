import time
import json
import os
import torch
import psutil
from typing import Dict, List, Optional

# ç›‘æ§é¢‘ç‡é…ç½® - ç»Ÿä¸€ä½¿ç”¨all_freqè®¾ç½®

# æ·»åŠ wandbæ”¯æŒ
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not installed. pip install wandb to enable logging.")

# å…¨å±€ç¼“å­˜GPUå³°å€¼æ€§èƒ½ï¼Œé¿å…é‡å¤è¯†åˆ«
_GPU_PEAK_FLOPS_CACHE = None

def make_json_serializable(obj):
    """ç¡®ä¿å¯¹è±¡å¯ä»¥JSONåºåˆ—åŒ–"""
    if isinstance(obj, torch.Tensor):
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif torch.is_tensor(obj):
        # å¤„ç†å…¶ä»–torch tensorç±»å‹
        return float(obj.item()) if obj.numel() == 1 else obj.tolist()
    elif hasattr(obj, 'dtype') and 'torch' in str(type(obj)):
        # å¤„ç†torchçš„æ ‡é‡ç±»å‹
        if 'float' in str(obj.dtype):
            return float(obj)
        elif 'int' in str(obj.dtype):
            return int(obj)
        else:
            return float(obj)
    elif isinstance(obj, (float, int)):
        return obj
    elif isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    else:
        return obj

def get_gpu_peak_flops():
    """è·å–GPUå³°å€¼FLOPsæ€§èƒ½ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä»…é¦–æ¬¡è¯†åˆ«"""
    global _GPU_PEAK_FLOPS_CACHE
    
    # å¦‚æœå·²ç»ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if _GPU_PEAK_FLOPS_CACHE is not None:
        return _GPU_PEAK_FLOPS_CACHE
    
    try:
        if not torch.cuda.is_available():
            _GPU_PEAK_FLOPS_CACHE = 312e12  # é»˜è®¤å€¼
            return _GPU_PEAK_FLOPS_CACHE
        
        # è·å–GPUåç§°
        gpu_name = torch.cuda.get_device_name(0).upper()
        
        # ä¸åŒGPUçš„å³°å€¼æ€§èƒ½ (TFLOPs for FP16/BF16)
        gpu_peak_flops = {
            # NVIDIA A100ç³»åˆ—
            'A100': 312e12,    # A100 80GB
            'A100-SXM': 312e12,
            'A100-PCIE': 312e12,
            # NVIDIA A800ç³»åˆ— (é’ˆå¯¹ä¸­å›½å¸‚åœºçš„A100å˜ä½“)
            'A800': 280e12,    # A800 80GB (ç¨ä½äºA100)
            'A800-SXM': 280e12,
            'A800-PCIE': 280e12,
            # NVIDIA H100ç³»åˆ—  
            'H100': 989e12,    # H100 80GB
            'H100-SXM': 989e12,
            'H100-PCIE': 756e12,
            # NVIDIA H800ç³»åˆ— (é’ˆå¯¹ä¸­å›½å¸‚åœºçš„H100å˜ä½“)
            'H800': 850e12,    # H800 80GB (ç¨ä½äºH100)
            'H800-SXM': 850e12,
            'H800-PCIE': 700e12,
            # NVIDIA V100ç³»åˆ—
            'V100': 112e12,    # V100 32GB
            'V100-SXM': 112e12,
            'V100-PCIE': 112e12,
            # NVIDIA RTXç³»åˆ—
            'RTX 4090': 165e12,
            'RTX 4080': 112e12,
            'RTX 3090': 71e12,
            'RTX 3080': 58e12,
            # NVIDIA T4
            'T4': 65e12,
            # NVIDIA L4
            'L4': 121e12,
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„GPU
        for gpu_model, peak_flops in gpu_peak_flops.items():
            if gpu_model in gpu_name:
                # ä»…é¦–æ¬¡è¯†åˆ«æ—¶æ‰“å°
                print(f"âœ… è¯†åˆ«GPU: {gpu_name} -> {gpu_model} ({peak_flops/1e12:.0f} TFLOPs)")
                _GPU_PEAK_FLOPS_CACHE = peak_flops
                return _GPU_PEAK_FLOPS_CACHE
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„GPUï¼Œä½¿ç”¨é»˜è®¤å€¼
        print(f"âš ï¸  æœªè¯†åˆ«çš„GPUç±»å‹: {gpu_name}ï¼Œä½¿ç”¨é»˜è®¤å³°å€¼æ€§èƒ½ (A100: 312 TFLOPs)")
        _GPU_PEAK_FLOPS_CACHE = 312e12
        return _GPU_PEAK_FLOPS_CACHE
        
    except Exception as e:
        print(f"è·å–GPUå³°å€¼æ€§èƒ½é”™è¯¯: {e}")
        _GPU_PEAK_FLOPS_CACHE = 312e12  # é»˜è®¤å€¼
        return _GPU_PEAK_FLOPS_CACHE

def calculate_mfu_with_profiler(model, batch_size: int, seq_length: int, step_time: float) -> float:
    """ä½¿ç”¨PyTorch Profilerè®¡ç®—MFU (Model FLOPs Utilization)
    
    MFU = å®é™…FLOPs/s / GPUå³°å€¼FLOPs/s
    
    å‚æ•°:
        model: æ¨¡å‹å®ä¾‹
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_length: å®é™…åºåˆ—é•¿åº¦
        step_time: æ­¥éª¤è€—æ—¶ï¼ˆç§’ï¼‰
    
    è¿”å›:
        mfu: Model FLOPs Utilization (0-1ä¹‹é—´çš„å€¼)
    """
    try:
        # ä½¿ç”¨profileræµ‹é‡FLOPs
        actual_flops = _measure_flops_with_profiler(model, batch_size, seq_length)
        
        if actual_flops <= 0:
            print("âš ï¸  Profileræ— æ³•æµ‹é‡FLOPsï¼Œè¿”å›0")
            return 0.0
        
        # è®¡ç®—å®é™…FLOPs/s
        actual_flops_per_second = actual_flops / step_time
        
        # è·å–GPUå³°å€¼æ€§èƒ½
        peak_flops_per_second = get_gpu_peak_flops()
        
        # è®¡ç®—MFU
        mfu = actual_flops_per_second / peak_flops_per_second
        return min(mfu, 1.0)  # é™åˆ¶åœ¨100%ä»¥å†…
        
    except Exception as e:
        print(f"Profiler MFUè®¡ç®—é”™è¯¯: {e}")
        return 0.0

def _measure_flops_with_profiler(model, batch_size: int, seq_length: int) -> float:
    """ä½¿ç”¨PyTorch Profileræµ‹é‡FLOPs"""
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„batchç”¨äºprofiling
        device = next(model.parameters()).device
        dummy_batch = _create_dummy_batch_for_profiling(batch_size, seq_length, device)
        
        model.eval()
        with torch.profiler.profile(
            activities=[torch.profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            with_flops=True,
            profile_memory=False
        ) as prof:
            with torch.no_grad():
                _ = model(**dummy_batch)
        
        # æ”¶é›†FLOPsç»Ÿè®¡
        total_flops = 0
        for event in prof.events():
            if hasattr(event, 'flops') and event.flops > 0:
                total_flops += event.flops
        
        return float(total_flops)
        
    except Exception as e:
        print(f"Profiler FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _create_dummy_batch_for_profiling(batch_size: int, seq_length: int, device: torch.device) -> Dict:
    """åˆ›å»ºç”¨äºprofilingçš„è™šæ‹Ÿbatch"""
    try:
        # åˆ›å»ºè™šæ‹Ÿçš„è¾“å…¥æ•°æ®
        input_ids = torch.randint(0, 1000, (batch_size, seq_length), device=device)
        attention_mask = torch.ones(batch_size, seq_length, device=device)
        pixel_values = torch.randn(batch_size, 3, 224, 224, device=device)  # å‡è®¾å›¾åƒå°ºå¯¸
        labels = torch.randint(0, 10, (batch_size,), device=device)
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "pixel_values": pixel_values,
            "labels": labels
        }
        
    except Exception as e:
        print(f"åˆ›å»ºè™šæ‹Ÿbatché”™è¯¯: {e}")
        return {}

def profile_model_flops(model, batch_example: Dict) -> float:
    """ä½¿ç”¨PyTorch profilerè·å–æ¨¡å‹å®é™…FLOPsï¼ˆåŒ…æ‹¬å‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
        model.train()
        
        # è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰
        actual_seq_length = _get_actual_sequence_length(model, batch_example)
        
        # åˆ†åˆ«æµ‹é‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„FLOPs
        forward_flops = _profile_forward_flops(model, batch_example)
        backward_flops = _profile_backward_flops(model, batch_example)
        
        total_flops = forward_flops + backward_flops
        
        if total_flops > 0:
            print(f"æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
            print(f"å®é™…åºåˆ—é•¿åº¦(åŒ…å«visual tokens): {actual_seq_length}")
            print(f"å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
            print(f"åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
            print(f"æ€»FLOPs: {total_flops:.2e}")
        else:
            print("æ— æ³•é€šè¿‡profileræµ‹é‡FLOPsï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            total_flops = _estimate_flops_fallback(model, batch_example, actual_seq_length)
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPs profilingé”™è¯¯: {e}")
        # å°è¯•è·å–åºåˆ—é•¿åº¦ç”¨äºä¼°ç®—
        try:
            actual_seq_length = _get_actual_sequence_length(model, batch_example)
            return _estimate_flops_fallback(model, batch_example, actual_seq_length)
        except:
            return _estimate_flops_fallback(model, batch_example)

def _profile_forward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡å‰å‘ä¼ æ’­çš„FLOPs"""
    try:
        model.eval()  # ä½¿ç”¨evalæ¨¡å¼é¿å…dropoutç­‰å½±å“FLOPsè®¡ç®—
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                with torch.no_grad():
                    # ä»…æ‰§è¡Œå‰å‘ä¼ æ’­
                    outputs = model(**batch_example)
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            return 0.0
        
    except Exception as e:
        print(f"å‰å‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _profile_backward_flops(model, batch_example: Dict) -> float:
    """æµ‹é‡åå‘ä¼ æ’­çš„FLOPs"""
    try:
        model.train()  # è®­ç»ƒæ¨¡å¼
        
        # å…ˆæ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆä¸åœ¨profilerä¸­ï¼‰
        outputs = model(**batch_example)
        loss = outputs.loss
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒwith_flops
        try:
            # æµ‹é‡åå‘ä¼ æ’­çš„FLOPs
            with torch.profiler.profile(
                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                record_shapes=True,
                with_flops=True,
                profile_memory=False
            ) as prof:
                # ä»…æ‰§è¡Œåå‘ä¼ æ’­
                loss.backward()
            
            # æ¸…ç†æ¢¯åº¦
            model.zero_grad()
            
            # è·å–FLOPsç»Ÿè®¡
            flops = 0
            for event in prof.events():
                if hasattr(event, 'flops') and event.flops > 0:
                    flops += event.flops
            
            return float(flops)
            
        except (AttributeError, TypeError) as e:
            print(f"PyTorch profilerä¸æ”¯æŒwith_flopså‚æ•°: {e}")
            model.zero_grad()  # ç¡®ä¿æ¸…ç†æ¢¯åº¦
            return 0.0
        
    except Exception as e:
        print(f"åå‘ä¼ æ’­FLOPsæµ‹é‡é”™è¯¯: {e}")
        return 0.0

def _get_actual_sequence_length(model, batch_example: Dict) -> int:
    """è·å–å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬visual tokens + text tokensï¼‰"""
    try:
        # ä¸´æ—¶è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼ä»¥è·å–è¾“å‡ºshape
        model.eval()
        
        with torch.no_grad():
            # æ‰§è¡Œå‰å‘ä¼ æ’­è·å–è¾“å‡º
            outputs = model(**batch_example)
            # è·å–å®é™…çš„åºåˆ—é•¿åº¦
            actual_seq_length = outputs.last_hidden_state.size(1)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()
        
        return actual_seq_length
        
    except Exception as e:
        print(f"è·å–å®é™…åºåˆ—é•¿åº¦é”™è¯¯: {e}")
        # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨æ–‡æœ¬é•¿åº¦ä½œä¸ºè¿‘ä¼¼
        return batch_example['input_ids'].size(1)

def _estimate_visual_tokens_count(batch_example: Dict) -> int:
    """ä¼°ç®—è§†è§‰tokensçš„æ•°é‡"""
    try:
        # å¯¹äºQwen2.5-VLï¼Œvisual tokensæ•°é‡é€šå¸¸åŸºäºå›¾åƒåˆ†è¾¨ç‡
        # å¦‚æœæœ‰image_grid_thwå‚æ•°ï¼Œä½¿ç”¨å®ƒæ¥è®¡ç®—
        if 'image_grid_thw' in batch_example:
            # image_grid_thwçš„å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, 3]ï¼Œå…¶ä¸­3ä»£è¡¨ [tiles, height, width]
            grid_thw = batch_example['image_grid_thw']
            if grid_thw.dim() == 2 and grid_thw.size(1) == 3:
                # è®¡ç®—æ¯ä¸ªå›¾åƒçš„visual tokensæ•°é‡ (tiles * height * width)
                visual_tokens_per_image = grid_thw[:, 0] * grid_thw[:, 1] * grid_thw[:, 2]
                # è¿”å›batchä¸­ç¬¬ä¸€ä¸ªå›¾åƒçš„visual tokensæ•°é‡ï¼ˆå‡è®¾batchå†…æ‰€æœ‰å›¾åƒç›¸åŒï¼‰
                return int(visual_tokens_per_image[0].item())
        
        # å¦‚æœæ²¡æœ‰image_grid_thwï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
        # å¯¹äºæ ‡å‡†çš„VLæ¨¡å‹ï¼Œä¸€èˆ¬ä¸€å¼ å›¾ç‰‡ä¼šäº§ç”Ÿå‡ ç™¾åˆ°å‡ åƒä¸ªvisual tokens
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¿å®ˆçš„ä¼°ç®—å€¼
        if 'pixel_values' in batch_example and batch_example['pixel_values'] is not None:
            # åŸºäºåƒç´ å€¼çš„å½¢çŠ¶è¿›è¡Œä¼°ç®—
            pixel_values = batch_example['pixel_values']
            if pixel_values.dim() >= 3:
                # ä¸€èˆ¬æ¥è¯´ï¼Œvisual tokensæ•°é‡ä¸å›¾åƒåˆ†è¾¨ç‡ç›¸å…³
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„ä¼°ç®—å…¬å¼
                return 576  # å¸¸è§çš„vision transformer patchæ•°é‡ (24*24)
        
        return 0  # å¦‚æœæ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œè¿”å›0
        
    except Exception as e:
        print(f"ä¼°ç®—visual tokensæ•°é‡é”™è¯¯: {e}")
        return 0

def _estimate_flops_fallback(model, batch_example: Dict, actual_seq_length: int = None) -> float:
    """å¤‡é€‰çš„FLOPsä¼°ç®—æ–¹æ³•ï¼ˆå‰å‘+åå‘ä¼ æ’­ï¼‰"""
    try:
        # è·å–æ¨¡å‹å‚æ•°æ•°é‡
        if hasattr(model, 'module'):
            param_count = sum(p.numel() for p in model.module.parameters())
        else:
            param_count = sum(p.numel() for p in model.parameters())
        
        # è·å–batch size
        batch_size = batch_example['input_ids'].size(0)
        
        # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¼°ç®—
        if actual_seq_length is not None:
            seq_length = actual_seq_length
        else:
            # ä¼°ç®—åºåˆ—é•¿åº¦ = æ–‡æœ¬tokens + visual tokens
            text_length = batch_example['input_ids'].size(1)
            visual_tokens = _estimate_visual_tokens_count(batch_example)
            seq_length = text_length + visual_tokens
        
        # æ›´å‡†ç¡®çš„FLOPsä¼°ç®—ï¼ˆåŸºäºTransformeræ¶æ„ï¼‰
        # å‰å‘ä¼ æ’­FLOPsä¼°ç®—
        forward_flops = _estimate_forward_flops(param_count, batch_size, seq_length)
        
        # åå‘ä¼ æ’­FLOPsä¼°ç®—ï¼ˆé€šå¸¸æ˜¯å‰å‘ä¼ æ’­çš„2å€ï¼‰
        backward_flops = 2 * forward_flops
        
        total_flops = forward_flops + backward_flops
        
        print(f"ä½¿ç”¨ä¼°ç®—æ–¹æ³•:")
        print(f"  å‚æ•°æ•°é‡: {param_count:.2e}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  æ–‡æœ¬tokensé•¿åº¦: {batch_example['input_ids'].size(1)}")
        if actual_seq_length is not None:
            print(f"  å®é™…åºåˆ—é•¿åº¦: {seq_length}")
        else:
            estimated_visual = _estimate_visual_tokens_count(batch_example)
            print(f"  ä¼°ç®—visual tokens: {estimated_visual}")
            print(f"  ä¼°ç®—æ€»åºåˆ—é•¿åº¦: {seq_length}")
        print(f"  å‰å‘ä¼ æ’­FLOPs: {forward_flops:.2e}")
        print(f"  åå‘ä¼ æ’­FLOPs: {backward_flops:.2e}")
        print(f"  æ€»FLOPs: {total_flops:.2e}")
        
        return float(total_flops)
        
    except Exception as e:
        print(f"FLOPsä¼°ç®—é”™è¯¯: {e}")
        return 0.0

def _estimate_forward_flops(param_count: int, batch_size: int, seq_length: int) -> float:
    """ä¼°ç®—å‰å‘ä¼ æ’­çš„FLOPs"""
    # å¯¹äºTransformeræ¨¡å‹ï¼Œå‰å‘ä¼ æ’­çš„FLOPsä¸»è¦æ¥è‡ªï¼š
    # 1. çŸ©é˜µä¹˜æ³•ï¼ˆçº¿æ€§å±‚ï¼‰
    # 2. æ³¨æ„åŠ›æœºåˆ¶
    # 3. æ¿€æ´»å‡½æ•°ç­‰
    
    # ç®€åŒ–ä¼°ç®—ï¼šæ¯ä¸ªå‚æ•°åœ¨å‰å‘ä¼ æ’­ä¸­å¤§çº¦å‚ä¸2æ¬¡ä¹˜æ³•è¿ç®—
    # å¯¹äºmultimodalæ¨¡å‹ï¼Œè€ƒè™‘è§†è§‰å’Œæ–‡æœ¬çš„äº¤äº’ï¼Œä½¿ç”¨ç¨é«˜çš„ç³»æ•°
    flops_per_token = 2.5 * param_count
    total_flops = flops_per_token * batch_size * seq_length
    
    return float(total_flops)

def get_gpu_stats():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    gpu_stats = {}
    try:
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                memory_reserved = torch.cuda.memory_reserved(i) / (1024**3)    # GB
                memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                
                # è·å–GPUåˆ©ç”¨ç‡ (è¿‘ä¼¼å€¼ï¼ŒåŸºäºå†…å­˜ä½¿ç”¨)
                memory_utilization = (memory_allocated / memory_total) * 100
                
                gpu_stats[f'gpu_{i}'] = {
                    'memory_allocated_gb': memory_allocated,
                    'memory_reserved_gb': memory_reserved,
                    'memory_total_gb': memory_total,
                    'memory_utilization_percent': memory_utilization
                }
                
                # å°è¯•è·å–GPUæ¸©åº¦å’ŒåŠŸè€— (å¦‚æœå¯ç”¨)
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # W
                    gpu_stats[f'gpu_{i}']['temperature_c'] = temp
                    gpu_stats[f'gpu_{i}']['power_usage_w'] = power
                except:
                    pass  # pynvmlä¸å¯ç”¨æ—¶è·³è¿‡
                    
    except Exception as e:
        print(f"GPUçŠ¶æ€è·å–é”™è¯¯: {e}")
    
    return gpu_stats

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨ï¼ˆæ”¯æŒwandbï¼‰"""
    
    def __init__(self, output_dir: str, config: Dict = None, log_file: str = "training_log.json", flops_profile_freq: int = 500):
        self.output_dir = output_dir
        self.log_file = os.path.join(output_dir, log_file)
        self.step_logs = []
        self.epoch_logs = []
        self.start_time = None
        self.step_start_time = None
        self.config = config or {}
        
        # FLOPs profilingé¢‘ç‡é…ç½®
        self.flops_profile_freq = flops_profile_freq
        
        # åˆå§‹åŒ–ç›‘æ§é¢‘ç‡é…ç½®
        self._init_monitor_frequencies()
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–wandb
        self.use_wandb = False
        self._init_wandb()
        
        # MFUè®¡ç®—ç›¸å…³å‚æ•°
        self.model_ref = None
        self.seq_length = config.get('model', {}).get('max_sequence_length', 512)
        
        # æ­£ç¡®è·å–batch_size - ä¼˜å…ˆä»DeepSpeedé…ç½®è·å–
        self.batch_size = self._get_effective_batch_size(config)
        
        self.actual_flops = None  # å­˜å‚¨å®é™…æµ‹é‡çš„FLOPs
        self.actual_seq_length = None  # å­˜å‚¨å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆåŒ…å«visual tokensï¼‰
        
        print(f"ğŸ“Š TrainingMonitoråˆå§‹åŒ–: batch_size={self.batch_size}, flops_profile_freq={self.flops_profile_freq}")
    
    def _init_monitor_frequencies(self):
        """åˆå§‹åŒ–ç›‘æ§é¢‘ç‡é…ç½® - æ‰€æœ‰é¢‘ç‡ç‹¬ç«‹è®¾ç½®"""
        # ä»configä¸­è·å–monitoré¢‘ç‡é…ç½®
        monitor_config = self.config.get('monitor', {})
        freq_config = monitor_config.get('freq', {})
        
        # ğŸ”¥ æ‰€æœ‰é¢‘ç‡éƒ½ä»monitor.freqä¸­ç‹¬ç«‹è®¾ç½®
        self.freq = {
            'training_log_freq': freq_config.get('training_log_freq', 10),           # è®­ç»ƒæŒ‡æ ‡è®°å½•é¢‘ç‡
            'perf_log_freq': freq_config.get('perf_log_freq', 20),                   # æ€§èƒ½æŒ‡æ ‡è®°å½•é¢‘ç‡
            'gpu_log_freq': freq_config.get('gpu_log_freq', 50),                     # GPUç›‘æ§é¢‘ç‡
            'local_save_freq': freq_config.get('local_save_freq', 200),              # æœ¬åœ°ä¿å­˜é¢‘ç‡
            'progress_update_freq': freq_config.get('progress_update_freq', 10),     # è¿›åº¦æ›´æ–°é¢‘ç‡
            'eval_log_freq': freq_config.get('eval_log_freq', 1),                    # è¯„ä¼°æŒ‡æ ‡è®°å½•é¢‘ç‡
        }
        
        # flops_profile_freqç‹¬ç«‹é…ç½®
        if hasattr(self, 'flops_profile_freq'):
            # æ„é€ å‡½æ•°å·²ç»è®¾ç½®äº†flops_profile_freqï¼Œä¿æŒä¸å˜
            pass
        else:
            # ä»é…ç½®ä¸­è·å–flops_profile_freqï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            self.flops_profile_freq = freq_config.get('flops_profile_freq', 500)
        
        # æ‰“å°ç›‘æ§é¢‘ç‡é…ç½®
        print(f"ğŸ”§ ç›‘æ§é¢‘ç‡é…ç½®:")
        for key, value in self.freq.items():
            print(f"   {key}: æ¯{value}æ­¥")
        print(f"   flops_profile_freq: æ¯{self.flops_profile_freq}æ­¥")
        
        # æ£€æŸ¥é¢‘ç‡è®¾ç½®æ˜¯å¦åˆç†
        if self.freq['training_log_freq'] > 100:
            print(f"âš ï¸  å»ºè®®ï¼štraining_log_freq={self.freq['training_log_freq']}å¯èƒ½å¤ªé«˜ï¼Œå»ºè®®è®¾ç½®ä¸º1-50ä»¥ç¡®ä¿æŒ‡æ ‡æ­£å¸¸æ˜¾ç¤º")
        if self.freq['perf_log_freq'] > 200:
            print(f"âš ï¸  å»ºè®®ï¼šperf_log_freq={self.freq['perf_log_freq']}å¯èƒ½å¤ªé«˜ï¼Œå»ºè®®è®¾ç½®ä¸º10-100ä»¥ç¡®ä¿æ€§èƒ½æŒ‡æ ‡æ­£å¸¸æ˜¾ç¤º")
    
    def _get_effective_batch_size(self, config: Dict) -> int:
        """æ­£ç¡®è·å–æœ‰æ•ˆçš„batch size"""
        try:
            # é¦–å…ˆå°è¯•ä»DeepSpeedé…ç½®è·å–
            deepspeed_config = config.get('deepspeed', {})
            if isinstance(deepspeed_config, str):
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
                import json
                with open(deepspeed_config, 'r') as f:
                    deepspeed_config = json.load(f)
            
            # ä¼˜å…ˆä½¿ç”¨DeepSpeedçš„train_batch_sizeï¼ˆè¿™æ˜¯çœŸæ­£çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼‰
            if 'train_batch_size' in deepspeed_config:
                batch_size = deepspeed_config['train_batch_size']
                print(f"ğŸ“Š ä»DeepSpeedé…ç½®è·å–batch_size: {batch_size}")
                return batch_size
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šä»train_micro_batch_size_per_gpuè®¡ç®—
            if 'train_micro_batch_size_per_gpu' in deepspeed_config:
                micro_batch = deepspeed_config['train_micro_batch_size_per_gpu']
                gradient_accumulation = deepspeed_config.get('gradient_accumulation_steps', 1)
                
                # è®¡ç®—ä¸–ç•Œå¤§å°
                try:
                    import torch.distributed as dist
                    if dist.is_available() and dist.is_initialized():
                        world_size = dist.get_world_size()
                    else:
                        world_size = 1
                except:
                    world_size = 1
                
                effective_batch_size = micro_batch * gradient_accumulation * world_size
                print(f"ğŸ“Š è®¡ç®—å¾—åˆ°batch_size: {micro_batch} x {gradient_accumulation} x {world_size} = {effective_batch_size}")
                return effective_batch_size
            
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä»æ ¹é…ç½®è·å–
            if 'train_batch_size' in config:
                batch_size = config['train_batch_size']
                print(f"ğŸ“Š ä»æ ¹é…ç½®è·å–batch_size: {batch_size}")
                return batch_size
            
            # é»˜è®¤å€¼
            print(f"ğŸ“Š ä½¿ç”¨é»˜è®¤batch_size: 32")
            return 32
            
        except Exception as e:
            print(f"âš ï¸  è·å–batch_sizeå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼32")
            return 32
    
    def _is_main_process(self):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹"""
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                rank = dist.get_rank()
                is_main = rank == 0
                # åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æˆ–éä¸»è¿›ç¨‹æ—¶æ‰“å°
                if not hasattr(self, '_main_process_checked') or not is_main:
                    print(f"ğŸ” åˆ†å¸ƒå¼è®­ç»ƒ: rank={rank}, is_main_process={is_main}")
                    self._main_process_checked = True
                return is_main
            else:
                # åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰“å°
                if not hasattr(self, '_main_process_checked'):
                    print(f"ğŸ” å•GPUè®­ç»ƒ: is_main_process=True")
                    self._main_process_checked = True
                return True  # éåˆ†å¸ƒå¼è®­ç»ƒæ—¶é»˜è®¤ä¸ºä¸»è¿›ç¨‹
        except ImportError:
            if not hasattr(self, '_main_process_checked'):
                print(f"ğŸ” torch.distributedä¸å¯ç”¨: is_main_process=True")
                self._main_process_checked = True
            return True
    
    def _init_wandb(self):
        """åˆå§‹åŒ–wandbï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰"""
        if not WANDB_AVAILABLE:
            return
        
        wandb_config = self.config.get('wandb', {})
        if not wandb_config.get('enabled', False):
            print("wandb logging disabled in config")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¦‚æœæ˜¯åˆ™åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–wandb
        is_main_process = True
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                # åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰rank 0è¿›ç¨‹åˆå§‹åŒ–wandb
                is_main_process = dist.get_rank() == 0
                if not is_main_process:
                    print(f"è¿›ç¨‹ rank {dist.get_rank()}: è·³è¿‡wandbåˆå§‹åŒ–ï¼ˆéä¸»è¿›ç¨‹ï¼‰")
                    return
        except ImportError:
            # å¦‚æœtorch.distributedä¸å¯ç”¨ï¼Œé»˜è®¤ä¸ºä¸»è¿›ç¨‹
            pass
        
        if not is_main_process:
            return
        
        try:
            # åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–wandb
            wandb.init(
                project=wandb_config.get('project', 'qwen_classification'),
                name=wandb_config.get('run_name'),
                tags=wandb_config.get('tags', []),
                notes=wandb_config.get('notes'),
                config=self.config,
                resume="allow"  # å…è®¸æ¢å¤
            )
            
            # è®°å½•æ¨¡å‹å’Œè®­ç»ƒé…ç½®
            try:
                if 'model' in self.config:
                    wandb.config.update({
                        'model_name': self.config['model'].get('pretrained_name', 'unknown'),
                        'num_labels': self.config['model'].get('num_labels', 'unknown')
                    })
                
                if 'training' in self.config:
                    wandb.config.update({
                        'learning_rate': self.config['training'].get('learning_rate', 'unknown'),
                        'num_epochs': self.config['training'].get('num_epochs', 'unknown'),
                        'batch_size': self.config.get('train_batch_size', 'unknown')
                    })
            except Exception as config_error:
                print(f"âš ï¸  wandbé…ç½®æ›´æ–°å¤±è´¥: {config_error}")
            
            self.use_wandb = True
            print("âœ… wandb initialized successfully")
            
            # æ˜¾ç¤ºwandbé“¾æ¥ä¿¡æ¯
            try:
                if wandb.run is not None:
                    print(f"ğŸ“Š wandb project: {wandb.run.project}")
                    print(f"ğŸ”— wandb run: {wandb.run.name}")
                    print(f"ğŸš€ View run at: {wandb.run.url}")
                    
                    # æ„å»ºé¡¹ç›®é“¾æ¥
                    if hasattr(wandb.run, 'entity') and hasattr(wandb.run, 'project'):
                        project_url = f"https://wandb.ai/{wandb.run.entity}/{wandb.run.project}"
                        print(f"â­ View project at: {project_url}")
                    
                    # å®šä¹‰evalæŒ‡æ ‡
                    self._define_eval_metrics()
                    
                    # å¼ºåˆ¶åˆ›å»ºevalå›¾è¡¨
                    self._create_eval_charts()
                    
                    # åˆ›å»ºè¯¦ç»†å›¾è¡¨
                    self._create_detailed_charts()
                    
                    # ğŸ”¥ å¼ºåˆ¶æäº¤åˆå§‹åŒ–æ•°æ®ï¼Œç¡®ä¿evalæŒ‡æ ‡è¢«WandBè¯†åˆ«
                    wandb.log({}, commit=True)
                    print("ğŸ”§ WandBåˆå§‹åŒ–æ•°æ®å·²æäº¤")
            except Exception as display_error:
                print(f"âš ï¸  wandbé“¾æ¥æ˜¾ç¤ºå¤±è´¥: {display_error}")
            
        except Exception as e:
            print(f"âŒ Failed to initialize wandb: {e}")
            self.use_wandb = False
    
    def set_model_ref(self, model):
        """è®¾ç½®æ¨¡å‹å¼•ç”¨ï¼Œç”¨äºMFUè®¡ç®—"""
        self.model_ref = model
    
    def _define_eval_metrics(self):
        """å®šä¹‰evalæŒ‡æ ‡ï¼Œç¡®ä¿wandbæ­£ç¡®è¯†åˆ«å’Œæ˜¾ç¤º"""
        try:
            if not self.use_wandb or not self._is_main_process():
                return
            
            import wandb
            if wandb.run is None:
                return
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ†åˆ«å®šä¹‰trainingå’ŒevalæŒ‡æ ‡ï¼Œä½¿ç”¨ç»Ÿä¸€çš„xè½´
            wandb.define_metric("step")
            wandb.define_metric("training/*", step_metric="step")
            wandb.define_metric("eval/*", step_metric="step")
            wandb.define_metric("perf/*", step_metric="step")
            
            print("âœ… å·²å®šä¹‰ç»Ÿä¸€xè½´ï¼štraining/*, eval/*, perf/* æŒ‡æ ‡ä½¿ç”¨'step'")
            
        except Exception as e:
            print(f"âš ï¸  å®šä¹‰evalæŒ‡æ ‡å¤±è´¥: {e}")
    
    def _create_eval_charts(self):
        """å¼ºåˆ¶ç¡®ä¿evalå›¾è¡¨åœ¨wandbç•Œé¢ä¸­å¯è§"""
        try:
            if not self.use_wandb or not self._is_main_process():
                return
            
            import wandb
            if wandb.run is None:
                return
            
            # ğŸ”¥ æ–°ç­–ç•¥ï¼šä¸å†å¼ºåˆ¶è®°å½•å ä½ç¬¦æ•°æ®ï¼Œè€Œæ˜¯ä¾èµ–metricå®šä¹‰å’Œé¦–æ¬¡çœŸå®evalæ•°æ®
            # è¿™æ ·å¯ä»¥é¿å…å›¾è¡¨ä¸­å‡ºç°ä¸ç›¸å…³çš„åˆå§‹å€¼
            
            # å‡†å¤‡evalæŒ‡æ ‡åˆ—è¡¨ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
            eval_metrics_list = [
                "eval/overall_loss",
                "eval/overall_accuracy", 
                "eval/overall_samples",
                "eval/overall_correct"
            ]
            
            # å¦‚æœæœ‰å¤šæ•°æ®é›†é…ç½®ï¼Œä¹Ÿæ·»åŠ å¯¹åº”çš„æŒ‡æ ‡
            dataset_configs = self.config.get('datasets', {}).get('dataset_configs', {})
            for dataset_name in dataset_configs.keys():
                eval_metrics_list.extend([
                    f"eval/{dataset_name}_loss",
                    f"eval/{dataset_name}_accuracy",
                    f"eval/{dataset_name}_samples"
                ])
            
            print(f"ğŸ“Š evalå›¾è¡¨å·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…é¦–æ¬¡è¯„ä¼°æ•°æ® - æŒ‡æ ‡: {eval_metrics_list}")
            
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºevalå›¾è¡¨å¤±è´¥: {e}")
    
    def _create_detailed_charts(self):
        """åˆ›å»ºè¯¦ç»†çš„è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡å›¾è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸è®°å½•åˆå§‹æ•°æ®"""
        try:
            if not self.use_wandb or not self._is_main_process():
                return
            
            # ç§»é™¤åˆå§‹æ•°æ®è®°å½•ï¼Œé¿å…step=0çš„é—®é¢˜
            # wandbä¼šåœ¨ç¬¬ä¸€æ¬¡çœŸå®æ•°æ®è®°å½•æ—¶è‡ªåŠ¨åˆ›å»ºå›¾è¡¨
            print("âœ… å›¾è¡¨å°†åœ¨å®é™…æ•°æ®è®°å½•æ—¶è‡ªåŠ¨åˆ›å»º")
            
        except Exception as e:
            print(f"å›¾è¡¨å‡†å¤‡å¤±è´¥: {e}")

    def _ensure_eval_charts_visible(self):
        """ç¡®ä¿evalå›¾è¡¨åœ¨wandbç•Œé¢ä¸­å¯è§ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘é¢å¤–è®°å½•"""
        try:
            if not self.use_wandb or not self._is_main_process():
                return
            
            # ç§»é™¤é¢å¤–çš„chart_visibility_checkè®°å½•ï¼Œé¿å…stepå†²çª
            print("âœ… evalå›¾è¡¨å°†åœ¨ç¬¬ä¸€æ¬¡è¯„ä¼°æ—¶è‡ªåŠ¨æ˜¾ç¤º")
            
        except Exception as e:
            print(f"âš ï¸  ç¡®ä¿evalå›¾è¡¨å¯è§æ€§å¤±è´¥: {e}")
    
    def profile_model_flops(self, batch_example: Dict):
        """æµ‹é‡æ¨¡å‹çš„å®é™…FLOPs"""
        if self.model_ref is None:
            print("æ¨¡å‹å¼•ç”¨æœªè®¾ç½®ï¼Œæ— æ³•æµ‹é‡FLOPs")
            return
        
        print("æ­£åœ¨æµ‹é‡æ¨¡å‹å®é™…FLOPs...")
        self.actual_flops = profile_model_flops(self.model_ref, batch_example)
        
        # åŒæ—¶è·å–å®é™…åºåˆ—é•¿åº¦
        try:
            self.actual_seq_length = _get_actual_sequence_length(self.model_ref, batch_example)
            print(f"âœ… å®é™…åºåˆ—é•¿åº¦: {self.actual_seq_length}")
        except Exception as e:
            print(f"âŒ è·å–åºåˆ—é•¿åº¦å¤±è´¥: {e}")
        
        if self.actual_flops > 0:
            print(f"âœ… æ¨¡å‹å®é™…FLOPs: {self.actual_flops:.2e}")
            
            # æ˜¾ç¤ºMFUè®¡ç®—ç›¸å…³ä¿¡æ¯
            self._show_mfu_calculation_info()
        else:
            print("âŒ FLOPsæµ‹é‡å¤±è´¥ï¼ŒMFUè®¡ç®—å°†è¢«ç¦ç”¨")
    
    def _show_mfu_calculation_info(self):
        """æ˜¾ç¤ºMFUè®¡ç®—çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            if self.actual_flops is None or self.actual_flops <= 0:
                return
            
            print(f"\nğŸ“Š MFUè®¡ç®—é…ç½®ä¿¡æ¯:")
            print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"  â€¢ å®é™…åºåˆ—é•¿åº¦: {self.actual_seq_length}")
            print(f"  â€¢ å®é™…FLOPs: {self.actual_flops:.2e}")
            
            # è·å–GPUå³°å€¼æ€§èƒ½
            peak_flops = get_gpu_peak_flops()
            print(f"  â€¢ GPUå³°å€¼æ€§èƒ½: {peak_flops:.2e} FLOPs/s")
            
            # ä¼°ç®—ä¸€ä¸ªæ ·æœ¬çš„MFU (å‡è®¾1ç§’çš„step time)
            sample_step_time = 1.0
            sample_mfu = calculate_mfu(self.model_ref, self.batch_size, self.actual_seq_length, sample_step_time, self.actual_flops)
            print(f"  â€¢ ç†è®ºæœ€å¤§MFU (1ç§’/æ­¥): {sample_mfu:.4f} ({sample_mfu*100:.2f}%)")
            
            # è®¡ç®—è¾¾åˆ°ç›®æ ‡MFUæ‰€éœ€çš„æ­¥éª¤æ—¶é—´
            target_mfus = [0.1, 0.2, 0.3, 0.5]
            print(f"  â€¢ è¾¾åˆ°ç›®æ ‡MFUæ‰€éœ€çš„æ­¥éª¤æ—¶é—´:")
            for target_mfu in target_mfus:
                required_time = self.actual_flops / (target_mfu * peak_flops)
                print(f"    - {target_mfu*100:.0f}% MFU: {required_time:.3f}ç§’/æ­¥")
                
        except Exception as e:
            print(f"æ˜¾ç¤ºMFUä¿¡æ¯é”™è¯¯: {e}")
    
    def set_actual_flops(self, flops: float, seq_length: int = None):
        """è®¾ç½®å®é™…FLOPsï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„åŒæ­¥ï¼‰"""
        self.actual_flops = flops
        if seq_length is not None:
            self.actual_seq_length = seq_length
    
    def _calculate_actual_seq_length(self, attention_mask):
        """åŠ¨æ€è®¡ç®—å½“å‰batchçš„å®é™…åºåˆ—é•¿åº¦"""
        try:
            if attention_mask is None:
                return self.seq_length
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦
            valid_lengths = attention_mask.sum(dim=1)  # [batch_size]
            
            # ä½¿ç”¨æ‰¹æ¬¡ä¸­çš„å¹³å‡æœ‰æ•ˆé•¿åº¦ï¼ˆæˆ–æœ€å¤§é•¿åº¦ï¼‰
            # è¿™é‡Œä½¿ç”¨å¹³å‡å€¼ï¼Œå› ä¸ºMFUé€šå¸¸å…³å¿ƒçš„æ˜¯æ•´ä½“ååé‡
            avg_seq_length = valid_lengths.float().mean().item()
            
            return int(avg_seq_length)
            
        except Exception as e:
            print(f"è®¡ç®—å®é™…åºåˆ—é•¿åº¦é”™è¯¯: {e}")
            return self.actual_seq_length if self.actual_seq_length is not None else self.seq_length
    
    def start_training(self):
        """å¼€å§‹è®­ç»ƒ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘åˆå§‹è®°å½•"""
        self.start_time = time.time()
        self.step_start_time = time.time()
        
        # ç§»é™¤training/startedè®°å½•ï¼Œå‡å°‘WandBè°ƒç”¨
        print("âœ… è®­ç»ƒç›‘æ§å·²å¯åŠ¨")
    
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float, attention_mask=None, real_time_flops=None, skip_wandb=False):
        """è®°å½•è®­ç»ƒæ­¥éª¤ - ä¿®å¤WandBè®°å½•é¢‘ç‡ï¼Œç¡®ä¿perfå’Œtrainingç»„æŒ‡æ ‡æ­£å¸¸æ˜¾ç¤º
        
        Args:
            skip_wandb: å¦‚æœä¸ºTrueï¼Œè·³è¿‡wandbè®°å½•ï¼ˆç”¨äºevalæ­¥éª¤æ—¶é¿å…é‡å¤è®°å½•ï¼‰
        """
        current_time = time.time()
        step_time = current_time - self.step_start_time
        
        # å¦‚æœæä¾›äº†å®æ—¶FLOPsï¼Œæ›´æ–°å½“å‰FLOPså€¼
        if real_time_flops is not None and real_time_flops > 0:
            self.actual_flops = real_time_flops
        
        log_entry = {
            'step': int(step),
            'epoch': int(epoch),
            'loss': float(loss),
            'grad_norm': float(grad_norm),
            'learning_rate': float(learning_rate),
            'step_time': float(step_time),
            'timestamp': float(current_time)
        }
        
        # å¦‚æœæœ‰å®æ—¶FLOPsï¼Œä¹Ÿè®°å½•åˆ°æ—¥å¿—ä¸­
        if real_time_flops is not None:
            log_entry['real_time_flops'] = float(real_time_flops)
        
        self.step_logs.append(log_entry)
        
        # ğŸ”¥ ä¿®å¤WandBè®°å½•é¢‘ç‡ - ç¡®ä¿trainingå’Œperfç»„æŒ‡æ ‡æ­£å¸¸æ˜¾ç¤ºï¼Œä½¿ç”¨åŠ¨æ€é¢‘ç‡
        if self.use_wandb and self._is_main_process() and not skip_wandb:
            # ä½¿ç”¨åŠ¨æ€é¢‘ç‡é…ç½®
            should_log_training = (step % self.freq['training_log_freq'] == 0)
            
            if should_log_training:
                # å‡†å¤‡åŸºç¡€è®­ç»ƒæŒ‡æ ‡
                wandb_data = {
                    "training/loss": float(loss),
                    "training/lr": float(learning_rate), 
                    "training/epoch": float(epoch),
                    "training/grad_norm": float(grad_norm),
                    "step": int(step)  # ğŸ”¥ æ·»åŠ ç»Ÿä¸€çš„stepå­—æ®µ
                }
                
                # ä½¿ç”¨åŠ¨æ€æ€§èƒ½æŒ‡æ ‡é¢‘ç‡
                should_log_perf = (step % self.freq['perf_log_freq'] == 0)
                
                if should_log_perf:
                    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡åˆ°perfç»„
                    wandb_data.update({
                        "perf/step_time": float(step_time),
                        "perf/steps_per_second": float(1.0 / step_time) if step_time > 0 else 0.0,
                    })
                    
                    # MFUå’ŒFLOPsç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰- ç»Ÿä¸€åœ¨perfç»„è®°å½•
                    if self.model_ref is not None and self.actual_flops is not None:
                        # ä¼˜å…ˆä½¿ç”¨å½“å‰batchçš„å®é™…åºåˆ—é•¿åº¦
                        if attention_mask is not None:
                            current_seq_length = self._calculate_actual_seq_length(attention_mask)
                        elif self.actual_seq_length is not None:
                            current_seq_length = self.actual_seq_length
                        else:
                            current_seq_length = self.seq_length
                        
                        # ä½¿ç”¨profilerè®¡ç®—MFU
                        if step % self.flops_profile_freq == 0:
                            # æ¯flops_profile_freqæ­¥ä½¿ç”¨profilerè®¡ç®—MFU
                            mfu = calculate_mfu_with_profiler(self.model_ref, self.batch_size, current_seq_length, step_time)
                            print(f"ğŸ” æ­¥éª¤ {step}: ä½¿ç”¨profilerè®¡ç®—MFU = {mfu:.4f}")
                        else:
                            # å…¶ä»–æ­¥éª¤ä½¿ç”¨ç¼“å­˜çš„MFUå€¼æˆ–è¿”å›0
                            mfu = 0.0
                        
                        # æ·»åŠ æ€§èƒ½ç›¸å…³æŒ‡æ ‡åˆ°perfç»„
                        current_flops = self.actual_flops if self.actual_flops is not None else 0.0
                        wandb_data.update({
                            "perf/mfu": float(mfu),
                            "perf/mfu_percent": float(mfu * 100),
                            "perf/tokens_per_second": float(self.batch_size * current_seq_length / step_time),
                            "perf/samples_per_second": float(self.batch_size / step_time),
                            "perf/actual_flops": float(current_flops),
                            "perf/actual_seq_length": float(current_seq_length),
                            "perf/flops_per_second": float(current_flops / step_time) if step_time > 0 else 0.0,
                        })
                        
                        # å¦‚æœæœ‰å®æ—¶FLOPsæµ‹é‡ï¼Œæ·»åŠ æ ‡è®°
                        if real_time_flops is not None:
                            wandb_data["perf/real_time_measurement"] = 1.0
                        else:
                            wandb_data["perf/real_time_measurement"] = 0.0
                
                # ä½¿ç”¨åŠ¨æ€GPUç›‘æ§é¢‘ç‡
                should_log_gpu = (step % self.freq['gpu_log_freq'] == 0)
                if should_log_gpu:
                    try:
                        if torch.cuda.is_available():
                            # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                            memory_allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
                            memory_reserved = torch.cuda.memory_reserved() / (1024**3)    # GB
                            memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                            memory_utilization = (memory_allocated / memory_total) * 100
                            
                            wandb_data.update({
                                "perf/gpu_memory_allocated_gb": float(memory_allocated),
                                "perf/gpu_memory_reserved_gb": float(memory_reserved),
                                "perf/gpu_memory_utilization_percent": float(memory_utilization),
                            })
                    except Exception as e:
                        pass  # å¿½ç•¥GPUç›‘æ§é”™è¯¯
                
                # ä¸€æ¬¡æ€§è®°å½•æ‰€æœ‰æŒ‡æ ‡
                wandb.log(wandb_data, step=int(step), commit=True)
        
        self.step_start_time = current_time
        
        # ä½¿ç”¨åŠ¨æ€æœ¬åœ°æ—¥å¿—ä¿å­˜é¢‘ç‡
        if step % self.freq['local_save_freq'] == 0:
            self.save_logs()
    
    def log_epoch(self, epoch: int, avg_loss: float, elapsed_time: float, current_step: int = None):
        """è®°å½•epochç»Ÿè®¡"""
        log_entry = {
            'epoch': int(epoch),
            'avg_loss': float(avg_loss),
            'elapsed_time': float(elapsed_time),
            'timestamp': float(time.time())
        }
        
        self.epoch_logs.append(log_entry)
        
        # è®°å½•åˆ°wandbï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if self.use_wandb and self._is_main_process():
            log_data = {
                "training/epoch_avg_loss": float(avg_loss),
                "training/epoch_time": float(elapsed_time),
                "training/epoch_number": int(epoch)
            }
            
            # å¦‚æœæä¾›äº†current_stepï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä¸æŒ‡å®šstepè®©wandbè‡ªåŠ¨å¤„ç†
            if current_step is not None:
                wandb.log(log_data, step=int(current_step), commit=True)
            else:
                wandb.log(log_data, commit=True)
        
        self.save_logs()
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float, additional_metrics: dict = None):
        """è®°å½•è¯„ä¼°ç»“æœ - åœ¨evalç»„ä¸­æ˜¾ç¤ºæŒ‡æ ‡"""
        if self.use_wandb and self._is_main_process():
            try:
                import wandb
                if wandb.run is None:
                    print("âš ï¸ WandBæœªåˆå§‹åŒ–ï¼Œè·³è¿‡evalæŒ‡æ ‡è®°å½•")
                    return
                
                log_data = {
                    "eval/overall_loss": float(eval_loss),
                    "eval/overall_accuracy": float(eval_accuracy),
                }
                
                # æ·»åŠ é¢å¤–çš„æŒ‡æ ‡
                if additional_metrics:
                    for key, value in additional_metrics.items():
                        # ç¡®ä¿é¢å¤–æŒ‡æ ‡ä¹Ÿåœ¨evalç»„ä¸­
                        if not key.startswith('eval/'):
                            key = f"eval/{key}"
                        log_data[key] = float(value) if isinstance(value, (int, float)) else value
                
                wandb.log(log_data, step=int(step), commit=True)
                print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²è®°å½•åˆ°WandB (step={step}): {list(log_data.keys())}")
                
            except Exception as e:
                print(f"âŒ è®°å½•evalæŒ‡æ ‡å¤±è´¥: {e}")
    
    def log_metrics(self, metrics: dict, step: int = None, commit: bool = True):
        """é€šç”¨çš„æŒ‡æ ‡è®°å½•æ–¹æ³• - ç¡®ä¿evalæŒ‡æ ‡æ­£ç¡®è®°å½•"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹ä¸”wandbå¯ç”¨
        if not self.use_wandb or not self._is_main_process():
            return
            
        if not WANDB_AVAILABLE:
            return

        # æ£€æŸ¥wandbæ˜¯å¦å·²åˆå§‹åŒ–
        try:
            import wandb
            if wandb.run is None:
                print("âš ï¸ WandBæœªåˆå§‹åŒ–ï¼Œè·³è¿‡æŒ‡æ ‡è®°å½•")
                return
        except Exception as e:
            return

        try:
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            log_data = {}
            eval_metrics_count = 0
            eval_metrics_list = []
            
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    log_data[key] = float(value)
                elif hasattr(value, 'item'):  # torch tensor
                    log_data[key] = float(value.item())
                else:
                    log_data[key] = value
                
                # ç»Ÿè®¡evalæŒ‡æ ‡æ•°é‡å’Œåç§°
                if 'eval/' in key:
                    eval_metrics_count += 1
                    eval_metrics_list.append(key)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æŒ‡æ ‡éƒ½æœ‰ç»Ÿä¸€çš„stepå­—æ®µ
            if step is not None:
                log_data["step"] = int(step)  # æ·»åŠ stepå­—æ®µåˆ°æ•°æ®ä¸­
            
            # è®°å½•æŒ‡æ ‡
            if step is not None:
                wandb.log(log_data, step=int(step), commit=commit)
                step_info = f"step={step}"
            else:
                wandb.log(log_data, commit=commit)
                step_info = "auto-step"
            
            # å¦‚æœåŒ…å«evalæŒ‡æ ‡ï¼Œç‰¹åˆ«è¯´æ˜
            if eval_metrics_count > 0:
                print(f"ğŸ“Š å·²è®°å½• {eval_metrics_count} ä¸ªevalæŒ‡æ ‡åˆ°WandB ({step_info})")
                print(f"   evalæŒ‡æ ‡: {eval_metrics_list}")
                print(f"   åŒ…å«ç»Ÿä¸€step: {log_data.get('step', 'N/A')}")
                
                # ğŸ”¥ é¢å¤–éªŒè¯ï¼šæ£€æŸ¥WandB runçŠ¶æ€
                if wandb.run is not None:
                    # ä½¿ç”¨å…¼å®¹çš„çŠ¶æ€æ£€æŸ¥æ–¹æ³•
                    run_state = getattr(wandb.run, 'state', 'unknown')
                    print(f"   WandB runçŠ¶æ€: {run_state}")
                    print(f"   WandBé¡¹ç›®: {wandb.run.project}")
                    print(f"   WandB run ID: {wandb.run.id}")
                    print(f"   å®é™…è®°å½•çš„æ•°æ®keys: {list(log_data.keys())}")
                    
                    # ğŸ”¥ å¼ºåˆ¶åˆ·æ–°ï¼Œç¡®ä¿æ•°æ®åŒæ­¥
                    try:
                        wandb.log({}, commit=True)  # å¼ºåˆ¶æäº¤
                        print(f"   âœ… å¼ºåˆ¶æäº¤å®Œæˆ")
                    except Exception as flush_error:
                        print(f"   âš ï¸ å¼ºåˆ¶æäº¤å¤±è´¥: {flush_error}")
                else:
                    print("   âš ï¸ WandB runä¸ºNoneï¼")
            
        except Exception as e:
            print(f"âŒ è®°å½•æŒ‡æ ‡åˆ°wandbå¤±è´¥: {e}")
            print(f"   å°è¯•è®°å½•çš„æŒ‡æ ‡: {list(metrics.keys())}")
            print(f"   step: {step}")
            import traceback
            traceback.print_exc()
    
    def save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            logs = {
                'step_logs': self.step_logs,
                'epoch_logs': self.epoch_logs,
                'total_training_time': time.time() - self.start_time if self.start_time else 0
            }
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥JSONåºåˆ—åŒ–
            serializable_logs = make_json_serializable(logs)
            
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_logs, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")
    
    def finish_training(self):
        """ç»“æŸè®­ç»ƒ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘WandBè°ƒç”¨"""
        if self.use_wandb and self._is_main_process():
            # ç®€åŒ–ç»“æŸè®°å½•ï¼Œåªè®°å½•æ€»æ—¶é—´
            total_time = time.time() - self.start_time if self.start_time else 0
            wandb.log({"training/total_time": total_time}, commit=True)
            wandb.finish()
            print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    def get_latest_metrics(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡"""
        if self.step_logs:
            return self.step_logs[-1]
        return None
    
    def get_avg_metrics(self, last_n_steps: int = 100) -> Dict:
        """è·å–æœ€è¿‘Næ­¥çš„å¹³å‡æŒ‡æ ‡"""
        if not self.step_logs:
            return {}
        
        recent_logs = self.step_logs[-last_n_steps:]
        
        if not recent_logs:
            return {}
        
        avg_loss = sum(log['loss'] for log in recent_logs) / len(recent_logs)
        avg_grad_norm = sum(log['grad_norm'] for log in recent_logs) / len(recent_logs)
        avg_step_time = sum(log['step_time'] for log in recent_logs) / len(recent_logs)
        
        return {
            'avg_loss': avg_loss,
            'avg_grad_norm': avg_grad_norm,
            'avg_step_time': avg_step_time,
            'num_steps': len(recent_logs)
        } 

class DummyMonitor:
    """è™šæ‹Ÿç›‘æ§å™¨ï¼Œç”¨äºéä¸»è¿›ç¨‹ï¼Œé¿å…ä¸å¿…è¦çš„wandbæ“ä½œ"""
    
    def __init__(self, output_dir: str, config: Dict = None, log_file: str = "training_log.json"):
        self.output_dir = output_dir
        self.config = config or {}
        self.use_wandb = False
        
        # æ·»åŠ å¿…è¦çš„å±æ€§
        self.actual_flops = None
        self.step_start_time = None
        self.batch_size = config.get('train_batch_size', 32) if config else 32
        
    def start_training(self):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸å¯åŠ¨è®­ç»ƒç›‘æ§"""
        pass
        
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float, attention_mask=None, real_time_flops=None, skip_wandb=False):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®°å½•æ­¥éª¤"""
        pass
    
    def log_epoch(self, epoch: int, avg_loss: float, elapsed_time: float, current_step: int = None):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®°å½•epoch"""
        pass
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float, additional_metrics: dict = None):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®°å½•è¯„ä¼°"""
        pass
    
    def log_metrics(self, metrics: dict, step: int = None, commit: bool = True):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®°å½•æŒ‡æ ‡"""
        pass
    
    def log_dataset_metrics(self, is_eval: bool = True, step: int = None):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®°å½•æ•°æ®é›†æŒ‡æ ‡"""
        pass
    
    def set_actual_flops(self, flops: float, seq_length: int = None):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®¾ç½®FLOPs"""
        pass
    
    def _calculate_actual_seq_length(self, attention_mask):
        """ç©ºå®ç°ï¼Œè¿”å›é»˜è®¤å€¼"""
        return 512  # è¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
    
    def save_logs(self):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸ä¿å­˜æ—¥å¿—"""
        pass
    
    def finish_training(self):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸ç»“æŸwandb"""
        pass
    
    def set_model_ref(self, model):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®¾ç½®æ¨¡å‹å¼•ç”¨"""
        pass
    
    def profile_model_flops(self, batch_example: Dict):
        """ç©ºå®ç°ï¼Œéä¸»è¿›ç¨‹ä¸è®¡ç®—FLOPs"""
        pass
    
    def get_latest_metrics(self):
        """ç©ºå®ç°ï¼Œè¿”å›None"""
        return None
    
    def get_avg_metrics(self, last_n_steps: int = 100):
        """ç©ºå®ç°ï¼Œè¿”å›ç©ºå­—å…¸"""
        return {} 

 