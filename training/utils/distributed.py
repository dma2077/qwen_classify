import os
import torch
import torch.distributed as dist
import time

class DistributedContext:
    """ç®¡ç†åˆ†å¸ƒå¼è®­ç»ƒçš„ä¸Šä¸‹æ–‡"""
    
    def __init__(self):
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        self.rank = int(os.environ.get('RANK', 0))
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.is_main_process = self.rank == 0
        
        # è®¾ç½®CUDAè®¾å¤‡
        if torch.cuda.is_available():
            torch.cuda.set_device(self.local_rank)
            self.device = torch.device(f'cuda:{self.local_rank}')
        else:
            self.device = torch.device('cpu')
    
    def print_info(self):
        """æ‰“å°åˆ†å¸ƒå¼è®­ç»ƒä¿¡æ¯"""
        if self.is_main_process:
            print(f"WORLD_SIZE: {self.world_size}")
            print(f"rank: {self.rank}")
            print(f"local_rank: {self.local_rank}")
            print(f"device: {self.device}")
    
    def barrier(self):
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        if self.world_size > 1:
            dist.barrier()
    
    def print_main(self, message):
        """åªæœ‰ä¸»è¿›ç¨‹æ‰“å°æ¶ˆæ¯"""
        if self.is_main_process:
            print(message)

def safe_all_reduce(tensor, op=dist.ReduceOp.SUM, timeout=None):
    """
    å®‰å…¨çš„all_reduceæ“ä½œï¼Œç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨æ€§èƒ½
    
    Args:
        tensor: è¦èšåˆçš„tensor
        op: èšåˆæ“ä½œç±»å‹
        timeout: ä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨ï¼Œé¿å…æ€§èƒ½å¼€é”€
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ‰§è¡Œ
    """
    if not (dist.is_available() and dist.is_initialized()):
        return True  # éåˆ†å¸ƒå¼ç¯å¢ƒï¼Œç›´æ¥è¿”å›æˆåŠŸ
    
    try:
        # ç›´æ¥è°ƒç”¨all_reduceï¼Œè®©NCCLè‡ªèº«å¤„ç†è¶…æ—¶
        dist.all_reduce(tensor, op=op)
        return True
    except Exception as e:
        print(f"âŒ all_reduceæ“ä½œå¤±è´¥: {e}")
        return False

def safe_barrier(timeout=None):
    """
    å®‰å…¨çš„barrieræ“ä½œï¼Œç®€åŒ–ç‰ˆæœ¬
    
    Args:
        timeout: ä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ‰§è¡Œ
    """
    if not (dist.is_available() and dist.is_initialized()):
        return True  # éåˆ†å¸ƒå¼ç¯å¢ƒï¼Œç›´æ¥è¿”å›æˆåŠŸ
    
    try:
        dist.barrier()
        return True
    except Exception as e:
        print(f"âŒ barrieræ“ä½œå¤±è´¥: {e}")
        return False

def batch_all_reduce(tensors, op=dist.ReduceOp.SUM):
    """
    æ‰¹é‡all_reduceæ“ä½œï¼Œå‡å°‘é€šä¿¡æ¬¡æ•°
    
    Args:
        tensors: tensoråˆ—è¡¨
        op: èšåˆæ“ä½œç±»å‹
        
    Returns:
        bool: æ˜¯å¦å…¨éƒ¨æˆåŠŸæ‰§è¡Œ
    """
    if not (dist.is_available() and dist.is_initialized()):
        return True  # éåˆ†å¸ƒå¼ç¯å¢ƒï¼Œç›´æ¥è¿”å›æˆåŠŸ
    
    if not tensors:
        return True
    
    try:
        # æ‰¹é‡å¤„ç†æ‰€æœ‰tensor
        for tensor in tensors:
            dist.all_reduce(tensor, op=op)
        return True
    except Exception as e:
        print(f"âŒ æ‰¹é‡all_reduceæ“ä½œå¤±è´¥: {e}")
        return False

def get_nccl_debug_info():
    """è·å–NCCLè°ƒè¯•ä¿¡æ¯"""
    info = {
        'NCCL_DEBUG': os.environ.get('NCCL_DEBUG', 'Not set'),
        'NCCL_SOCKET_IFNAME': os.environ.get('NCCL_SOCKET_IFNAME', 'Not set'),
        'NCCL_IB_DISABLE': os.environ.get('NCCL_IB_DISABLE', 'Not set'),
        'NCCL_P2P_DISABLE': os.environ.get('NCCL_P2P_DISABLE', 'Not set'),
        'NCCL_TREE_THRESHOLD': os.environ.get('NCCL_TREE_THRESHOLD', 'Not set'),
    }
    
    print("ğŸ” NCCLç¯å¢ƒå˜é‡:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    return info

def setup_nccl_timeout_env():
    """è®¾ç½®è½»é‡çº§çš„NCCLè¶…æ—¶ç¯å¢ƒå˜é‡ï¼Œä¸“æ³¨æ€§èƒ½"""
    # åªè®¾ç½®å¿…è¦çš„è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…è¿‡åº¦é…ç½®å½±å“æ€§èƒ½
    if 'NCCL_TIMEOUT' not in os.environ:
        os.environ['NCCL_TIMEOUT'] = '600'  # 10åˆ†é’Ÿè¶…æ—¶ï¼Œæ¯”è¾ƒå®½æ¾
    
    # å¯ç”¨å¼‚æ­¥é”™è¯¯å¤„ç†ï¼Œä½†ä¸å¼ºåˆ¶ç¦ç”¨æ€§èƒ½ä¼˜åŒ–
    if 'NCCL_ASYNC_ERROR_HANDLING' not in os.environ:
        os.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1'
    
    print("âœ… å·²è®¾ç½®è½»é‡çº§NCCLè¶…æ—¶ä¿æŠ¤")
    
    # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if os.environ.get('NCCL_DEBUG', '').upper() in ['INFO', 'WARN']:
        get_nccl_debug_info() 