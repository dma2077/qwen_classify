"""
è®­ç»ƒé…ç½®å·¥å…·å‡½æ•°
"""

def get_effective_steps_per_epoch(config, train_loader):
    """è®¡ç®—æ­£ç¡®çš„æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch
    
    Args:
        config: è®­ç»ƒé…ç½®
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        
    Returns:
        int: æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch
    """
    import json
    
    # è·å–DeepSpeedé…ç½®
    deepspeed_config_path = config.get('deepspeed', '')
    if not deepspeed_config_path:
        raise ValueError("DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„æœªè®¾ç½®")
    
    try:
        with open(deepspeed_config_path, 'r') as f:
            deepspeed_config = json.load(f)
    except Exception as e:
        raise ValueError(f"DeepSpeedé…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
    
    # è·å–å…³é”®å‚æ•°
    train_batch_size = deepspeed_config.get('train_batch_size', 256)
    dataset_size = len(train_loader.dataset)
    
    # åŸºäºæ€»æ‰¹æ¬¡å¤§å°è®¡ç®—æœ‰æ•ˆæ­¥æ•°
    effective_steps_per_epoch = dataset_size // train_batch_size
    if dataset_size % train_batch_size != 0:
        effective_steps_per_epoch += 1  # å‘ä¸Šå–æ•´
    
    print(f"ğŸ“Š æ­¥æ•°è®¡ç®—: æ•°æ®é›†å¤§å°={dataset_size:,}, æ€»æ‰¹æ¬¡å¤§å°={train_batch_size}, æœ‰æ•ˆæ­¥æ•°={effective_steps_per_epoch}")
    
    return effective_steps_per_epoch

def get_total_effective_steps(config, train_loader):
    """è®¡ç®—æ€»çš„æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆæ‰€æœ‰epochsï¼‰
    
    Args:
        config: è®­ç»ƒé…ç½®
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        
    Returns:
        int: æ€»çš„æœ‰æ•ˆè®­ç»ƒæ­¥æ•°
    """
    import json
    
    # è·å–DeepSpeedé…ç½®
    deepspeed_config_path = config.get('deepspeed', '')
    if not deepspeed_config_path:
        raise ValueError("DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„æœªè®¾ç½®")
    
    try:
        with open(deepspeed_config_path, 'r') as f:
            deepspeed_config = json.load(f)
    except Exception as e:
        raise ValueError(f"DeepSpeedé…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
    
    # è·å–å…³é”®å‚æ•°
    train_batch_size = deepspeed_config.get('train_batch_size', 256)
    micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 8)
    gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 4)
    dataset_size = len(train_loader.dataset)
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰å…³é”®å‚æ•°
    print(f"ğŸ” DeepSpeedé…ç½®è°ƒè¯•:")
    print(f"  â€¢ train_batch_size: {train_batch_size}")
    print(f"  â€¢ micro_batch_size_per_gpu: {micro_batch_size_per_gpu}")
    print(f"  â€¢ gradient_accumulation_steps: {gradient_accumulation_steps}")
    print(f"  â€¢ dataset_size: {dataset_size}")
    print(f"  â€¢ len(train_loader): {len(train_loader)}")
    
    # è·å–epochsæ•°
    training_config = config['training']
    num_epochs = training_config.get('epochs') or training_config.get('num_epochs')
    if isinstance(num_epochs, str):
        num_epochs = int(num_epochs)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    # DeepSpeedçš„train_batch_sizeå·²ç»æ˜¯å…¨å±€æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    # ä¸åº”è¯¥å†ä¹˜ä»¥ä»»ä½•æ¢¯åº¦ç´¯ç§¯å› å­
    effective_steps_per_epoch = dataset_size // train_batch_size
    if dataset_size % train_batch_size != 0:
        effective_steps_per_epoch += 1  # å‘ä¸Šå–æ•´
    
    # è®¡ç®—æ€»çš„æœ‰æ•ˆæ­¥æ•°
    total_effective_steps = effective_steps_per_epoch * num_epochs
    
    print(f"ğŸ” æ­¥æ•°è®¡ç®—è°ƒè¯•:")
    print(f"  â€¢ è®¡ç®—å…¬å¼: {dataset_size} Ã· {train_batch_size} = {effective_steps_per_epoch}")
    print(f"  â€¢ æ¯epochæœ‰æ•ˆæ­¥æ•°: {effective_steps_per_epoch}")
    print(f"  â€¢ æ€»epochs: {num_epochs}")
    print(f"  â€¢ æ€»æœ‰æ•ˆæ­¥æ•°: {total_effective_steps}")
    
    # ğŸ” éªŒè¯è®¡ç®—ï¼šä¸DataLoaderæ­¥æ•°å¯¹æ¯”
    dataloader_steps = len(train_loader)
    expected_ratio = dataloader_steps // effective_steps_per_epoch
    print(f"ğŸ” éªŒè¯ä¿¡æ¯:")
    print(f"  â€¢ DataLoaderæ­¥æ•°: {dataloader_steps}")
    print(f"  â€¢ é¢„æœŸæ¯”ç‡(åº”è¯¥ç­‰äºgradient_accumulation_steps): {expected_ratio}")
    print(f"  â€¢ gradient_accumulation_steps: {gradient_accumulation_steps}")
    
    return total_effective_steps

def prepare_config(config):
    """å‡†å¤‡é…ç½®å‚æ•°"""
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    required_keys = ['model', 'training', 'data']
    for key in required_keys:
        if key not in config:
            raise ValueError(f"é…ç½®ä¸­ç¼ºå°‘å¿…è¦çš„é”®: {key}")
    
    # è®¾ç½®trainingèŠ‚ç‚¹ä¸‹çš„é»˜è®¤å€¼
    training_config = config['training']
    training_config.setdefault('logging_steps', 50)
    training_config.setdefault('save_steps', 1000) 
    training_config.setdefault('eval_steps', 1000)
    training_config.setdefault('save_hf_format', True)
    training_config.setdefault('save_deepspeed_format', True)
    
    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†å­—æ®µåæ˜ å°„
    if 'epochs' in training_config and 'num_epochs' not in training_config:
        training_config['num_epochs'] = training_config['epochs']
        print(f"ğŸ“‹ é…ç½®æ˜ å°„: epochs -> num_epochs = {training_config['num_epochs']}")
    
    if 'lr' in training_config and 'learning_rate' not in training_config:
        training_config['learning_rate'] = training_config['lr']
        print(f"ğŸ“‹ é…ç½®æ˜ å°„: lr -> learning_rate = {training_config['learning_rate']}")
    
    # å°†å¸¸ç”¨çš„é…ç½®é¡¹æå‡åˆ°æ ¹å±‚çº§ï¼Œæ–¹ä¾¿è®¿é—®
    config['logging_steps'] = training_config['logging_steps']
    config['save_steps'] = training_config['save_steps']
    config['eval_steps'] = training_config['eval_steps']
    config['save_hf_format'] = training_config['save_hf_format']
    config['save_deepspeed_format'] = training_config['save_deepspeed_format']
    
    # ç¡®ä¿output_diråœ¨æ ¹å±‚çº§
    if 'output_dir' not in config and 'output_dir' in training_config:
        config['output_dir'] = training_config['output_dir']
    
    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†DeepSpeedé…ç½®ç»“æ„
    deepspeed_config = config.get('deepspeed')
    if deepspeed_config is None:
        raise ValueError("DeepSpeedé…ç½®æœªæ‰¾åˆ°ï¼")
    
    # å¤„ç†ä¸åŒçš„DeepSpeedé…ç½®æ ¼å¼
    if isinstance(deepspeed_config, dict):
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–config_file
        if 'config_file' in deepspeed_config:
            config['deepspeed'] = deepspeed_config['config_file']
            print(f"ğŸ“‹ DeepSpeedé…ç½®: ä»å­—å…¸æ ¼å¼æå– -> {deepspeed_config['config_file']}")
        else:
            raise ValueError("DeepSpeedé…ç½®å­—å…¸ä¸­ç¼ºå°‘config_fileå­—æ®µ")
    elif isinstance(deepspeed_config, str):
        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¿æŒä¸å˜
        print(f"ğŸ“‹ DeepSpeedé…ç½®: ç›´æ¥ä½¿ç”¨è·¯å¾„ -> {deepspeed_config}")
    else:
        raise ValueError("DeepSpeedé…ç½®å¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²æˆ–åŒ…å«config_fileçš„å­—å…¸")
    
    # ğŸ”¥ æ–°å¢ï¼šéªŒè¯WandBé…ç½®
    wandb_config = config.get('wandb', {})
    if wandb_config.get('enabled', False):
        print(f"ğŸ“‹ WandBé…ç½®: å·²å¯ç”¨")
        print(f"   ğŸ“Š é¡¹ç›®: {wandb_config.get('project', 'qwen_classification')}")
        print(f"   ğŸƒ è¿è¡Œåç§°: {wandb_config.get('run_name', 'auto-generated')}")
    else:
        print(f"ğŸ“‹ WandBé…ç½®: ç¦ç”¨")
    
    import os
    # éªŒè¯DeepSpeedé…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    deepspeed_config_path = config['deepspeed']
    if not os.path.exists(deepspeed_config_path):
        raise FileNotFoundError(f"DeepSpeedé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {deepspeed_config_path}")
    
    return config 