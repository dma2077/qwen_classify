import math
from transformers import (
    get_cosine_schedule_with_warmup,
    get_linear_schedule_with_warmup,
    get_polynomial_decay_schedule_with_warmup,
    get_constant_schedule_with_warmup
)
from torch.optim.lr_scheduler import LambdaLR

def _calculate_warmup_steps(warmup_config, total_training_steps):
    """è®¡ç®—warmupæ­¥æ•°ï¼Œæ”¯æŒç»å¯¹å€¼å’Œæ¯”ä¾‹
    
    Args:
        warmup_config: warmupé…ç½®ï¼Œå¯ä»¥æ˜¯ï¼š
            - int/str: ç»å¯¹æ­¥æ•° (å¦‚ 200)
            - float: æ¯”ä¾‹ (å¦‚ 0.1 è¡¨ç¤ºæ€»æ­¥æ•°çš„10%)
        total_training_steps: æ€»è®­ç»ƒæ­¥æ•°
        
    Returns:
        int: å®é™…çš„warmupæ­¥æ•°
    """
    # ç±»å‹è½¬æ¢å’Œå¤„ç†
    if isinstance(warmup_config, str):
        try:
            # å°è¯•è½¬æ¢ä¸ºfloatä»¥æ”¯æŒå­—ç¬¦ä¸²å½¢å¼çš„æ¯”ä¾‹ (å¦‚ "0.1")
            warmup_value = float(warmup_config)
        except ValueError:
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå½“ä½œæ•´æ•°å¤„ç†
            warmup_value = int(warmup_config)
    else:
        warmup_value = warmup_config
    
    # åˆ¤æ–­æ˜¯æ¯”ä¾‹è¿˜æ˜¯ç»å¯¹å€¼
    if isinstance(warmup_value, float) and 0 < warmup_value < 1:
        # æ¯”ä¾‹å½¢å¼ï¼š0 < value < 1
        num_warmup_steps = int(warmup_value * total_training_steps)
        warmup_type = f"æ¯”ä¾‹ ({warmup_value:.1%})"
    elif isinstance(warmup_value, float) and warmup_value >= 1:
        # å¤§äºç­‰äº1çš„floatï¼Œå½“ä½œç»å¯¹å€¼å¤„ç†
        num_warmup_steps = int(warmup_value)
        warmup_type = "ç»å¯¹æ­¥æ•°"
    else:
        # æ•´æ•°å½¢å¼ï¼šç»å¯¹æ­¥æ•°
        num_warmup_steps = int(warmup_value)
        warmup_type = "ç»å¯¹æ­¥æ•°"
    
    # ç¡®ä¿warmupæ­¥æ•°ä¸è¶…è¿‡æ€»è®­ç»ƒæ­¥æ•°
    num_warmup_steps = min(num_warmup_steps, total_training_steps)
    
    return num_warmup_steps, warmup_type

def create_lr_scheduler(optimizer, config, steps_per_epoch):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹ï¼š
    - cosine: ä½™å¼¦è¡°å‡è°ƒåº¦å™¨
    - cosine_with_hold: ä½™å¼¦+å¹³ç¨³æœŸè°ƒåº¦å™¨
    - linear: çº¿æ€§è¡°å‡è°ƒåº¦å™¨  
    - polynomial: å¤šé¡¹å¼è¡°å‡è°ƒåº¦å™¨
    - exponential: æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨
    - constant: å¸¸æ•°è°ƒåº¦å™¨ï¼ˆwarmupåä¿æŒä¸å˜ï¼‰
    - cosine_restarts: å¸¦é‡å¯çš„ä½™å¼¦è°ƒåº¦å™¨
    """
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    lr_config = config['training'].get('lr_scheduler', {})
    scheduler_type = lr_config.get('type', 'cosine')
    warmup_steps_config = config['training']['warmup_steps']
    num_epochs = config['training']['num_epochs']
    
    # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    if isinstance(num_epochs, str):
        num_epochs = int(num_epochs)
    
    # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆè€ƒè™‘DeepSpeedçš„åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
    # è·å–DeepSpeedé…ç½®
    deepspeed_config = config.get('deepspeed', {})
    if isinstance(deepspeed_config, str):
        import json
        with open(deepspeed_config, 'r') as f:
            deepspeed_config = json.load(f)
    
    # è·å–DeepSpeedå‚æ•°
    micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
    gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
    train_batch_size = deepspeed_config.get('train_batch_size', 32)
    
    # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆåŸºäºDataLoaderæ­¥æ•°å’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
    effective_steps_per_epoch = steps_per_epoch // gradient_accumulation_steps
    num_training_steps = effective_steps_per_epoch * num_epochs
    
    # å¤„ç†warmup_stepsï¼šæ”¯æŒç»å¯¹å€¼å’Œæ¯”ä¾‹
    num_warmup_steps, warmup_type = _calculate_warmup_steps(warmup_steps_config, num_training_steps)
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯
    try:
        import torch.distributed as dist
        is_main_process = not (dist.is_available() and dist.is_initialized()) or dist.get_rank() == 0
    except ImportError:
        is_main_process = True
    
    if is_main_process:
        print(f"\nğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®:")
        print(f"  â€¢ è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
        print(f"  â€¢ Warmupé…ç½®: {warmup_steps_config} ({warmup_type})")
        print(f"  â€¢ å®é™…Warmupæ­¥æ•°: {num_warmup_steps:,}")
        print(f"  â€¢ æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps:,}")
        print(f"  â€¢ Warmupæ¯”ä¾‹: {num_warmup_steps/num_training_steps:.1%}")
        print(f"  â€¢ æ¯GPUå¾®æ‰¹æ¬¡å¤§å°: {micro_batch_size_per_gpu}")
        print(f"  â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        print(f"  â€¢ æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_batch_size}")
        print(f"  â€¢ æ¯GPU DataLoaderæ­¥æ•°: {steps_per_epoch:,}")
        print(f"  â€¢ æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch: {effective_steps_per_epoch:,}")
    
    # æ ¹æ®è°ƒåº¦å™¨ç±»å‹åˆ›å»ºç›¸åº”çš„è°ƒåº¦å™¨
    if scheduler_type == 'cosine':
        return create_cosine_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'cosine_with_hold':
        return create_cosine_with_hold_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'linear':
        return create_linear_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'polynomial':
        return create_polynomial_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'exponential':
        return create_exponential_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'constant':
        return create_constant_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process)
    elif scheduler_type == 'cosine_restarts':
        return create_cosine_restarts_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, effective_steps_per_epoch, is_main_process)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")


def create_cosine_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºä½™å¼¦è¡°å‡è°ƒåº¦å™¨"""
    num_cycles = lr_config.get('num_cycles', 0.5)
    final_lr_ratio = lr_config.get('final_lr_ratio', 0.0)  # æœ€ç»ˆå­¦ä¹ ç‡ç›¸å¯¹äºåˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹
    
    if is_main_process:
        print(f"  â€¢ ä½™å¼¦å‘¨æœŸæ•°: {num_cycles}")
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_lr_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_lr_ratio if final_lr_ratio > 0 else 'âˆ'}x")
    
    # å¦‚æœéœ€è¦è‡ªå®šä¹‰æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ï¼Œä½¿ç”¨è‡ªå®šä¹‰å®ç°
    if final_lr_ratio != 0.0:
        return create_custom_cosine_scheduler(optimizer, num_warmup_steps, num_training_steps, num_cycles, final_lr_ratio)
    else:
        return get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
            num_cycles=num_cycles,
        )


def create_cosine_with_hold_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºä½™å¼¦+å¹³ç¨³æœŸè°ƒåº¦å™¨"""
    hold_steps = lr_config.get('hold_steps', None)
    hold_ratio = lr_config.get('hold_ratio', 0.3)  # å¹³ç¨³æœŸå æ€»æ­¥æ•°çš„æ¯”ä¾‹
    final_lr_ratio = lr_config.get('final_lr_ratio', 0.0)
    num_cycles = lr_config.get('num_cycles', 0.5)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šhold_stepsï¼Œåˆ™æ ¹æ®hold_ratioè®¡ç®—
    if hold_steps is None:
        total_non_warmup_steps = num_training_steps - num_warmup_steps
        hold_steps = int(total_non_warmup_steps * hold_ratio)
    
    # è®¡ç®—å„é˜¶æ®µæ­¥æ•°
    decay_steps = num_training_steps - num_warmup_steps - hold_steps
    
    if is_main_process:
        print(f"  â€¢ Warmupæ­¥æ•°: {num_warmup_steps:,}")
        print(f"  â€¢ Holdå¹³ç¨³æœŸæ­¥æ•°: {hold_steps:,}")
        print(f"  â€¢ Cosineè¡°å‡æ­¥æ•°: {decay_steps:,}")
        print(f"  â€¢ Holdæ¯”ä¾‹: {hold_steps/(num_training_steps-num_warmup_steps):.1%}")
        print(f"  â€¢ ä½™å¼¦å‘¨æœŸæ•°: {num_cycles}")
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_lr_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_lr_ratio if final_lr_ratio > 0 else 'âˆ'}x")
    
    return create_custom_cosine_with_hold_scheduler(
        optimizer, num_warmup_steps, hold_steps, num_training_steps, num_cycles, final_lr_ratio
    )


def create_linear_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºçº¿æ€§è¡°å‡è°ƒåº¦å™¨"""
    final_lr_ratio = lr_config.get('final_lr_ratio', 0.0)
    
    if is_main_process:
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_lr_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_lr_ratio if final_lr_ratio > 0 else 'âˆ'}x")
    
    if final_lr_ratio != 0.0:
        return create_custom_linear_scheduler(optimizer, num_warmup_steps, num_training_steps, final_lr_ratio)
    else:
        return get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )


def create_polynomial_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºå¤šé¡¹å¼è¡°å‡è°ƒåº¦å™¨"""
    power = lr_config.get('power', 1.0)
    final_lr_ratio = lr_config.get('final_lr_ratio', 0.0)
    
    if is_main_process:
        print(f"  â€¢ å¤šé¡¹å¼å¹‚æ¬¡: {power}")
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_lr_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_lr_ratio if final_lr_ratio > 0 else 'âˆ'}x")
    
    if final_lr_ratio != 0.0:
        return create_custom_polynomial_scheduler(optimizer, num_warmup_steps, num_training_steps, power, final_lr_ratio)
    else:
        return get_polynomial_decay_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
            power=power,
        )


def create_exponential_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºæŒ‡æ•°è¡°å‡è°ƒåº¦å™¨"""
    decay_rate = lr_config.get('decay_rate', 0.95)
    final_lr_ratio = lr_config.get('final_lr_ratio', None)
    
    # å¦‚æœæŒ‡å®šäº†æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ï¼Œè®¡ç®—å¯¹åº”çš„è¡°å‡ç‡
    if final_lr_ratio is not None:
        # final_lr_ratio = decay_rate^(num_training_steps - num_warmup_steps)
        decay_steps = num_training_steps - num_warmup_steps
        if decay_steps > 0:
            decay_rate = final_lr_ratio ** (1.0 / decay_steps)
    
    if is_main_process:
        print(f"  â€¢ æŒ‡æ•°è¡°å‡ç‡: {decay_rate:.6f}")
        final_ratio = decay_rate ** (num_training_steps - num_warmup_steps)
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_ratio:.1f}x")
    
    return create_custom_exponential_scheduler(optimizer, num_warmup_steps, num_training_steps, decay_rate)


def create_constant_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, is_main_process):
    """åˆ›å»ºå¸¸æ•°è°ƒåº¦å™¨ï¼ˆwarmupåä¿æŒå­¦ä¹ ç‡ä¸å˜ï¼‰"""
    if is_main_process:
        print(f"  â€¢ Warmupåå­¦ä¹ ç‡ä¿æŒä¸å˜")
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: 100.0%")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: 1.0x")
    
    return get_constant_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
    )


def create_cosine_restarts_scheduler(optimizer, lr_config, num_warmup_steps, num_training_steps, steps_per_epoch, is_main_process):
    """åˆ›å»ºå¸¦é‡å¯çš„ä½™å¼¦è°ƒåº¦å™¨"""
    restart_period_epochs = lr_config.get('restart_period_epochs', 2)
    restart_period_steps = restart_period_epochs * steps_per_epoch
    final_lr_ratio = lr_config.get('final_lr_ratio', 0.1)
    
    if is_main_process:
        print(f"  â€¢ é‡å¯å‘¨æœŸ: {restart_period_epochs} epochs ({restart_period_steps} steps)")
        print(f"  â€¢ æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹: {final_lr_ratio:.1%}")
        print(f"  â€¢ å­¦ä¹ ç‡è¡°å‡å€æ•°: {1/final_lr_ratio:.1f}x")
    
    return create_custom_cosine_restarts_scheduler(optimizer, num_warmup_steps, num_training_steps, restart_period_steps, final_lr_ratio)


# è‡ªå®šä¹‰è°ƒåº¦å™¨å®ç°

def create_custom_cosine_scheduler(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, final_lr_ratio=0.0):
    """è‡ªå®šä¹‰ä½™å¼¦è°ƒåº¦å™¨ï¼Œæ”¯æŒè®¾ç½®æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))
        
        # åœ¨final_lr_ratioå’Œ1.0ä¹‹é—´è¿›è¡Œä½™å¼¦è¡°å‡
        return final_lr_ratio + (1.0 - final_lr_ratio) * cosine_decay
    
    return LambdaLR(optimizer, lr_lambda)


def create_custom_cosine_with_hold_scheduler(optimizer, num_warmup_steps, hold_steps, num_training_steps, num_cycles=0.5, final_lr_ratio=0.0):
    """è‡ªå®šä¹‰ä½™å¼¦+å¹³ç¨³æœŸè°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        # é˜¶æ®µ1: Warmup - çº¿æ€§å¢é•¿åˆ°ç›®æ ‡å­¦ä¹ ç‡
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        # é˜¶æ®µ2: Hold - ä¿æŒåœ¨ç›®æ ‡å­¦ä¹ ç‡
        if current_step < num_warmup_steps + hold_steps:
            return 1.0
        
        # é˜¶æ®µ3: Cosine Decay - ä½™å¼¦è¡°å‡
        decay_start_step = num_warmup_steps + hold_steps
        decay_steps = num_training_steps - decay_start_step
        
        if decay_steps <= 0:
            return final_lr_ratio
        
        progress = float(current_step - decay_start_step) / float(decay_steps)
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))
        
        # åœ¨final_lr_ratioå’Œ1.0ä¹‹é—´è¿›è¡Œä½™å¼¦è¡°å‡
        return final_lr_ratio + (1.0 - final_lr_ratio) * cosine_decay
    
    return LambdaLR(optimizer, lr_lambda)


def create_custom_linear_scheduler(optimizer, num_warmup_steps, num_training_steps, final_lr_ratio=0.0):
    """è‡ªå®šä¹‰çº¿æ€§è°ƒåº¦å™¨ï¼Œæ”¯æŒè®¾ç½®æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return final_lr_ratio + (1.0 - final_lr_ratio) * (1.0 - progress)
    
    return LambdaLR(optimizer, lr_lambda)


def create_custom_polynomial_scheduler(optimizer, num_warmup_steps, num_training_steps, power=1.0, final_lr_ratio=0.0):
    """è‡ªå®šä¹‰å¤šé¡¹å¼è°ƒåº¦å™¨ï¼Œæ”¯æŒè®¾ç½®æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        decay = (1.0 - progress) ** power
        return final_lr_ratio + (1.0 - final_lr_ratio) * decay
    
    return LambdaLR(optimizer, lr_lambda)


def create_custom_exponential_scheduler(optimizer, num_warmup_steps, num_training_steps, decay_rate=0.95):
    """è‡ªå®šä¹‰æŒ‡æ•°è°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        decay_steps = current_step - num_warmup_steps
        return decay_rate ** decay_steps
    
    return LambdaLR(optimizer, lr_lambda)


def create_custom_cosine_restarts_scheduler(optimizer, num_warmup_steps, num_training_steps, restart_period, final_lr_ratio=0.1):
    """è‡ªå®šä¹‰å¸¦é‡å¯çš„ä½™å¼¦è°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        # é‡å¯é€»è¾‘
        effective_step = current_step - num_warmup_steps
        cycle_step = effective_step % restart_period
        progress = float(cycle_step) / float(max(1, restart_period))
        
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
        return final_lr_ratio + (1.0 - final_lr_ratio) * cosine_decay
    
    return LambdaLR(optimizer, lr_lambda)
