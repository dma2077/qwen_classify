import os
import time
import json
import torch
import deepspeed
from tqdm import tqdm
from transformers import AutoProcessor
from .utils.model_utils import save_hf_model
from .utils.distributed import DistributedContext
from .utils.monitor import TrainingMonitor, make_json_serializable
from .utils.evaluation import evaluate_model

class DeepSpeedTrainer:
    def __init__(self, config):
        # å‡è®¾é…ç½®å·²ç»é€šè¿‡prepare_configå¤„ç†è¿‡
        self.config = config
        self.dist_ctx = DistributedContext()
        self.monitor = TrainingMonitor(self.config['output_dir'])
        self.model = None
        self.train_loader = None
        self.val_loader = None
        self.optimizer = None
        self.lr_scheduler = None
        self.current_step = 0
        self.current_epoch = 0
        
    def setup_model(self, model, train_loader, val_loader, optimizer, lr_scheduler):
        """è®¾ç½®æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        
        # åˆå§‹åŒ–DeepSpeed
        self.model, self.optimizer, _, self.lr_scheduler = deepspeed.initialize(
            model=model,
            optimizer=optimizer,
            lr_scheduler=lr_scheduler,
            config=self.config['deepspeed']
        )
        
        self.dist_ctx.print_info()
        self.dist_ctx.print_main(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.dist_ctx.device}")
        
    def save_checkpoint(self, step):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.config['output_dir'], f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        training_info = {
            'step': step,
            'epoch': self.current_epoch,
            'config': self.config,
            'timestamp': time.time()
        }
        
        with open(os.path.join(checkpoint_dir, 'training_info.json'), 'w') as f:
            # ä½¿ç”¨make_json_serializableç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥åºåˆ—åŒ–
            json.dump(make_json_serializable(training_info), f, indent=2)
        
        # ä¿å­˜DeepSpeedæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        if self.config.get('save_deepspeed_format', True):
            deepspeed_dir = os.path.join(checkpoint_dir, 'deepspeed')
            self.model.save_checkpoint(deepspeed_dir)
            self.dist_ctx.print_main(f"DeepSpeedæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {deepspeed_dir}")
        
        # ä¿å­˜HuggingFaceæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        if self.config.get('save_hf_format', True):
            if self.dist_ctx.is_main_process:
                hf_dir = save_hf_model(self.model, self.config, checkpoint_dir)
                if hf_dir:
                    self.dist_ctx.print_main(f"HuggingFaceæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {hf_dir}")
        
        self.dist_ctx.barrier()
        
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.dist_ctx.print_main("å¼€å§‹è¯„ä¼°...")
        eval_loss, eval_accuracy = evaluate_model(self.model, self.val_loader, self.dist_ctx.device)
        self.dist_ctx.print_main(f"éªŒè¯æŸå¤±: {eval_loss:.4f}, å‡†ç¡®ç‡: {eval_accuracy:.4f}")
        return eval_loss, eval_accuracy
        
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        self.dist_ctx.print_main("å¼€å§‹è®­ç»ƒ...")
        self.monitor.start_training()
        
        num_epochs = self.config['training']['num_epochs']
        logging_steps = self.config['logging_steps']
        save_steps = self.config['save_steps']
        eval_steps = self.config['eval_steps']
        
        # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆè€ƒè™‘DeepSpeedçš„åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
        deepspeed_config = self.config.get('deepspeed', {})
        if isinstance(deepspeed_config, str):
            import json
            with open(deepspeed_config, 'r') as f:
                deepspeed_config = json.load(f)
        
        # è·å–DeepSpeedå‚æ•°
        micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
        gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
        train_batch_size = deepspeed_config.get('train_batch_size', 32)
        
        # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆåŸºäºå®é™…çš„DataLoaderé•¿åº¦ï¼‰
        dataloader_steps_per_epoch = len(self.train_loader)
        effective_steps_per_epoch = dataloader_steps_per_epoch // gradient_accumulation_steps
        total_effective_steps = effective_steps_per_epoch * num_epochs
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåŸºäºæœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼‰
        pbar = tqdm(total=total_effective_steps, desc="Training Steps", disable=not self.dist_ctx.is_main_process)
        
        # è®¡ç®—éªŒè¯ä¿¡æ¯
        dataset_size = len(self.train_loader.dataset)
        samples_per_gpu = dataloader_steps_per_epoch * micro_batch_size_per_gpu
        
        # ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼è¾“å‡ºè®­ç»ƒé…ç½®ä¿¡æ¯
        if self.dist_ctx.is_main_process:
            print("="*80)
            print("ğŸš€ è®­ç»ƒé…ç½®ä¿¡æ¯")
            print("="*80)
            print(f"ğŸ“Š æ•°æ®é›†é…ç½®:")
            print(f"  â€¢ æ€»æ•°æ®é›†å¤§å°: {dataset_size:,}")
            print(f"  â€¢ æ¯GPUå¤„ç†æ ·æœ¬æ•°: {samples_per_gpu:,}")
            print(f"ğŸ“¦ æ‰¹æ¬¡é…ç½®:")
            print(f"  â€¢ æ¯GPUå¾®æ‰¹æ¬¡å¤§å°: {micro_batch_size_per_gpu}")
            print(f"  â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
            print(f"  â€¢ æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_batch_size}")
            print(f"ğŸ“ˆ æ­¥æ•°ç»Ÿè®¡:")
            print(f"  â€¢ æ¯GPU DataLoaderæ­¥æ•°: {dataloader_steps_per_epoch:,}")
            print(f"  â€¢ æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch: {effective_steps_per_epoch:,}")
            print(f"  â€¢ æ€»æœ‰æ•ˆè®­ç»ƒæ­¥æ•°: {total_effective_steps:,}")
            print("="*80)
        
        effective_step = 0  # ç”¨äºè·Ÿè¸ªæœ‰æ•ˆæ­¥æ•°
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            self.model.train()
            
            # ä¸ºåˆ†å¸ƒå¼é‡‡æ ·å™¨è®¾ç½®epochï¼ˆç¡®ä¿æ¯ä¸ªepochçš„shuffleæ­£ç¡®ï¼‰
            if hasattr(self.train_loader.sampler, 'set_epoch'):
                self.train_loader.sampler.set_epoch(epoch)
            
            epoch_loss = 0
            epoch_start_time = time.time()
            
            for batch_idx, batch in enumerate(self.train_loader):
                self.current_step += 1
                
                # å‰å‘ä¼ æ’­
                inputs = batch["input_ids"].to(self.dist_ctx.device)
                attention_mask = batch["attention_mask"].to(self.dist_ctx.device)
                pixel_values = batch["pixel_values"].to(self.dist_ctx.device)
                labels = batch["labels"].to(self.dist_ctx.device)
                
                # æ·»åŠ image_grid_thwå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                forward_kwargs = {
                    "input_ids": inputs,
                    "attention_mask": attention_mask,
                    "pixel_values": pixel_values,
                    "labels": labels
                }
                
                # æ£€æŸ¥å¹¶æ·»åŠ image_grid_thwå‚æ•°
                if "image_grid_thw" in batch:
                    forward_kwargs["image_grid_thw"] = batch["image_grid_thw"].to(self.dist_ctx.device)
                
                outputs = self.model(**forward_kwargs)
                
                loss = outputs.loss
                epoch_loss += loss.item()
                
                # åå‘ä¼ æ’­
                self.model.backward(loss)
                
                # è·å–æ¢¯åº¦èŒƒæ•°
                grad_norm = self.model.get_global_grad_norm()
                
                # æ›´æ–°å‚æ•°
                self.model.step()
                
                # æ›´æ–°è¿›åº¦æ¡ï¼ˆåªåœ¨æœ‰æ•ˆæ­¥æ•°æ—¶æ›´æ–°ï¼‰
                if self.current_step % gradient_accumulation_steps == 0:
                    effective_step += 1
                    pbar.update(1)
                    pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                        'epoch': f'{epoch + batch_idx/len(self.train_loader):.2f}'
                    })
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                current_lr = self.optimizer.param_groups[0]['lr']
                # ç¡®ä¿grad_normæ˜¯floatç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
                # å¤„ç†grad_normå¯èƒ½ä¸ºNoneçš„æƒ…å†µ
                if grad_norm is None:
                    grad_norm_value = 0.0
                elif hasattr(grad_norm, 'item'):
                    grad_norm_value = float(grad_norm.item())
                else:
                    grad_norm_value = float(grad_norm)
                self.monitor.log_step(self.current_step, epoch, loss.item(), grad_norm_value, current_lr)
                
                # è¯¦ç»†æ—¥å¿—è®°å½•
                if self.current_step % logging_steps == 0:
                    # ä½¿ç”¨tqdm.write()æ¥é¿å…ä¸è¿›åº¦æ¡å†²çª
                    log_message = (
                        f"Step {self.current_step:,} | "
                        f"Loss: {loss.item():.4f} | "
                        f"Grad Norm: {grad_norm_value:.4f} | "
                        f"LR: {current_lr:.2e} | "
                        f"Epoch: {epoch + batch_idx/len(self.train_loader):.2f}"
                    )
                    if self.dist_ctx.is_main_process:
                        pbar.write(log_message)
                
                # å®šæœŸè¯„ä¼°ï¼ˆåŸºäºæœ‰æ•ˆæ­¥æ•°ï¼‰
                if effective_step > 0 and effective_step % eval_steps == 0:
                    # æš‚æ—¶åˆ·æ–°è¿›åº¦æ¡ä»¥é¿å…è¾“å‡ºå†²çª
                    pbar.clear()
                    eval_loss, eval_accuracy = self.evaluate()
                    self.model.train()
                    # é‡æ–°æ˜¾ç¤ºè¿›åº¦æ¡
                    pbar.refresh()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŸºäºæœ‰æ•ˆæ­¥æ•°ï¼‰
                if effective_step > 0 and effective_step % save_steps == 0:
                    pbar.clear()
                    self.save_checkpoint(effective_step)
                    pbar.refresh()
            
            # Epochç»“æŸç»Ÿè®¡
            epoch_time = time.time() - epoch_start_time
            avg_loss = epoch_loss / len(self.train_loader)
            self.monitor.log_epoch(epoch, avg_loss, epoch_time)
            
            # ä½¿ç”¨tqdm.write()è¾“å‡ºepochç»Ÿè®¡ä¿¡æ¯
            epoch_message = (
                f"ğŸ“Š Epoch {epoch+1}/{num_epochs} å®Œæˆ | "
                f"å¹³å‡æŸå¤±: {avg_loss:.4f} | "
                f"è€—æ—¶: {epoch_time:.2f}ç§’ | "
                f"æœ‰æ•ˆæ­¥æ•°: {effective_step:,}"
            )
            if self.dist_ctx.is_main_process:
                pbar.write(epoch_message)
        
        pbar.close()
        
        # è®­ç»ƒç»“æŸå‰è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        if self.dist_ctx.is_main_process:
            print("\nğŸ¯ è®­ç»ƒå³å°†å®Œæˆï¼Œè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        eval_loss, eval_accuracy = self.evaluate()
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        if self.dist_ctx.is_main_process:
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹...")
        self.save_checkpoint(effective_step)
        
        if self.dist_ctx.is_main_process:
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ - æŸå¤±: {eval_loss:.4f}, å‡†ç¡®ç‡: {eval_accuracy:.4f}")
        self.monitor.save_logs()
        
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            self.model.load_checkpoint(checkpoint_path)
            self.dist_ctx.print_main(f"æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
        else:
            self.dist_ctx.print_main(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            
    def get_training_stats(self):
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return self.monitor.get_avg_metrics() 