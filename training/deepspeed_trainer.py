import os
import time
import json
import torch
import deepspeed
from tqdm import tqdm
from transformers import AutoProcessor
from collections import defaultdict
from .utils.model_utils import save_hf_model
from .utils.distributed import DistributedContext
from .utils.evaluation import evaluate_multi_dataset
from .utils.monitor import TrainingMonitor, make_json_serializable, calculate_mfu
from data.dataloader import create_full_eval_dataloader

class DeepSpeedTrainer:
    def __init__(self, config):
        # å‡è®¾é…ç½®å·²ç»é€šè¿‡prepare_configå¤„ç†è¿‡
        self.config = config
        self.dist_ctx = DistributedContext()
        
        # è®¾ç½®NCCLè¶…æ—¶ä¿æŠ¤ï¼ˆåœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼‰
        if self.dist_ctx.world_size > 1:
            from .utils.distributed import setup_nccl_timeout_env
            setup_nccl_timeout_env()
        
        # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºå®Œæ•´çš„TrainingMonitorï¼Œéä¸»è¿›ç¨‹ä½¿ç”¨DummyMonitor
        if self.dist_ctx.is_main_process:
            from training.utils.monitor import TrainingMonitor
            self.monitor = TrainingMonitor(self.config['output_dir'], config)
            print("âœ… ä¸»è¿›ç¨‹ï¼šåˆ›å»ºå®Œæ•´TrainingMonitorï¼ˆåŒ…å«wandbï¼‰")
        else:
            from training.utils.monitor import DummyMonitor  
            self.monitor = DummyMonitor(self.config['output_dir'], config)
            print(f"â„¹ï¸  è¿›ç¨‹ rank {self.dist_ctx.rank}ï¼šä½¿ç”¨DummyMonitorï¼ˆæ— wandbï¼‰")
        
        self.model = None
        self.train_loader = None
        self.val_loader = None
        self.optimizer = None
        self.lr_scheduler = None
        self.current_step = 0
        self.current_epoch = 0
        
        # å¤šæ•°æ®é›†æ”¯æŒ
        self.dataset_configs = self.config.get('datasets', {}).get('dataset_configs', {})
        self.enable_dataset_metrics = self.config.get('wandb', {}).get('log_dataset_metrics', True)
        
        # ç”¨äºè·Ÿè¸ªå„æ•°æ®é›†çš„æŒ‡æ ‡
        self.dataset_metrics = defaultdict(lambda: {
            'total_loss': 0.0,
            'total_samples': 0,
            'correct_samples': 0,
            'step_count': 0
        })
        
        # æœ€ä½³æ¨¡å‹è¿½è¸ª
        self.best_model_config = self.config.get('training', {}).get('best_model_tracking', {})
        self.best_model_enabled = self.best_model_config.get('enabled', True)
        self.best_metric_name = self.best_model_config.get('metric', 'overall_accuracy')
        self.best_metric_mode = self.best_model_config.get('mode', 'max')  # 'max' or 'min'
        self.save_best_only = self.best_model_config.get('save_best_only', True)
        
        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡
        if self.best_metric_mode == 'max':
            self.best_metric_value = float('-inf')
        else:
            self.best_metric_value = float('inf')
        
        self.best_model_step = 0
        self.best_model_path = None
        
        # è¯„ä¼°é…ç½®
        self.eval_config = self.config.get('training', {}).get('evaluation', {})
        self.partial_eval_during_training = self.eval_config.get('partial_eval_during_training', True)
        self.full_eval_at_end = self.eval_config.get('full_eval_at_end', True)
        self.eval_best_model_only = self.eval_config.get('eval_best_model_only', True)
        
    def setup_model(self, model, train_loader, val_loader, optimizer, lr_scheduler):
        """è®¾ç½®æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        
        # åˆå§‹åŒ–DeepSpeed
        self.model, self.optimizer, _, self.lr_scheduler = deepspeed.initialize(
            model=model,
            optimizer=optimizer,
            lr_scheduler=lr_scheduler,
            config=self.config['deepspeed']
        )
        
        self.dist_ctx.print_info()
        self.dist_ctx.print_main(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.dist_ctx.device}")
        
        # è®¾ç½®monitorçš„modelå¼•ç”¨ç”¨äºMFUè®¡ç®—
        self.monitor.set_model_ref(self.model)
        
    def save_checkpoint(self, step, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if is_best:
            checkpoint_dir = os.path.join(self.config['output_dir'], f"best-model-step-{step}")
        else:
            checkpoint_dir = os.path.join(self.config['output_dir'], f"checkpoint-{step}")
        
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        training_info = {
            'step': step,
            'epoch': self.current_epoch,
            'config': self.config,
            'dataset_metrics': dict(self.dataset_metrics),  # ä¿å­˜æ•°æ®é›†æŒ‡æ ‡
            'is_best_model': is_best,
            'best_metric_value': self.best_metric_value if is_best else None,
            'timestamp': time.time()
        }
        
        with open(os.path.join(checkpoint_dir, 'training_info.json'), 'w') as f:
            # ä½¿ç”¨make_json_serializableç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥åºåˆ—åŒ–
            json.dump(make_json_serializable(training_info), f, indent=2)
        
        # ä¿å­˜DeepSpeedæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        if self.config.get('save_deepspeed_format', True):
            deepspeed_dir = os.path.join(checkpoint_dir, 'deepspeed')
            self.model.save_checkpoint(deepspeed_dir)
            if is_best:
                self.dist_ctx.print_main(f"ğŸ† æœ€ä½³æ¨¡å‹DeepSpeedæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {deepspeed_dir}")
            else:
                self.dist_ctx.print_main(f"DeepSpeedæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {deepspeed_dir}")
        
        # ä¿å­˜HuggingFaceæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        if self.config.get('save_hf_format', True):
            if self.dist_ctx.is_main_process:
                hf_dir = save_hf_model(self.model, self.config, checkpoint_dir)
                if hf_dir:
                    if is_best:
                        self.dist_ctx.print_main(f"ğŸ† æœ€ä½³æ¨¡å‹HuggingFaceæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {hf_dir}")
                    else:
                        self.dist_ctx.print_main(f"HuggingFaceæ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {hf_dir}")
        
        if is_best:
            self.best_model_path = checkpoint_dir
        
        self.dist_ctx.barrier()
        return checkpoint_dir
    
    def _is_better_metric(self, current_value, best_value):
        """åˆ¤æ–­å½“å‰æŒ‡æ ‡æ˜¯å¦æ›´å¥½"""
        if self.best_metric_mode == 'max':
            return current_value > best_value
        else:
            return current_value < best_value
    
    def _cleanup_old_best_models(self):
        """æ¸…ç†æ‰€æœ‰æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€æ–°çš„æœ€ä½³æ¨¡å‹"""
        if not self.save_best_only:
            return
            
        try:
            import glob
            import shutil
            
            # æŸ¥æ‰¾æ‰€æœ‰æ£€æŸ¥ç‚¹ç›®å½•
            best_model_pattern = os.path.join(self.config['output_dir'], "best-model-step-*")
            checkpoint_pattern = os.path.join(self.config['output_dir'], "checkpoint-*")
            
            best_model_dirs = glob.glob(best_model_pattern)
            checkpoint_dirs = glob.glob(checkpoint_pattern)
            
            dirs_to_remove = []
            
            # 1. åˆ é™¤æ‰€æœ‰å¸¸è§„æ£€æŸ¥ç‚¹ï¼ˆcheckpoint-*ï¼‰
            dirs_to_remove.extend(checkpoint_dirs)
            
            # 2. åˆ é™¤é™¤æœ€æ–°ä¹‹å¤–çš„æ‰€æœ‰æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
            if len(best_model_dirs) > 1:
                def extract_step(path):
                    try:
                        return int(os.path.basename(path).split('-')[-1])
                    except:
                        return 0
                
                best_model_dirs.sort(key=extract_step)
                dirs_to_remove.extend(best_model_dirs[:-1])  # ä¿ç•™æœ€åä¸€ä¸ªï¼ˆæœ€æ–°çš„ï¼‰
            
            # æ‰§è¡Œæ¸…ç†
            total_removed = 0
            for dir_path in dirs_to_remove:
                if os.path.exists(dir_path):
                    dir_name = os.path.basename(dir_path)
                    self.dist_ctx.print_main(f"ğŸ—‘ï¸  åˆ é™¤æ£€æŸ¥ç‚¹: {dir_name}")
                    shutil.rmtree(dir_path)
                    total_removed += 1
            
            # æ˜¾ç¤ºæ¸…ç†ç»“æœ
            if total_removed > 0:
                self.dist_ctx.print_main(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {total_removed} ä¸ªæ£€æŸ¥ç‚¹")
                
                # æ˜¾ç¤ºä¿ç•™çš„æœ€ä½³æ¨¡å‹
                remaining_best = glob.glob(best_model_pattern)
                if remaining_best:
                    remaining_best.sort(key=lambda x: int(os.path.basename(x).split('-')[-1]))
                    self.dist_ctx.print_main(f"ğŸ† ä¿ç•™æœ€ä½³æ¨¡å‹: {os.path.basename(remaining_best[-1])}")
            else:
                self.dist_ctx.print_main("âœ… æ— éœ€æ¸…ç†ï¼Œç›®å½•å·²ç»å¾ˆå¹²å‡€")
                
        except Exception as e:
            self.dist_ctx.print_main(f"âš ï¸  æ¸…ç†æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")

    def _update_best_model(self, eval_results, step):
        """æ›´æ–°æœ€ä½³æ¨¡å‹"""
        if not self.best_model_enabled:
            return False
        
        # è·å–å½“å‰æŒ‡æ ‡å€¼
        if self.best_metric_name == 'overall_accuracy':
            current_value = eval_results.get('overall_accuracy', 0.0)
        elif self.best_metric_name == 'overall_loss':
            current_value = eval_results.get('overall_loss', float('inf'))
        else:
            # æ”¯æŒæ•°æ®é›†ç‰¹å®šæŒ‡æ ‡ï¼Œå¦‚ 'food101_accuracy'
            if 'dataset_metrics' in eval_results:
                for dataset_name, metrics in eval_results['dataset_metrics'].items():
                    metric_key = self.best_metric_name.replace(f'{dataset_name}_', '')
                    if self.best_metric_name.startswith(dataset_name) and metric_key in metrics:
                        current_value = metrics[metric_key]
                        break
                else:
                    current_value = eval_results.get('overall_accuracy', 0.0)  # é»˜è®¤ä½¿ç”¨overall_accuracy
            else:
                current_value = eval_results.get('overall_accuracy', 0.0)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        if self._is_better_metric(current_value, self.best_metric_value):
            self.best_metric_value = current_value
            self.best_model_step = step
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self.save_checkpoint(step, is_best=True)
            
            # æ¸…ç†æ—§çš„æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨äº†ä»…ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
            if self.save_best_only:
                self._cleanup_old_best_models()
            
            # è®°å½•åˆ°wandb
            self.monitor.log_metrics({
                'best_model_step': step,
                f'best_{self.best_metric_name}': current_value
            }, step)
            
            self.dist_ctx.print_main(
                f"ğŸ† å‘ç°æ›´å¥½æ¨¡å‹! {self.best_metric_name}: {current_value:.4f} "
                f"(æ­¥éª¤ {step})"
            )
            return True
        
        return False
    
    def _aggregate_loss(self, loss):
        """åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­èšåˆloss"""
        if self.dist_ctx.world_size <= 1:
            return loss.item()
        
        try:
            import torch.distributed as dist
            # å°†å½“å‰GPUçš„losså¹¿æ’­åˆ°æ‰€æœ‰è¿›ç¨‹å¹¶æ±‚å¹³å‡
            loss_tensor = torch.tensor(loss.item(), dtype=torch.float32, device=self.dist_ctx.device)
            
            # ä½¿ç”¨all_reduceæ¥è®¡ç®—æ‰€æœ‰GPUçš„å¹³å‡loss
            dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
            aggregated_loss = loss_tensor.item() / self.dist_ctx.world_size
            
            return aggregated_loss
            
        except Exception as e:
            # å¦‚æœèšåˆå¤±è´¥ï¼Œè¿”å›å½“å‰GPUçš„loss
            print(f"âš ï¸  Lossèšåˆå¤±è´¥ï¼Œä½¿ç”¨å½“å‰GPU loss: {e}")
            return loss.item()
    
    def _update_dataset_metrics(self, batch, outputs, aggregated_loss):
        """æ›´æ–°å„æ•°æ®é›†çš„æŒ‡æ ‡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è®¡ç®—å¼€é”€"""
        if not self.enable_dataset_metrics:
            return
            
        dataset_names = batch.get("dataset_names", [])
        labels = batch.get("labels")
        
        if not dataset_names or labels is None or outputs.logits is None:
            return
        
        # åªåœ¨å¿…è¦æ—¶è®¡ç®—é¢„æµ‹ç»“æœï¼ˆé¿å…æ¯æ¬¡éƒ½è®¡ç®—ï¼‰
        predictions = None
        
        # æŒ‰æ•°æ®é›†ç»Ÿè®¡æŒ‡æ ‡ - ç®€åŒ–å¾ªç¯å’Œè®¡ç®—
        dataset_count = len(dataset_names)
        if dataset_count == 0:
            return
            
        # æ‰¹é‡æ›´æ–°åŸºç¡€æŒ‡æ ‡ï¼Œé¿å…é€ä¸ªæ›´æ–°
        avg_loss_per_sample = aggregated_loss / dataset_count
        
        for i, dataset_name in enumerate(dataset_names):
            if i >= len(labels):
                continue
            
            # å»¶è¿Ÿè®¡ç®—é¢„æµ‹ç»“æœï¼Œåªåœ¨éœ€è¦æ—¶è®¡ç®—
            if predictions is None:
                predictions = torch.argmax(outputs.logits, dim=-1)
            
            if i >= len(predictions):
                continue
                
            # ç®€åŒ–æŒ‡æ ‡æ›´æ–°ï¼Œå‡å°‘é‡å¤è®¡ç®—
            metrics = self.dataset_metrics[dataset_name]
            metrics['total_loss'] += avg_loss_per_sample
            metrics['total_samples'] += 1
            metrics['step_count'] += 1
            
            # åªåœ¨éœ€è¦æ—¶è¿›è¡Œtensorè½¬æ¢
            if predictions[i].item() == labels[i].item():
                metrics['correct_samples'] += 1
    
    def _log_dataset_metrics(self, step, is_eval=False):
        """è®°å½•å„æ•°æ®é›†çš„æŒ‡æ ‡"""
        if not self.enable_dataset_metrics or not self.dataset_metrics:
            return
            
        # è®¡ç®—å¹¶è¾“å‡ºå„æ•°æ®é›†çš„æŒ‡æ ‡
        dataset_log_data = {}
        overall_samples = 0
        overall_correct = 0
        
        # æ ¹æ®æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼é€‰æ‹©æŒ‡æ ‡ç»„
        metric_group = "eval" if is_eval else "training"
        
        for dataset_name, metrics in self.dataset_metrics.items():
            if metrics['total_samples'] == 0:
                continue
                
            avg_loss = metrics['total_loss'] / metrics['step_count'] if metrics['step_count'] > 0 else 0
            accuracy = metrics['correct_samples'] / metrics['total_samples']
            
            dataset_log_data[f"{metric_group}/{dataset_name}_loss"] = avg_loss
            dataset_log_data[f"{metric_group}/{dataset_name}_accuracy"] = accuracy
            dataset_log_data[f"{metric_group}/{dataset_name}_samples"] = metrics['total_samples']
            
            # ç´¯è®¡æ•´ä½“æŒ‡æ ‡
            overall_samples += metrics['total_samples']
            overall_correct += metrics['correct_samples']
            
            # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            if self.dist_ctx.is_main_process:
                prefix = "EVAL" if is_eval else "TRAIN"
                print(f"ğŸ“Š {prefix} - {dataset_name}: "
                      f"Loss={avg_loss:.4f}, Acc={accuracy:.4f} ({accuracy*100:.2f}%), "
                      f"Samples={metrics['total_samples']}")
        
        # æ·»åŠ æ•´ä½“æŒ‡æ ‡
        if overall_samples > 0:
            overall_accuracy = overall_correct / overall_samples
            dataset_log_data[f"{metric_group}/overall_accuracy"] = overall_accuracy
            dataset_log_data[f"{metric_group}/overall_samples"] = overall_samples
            dataset_log_data[f"{metric_group}/overall_correct"] = overall_correct
            
            if self.dist_ctx.is_main_process:
                prefix = "EVAL" if is_eval else "TRAIN"
                print(f"ğŸ“Š {prefix} - OVERALL: "
                      f"Acc={overall_accuracy:.4f} ({overall_accuracy*100:.2f}%), "
                      f"Samples={overall_samples}")
        
        # è®°å½•åˆ°wandb
        if dataset_log_data:
            self.monitor.log_metrics(dataset_log_data, step, commit=False)
            
        # å¦‚æœä¸æ˜¯evalæ¨¡å¼ï¼Œé‡ç½®è®­ç»ƒæŒ‡æ ‡
        if not is_eval:
            self.dataset_metrics.clear()
    
    def _forward_backward_with_profiling(self, forward_kwargs):
        """åœ¨å‰å‘+åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å®æ—¶æµ‹é‡FLOPs"""
        try:
            total_flops = 0.0
            outputs = None
            loss = None
            
            # æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒFLOPs profiling
            try:
                # ä½¿ç”¨profileråŒ…è£…å®Œæ•´çš„å‰å‘+åå‘ä¼ æ’­è¿‡ç¨‹
                with torch.profiler.profile(
                    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
                    record_shapes=True,
                    with_flops=True,
                    profile_memory=False
                ) as prof:
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(**forward_kwargs)
                    loss = outputs.loss
                    
                    # åå‘ä¼ æ’­
                    self.model.backward(loss)
                
                # æ”¶é›†FLOPsç»Ÿè®¡
                for event in prof.events():
                    if hasattr(event, 'flops') and event.flops > 0:
                        total_flops += event.flops
                
                return outputs, loss, float(total_flops)
                
            except (AttributeError, TypeError) as e:
                # å¦‚æœprofilerä¸æ”¯æŒwith_flopsï¼Œå›é€€åˆ°æ­£å¸¸æ‰§è¡Œ
                print(f"âš ï¸  Profilerä¸æ”¯æŒFLOPsæµ‹é‡ï¼Œä½¿ç”¨æ­£å¸¸æ¨¡å¼: {e}")
                outputs = self.model(**forward_kwargs)
                loss = outputs.loss
                self.model.backward(loss)
                return outputs, loss, 0.0
                
        except Exception as e:
            print(f"âŒ å®æ—¶FLOPsæµ‹é‡å¤±è´¥: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶æ‰§è¡Œæ­£å¸¸çš„å‰å‘+åå‘ä¼ æ’­
            outputs = self.model(**forward_kwargs)
            loss = outputs.loss
            self.model.backward(loss)
            return outputs, loss, 0.0
        
    def evaluate(self, step=None):
        """è¯„ä¼°æ¨¡å‹ï¼Œç»Ÿä¸€ä½¿ç”¨å¤šæ•°æ®é›†è¯„ä¼°é€»è¾‘
        
        Args:
            step: å½“å‰æ­¥æ•°ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºæœ€ä½³æ¨¡å‹ä¿å­˜ï¼›å¦åˆ™ä½¿ç”¨self.current_step
        """
        self.dist_ctx.print_main("ğŸ” å¼€å§‹è¯„ä¼°...")
        
        # ç»Ÿä¸€ä½¿ç”¨å¤šæ•°æ®é›†è¯„ä¼°å‡½æ•°
        eval_results = evaluate_multi_dataset(self.model, self.val_loader, self.dist_ctx.device, self.dataset_configs)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.dist_ctx.print_main(f"ğŸ” eval_results åŸå§‹æ•°æ®: {eval_results}")
        
        # å‡†å¤‡wandbè®°å½•æ•°æ®
        eval_log_data = {}
        overall_samples = 0
        overall_correct = 0
        
        # å¤„ç†æ•°æ®é›†æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if eval_results and 'dataset_metrics' in eval_results and eval_results['dataset_metrics']:
            self.dist_ctx.print_main(f"ğŸ“Š æ£€æµ‹åˆ°å¤šæ•°æ®é›†è¯„ä¼°ç»“æœ:")
            # å¤šæ•°æ®é›†æƒ…å†µï¼šè®°å½•æ¯ä¸ªæ•°æ®é›†çš„æŒ‡æ ‡
            for dataset_name, metrics in eval_results['dataset_metrics'].items():
                eval_log_data[f"eval/{dataset_name}_loss"] = metrics['loss']
                eval_log_data[f"eval/{dataset_name}_accuracy"] = metrics['accuracy']
                eval_log_data[f"eval/{dataset_name}_samples"] = metrics['samples']
                
                overall_samples += metrics['samples']
                overall_correct += metrics['correct']
                
                # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„è¯¦ç»†ç»“æœ
                self.dist_ctx.print_main(f"  ğŸ“‚ {dataset_name}:")
                self.dist_ctx.print_main(f"     Loss: {metrics['loss']:.6f}")
                self.dist_ctx.print_main(f"     Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
                self.dist_ctx.print_main(f"     Samples: {metrics['samples']:,} (Correct: {metrics['correct']:,})")
        else:
            self.dist_ctx.print_main(f"ğŸ“Š æ£€æµ‹åˆ°å•æ•°æ®é›†è¯„ä¼°ç»“æœ:")
            # å•æ•°æ®é›†æƒ…å†µï¼šä½¿ç”¨overallæŒ‡æ ‡ä½œä¸ºä¸»è¦æŒ‡æ ‡
            eval_log_data["eval/loss"] = eval_results.get('overall_loss', 0)
            eval_log_data["eval/accuracy"] = eval_results.get('overall_accuracy', 0)
            overall_samples = eval_results.get('total_samples', 0)
            overall_correct = eval_results.get('total_correct', 0)
            
            self.dist_ctx.print_main(f"  ğŸ“ˆ Loss: {eval_results.get('overall_loss', 0):.6f}")
            self.dist_ctx.print_main(f"  ğŸ¯ Accuracy: {eval_results.get('overall_accuracy', 0):.4f} ({eval_results.get('overall_accuracy', 0)*100:.2f}%)")
            self.dist_ctx.print_main(f"  ğŸ“Š Samples: {overall_samples:,} (Correct: {overall_correct:,})")
        
        # æ·»åŠ æ•´ä½“æŒ‡æ ‡ï¼ˆé€‚ç”¨äºå•æ•°æ®é›†å’Œå¤šæ•°æ®é›†ï¼‰
        if overall_samples > 0:
            overall_accuracy = overall_correct / overall_samples
        else:
            overall_accuracy = eval_results.get('overall_accuracy', 0)
            overall_samples = eval_results.get('total_samples', 0)
            overall_correct = eval_results.get('total_correct', 0)
        
        # æ€»æ˜¯æ·»åŠ æ•´ä½“æŒ‡æ ‡
        overall_loss = eval_results.get('overall_loss', 0)
        eval_log_data["eval/overall_loss"] = overall_loss
        eval_log_data["eval/overall_accuracy"] = overall_accuracy
        eval_log_data["eval/overall_samples"] = overall_samples
        eval_log_data["eval/overall_correct"] = overall_correct
        
        # è®°å½•åˆ°wandb
        current_step = step if step is not None else self.current_step
        
        # æ‰“å°ç»¼åˆè¯„ä¼°ç»“æœ
        self.dist_ctx.print_main("=" * 80)
        self.dist_ctx.print_main(f"ğŸ“Š è¯„ä¼°å®Œæˆ (Step {current_step})")
        self.dist_ctx.print_main("=" * 80)
        self.dist_ctx.print_main(f"ğŸ¯ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
        self.dist_ctx.print_main(f"ğŸ“ˆ æ•´ä½“æŸå¤±:   {overall_loss:.6f}")
        self.dist_ctx.print_main(f"ğŸ“Š æ€»æ ·æœ¬æ•°:   {overall_samples:,}")
        self.dist_ctx.print_main(f"âœ… æ­£ç¡®æ ·æœ¬:   {overall_correct:,}")
        self.dist_ctx.print_main("=" * 80)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.dist_ctx.print_main(f"ğŸ” å‡†å¤‡è®°å½•evalæŒ‡æ ‡åˆ°wandb (step={current_step}):")
        for key, value in eval_log_data.items():
            self.dist_ctx.print_main(f"   â€¢ {key}: {value}")
        
        self.monitor.log_metrics(eval_log_data, current_step)
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        eval_results_for_best = {
            'overall_loss': overall_loss,
            'overall_accuracy': overall_accuracy
        }
        self._update_best_model(eval_results_for_best, current_step)
        
        # è¿”å›æ•´ä½“æŒ‡æ ‡
        self.dist_ctx.print_main(f"âœ… è¯„ä¼°ç»“æŸ - éªŒè¯æŸå¤±: {overall_loss:.4f}, å‡†ç¡®ç‡: {overall_accuracy:.4f}")
        return overall_loss, overall_accuracy
    
    def full_evaluation_on_best_model(self):
        """åœ¨æœ€ä½³æ¨¡å‹ä¸Šè¿›è¡Œå®Œæ•´è¯„ä¼°"""
        if not self.full_eval_at_end or not self.best_model_path:
            return
        
        self.dist_ctx.print_main("\n" + "="*80)
        self.dist_ctx.print_main("ğŸ” å¼€å§‹å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œå®Œæ•´è¯„ä¼°")
        self.dist_ctx.print_main("="*80)
        
        # åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨
        # å®‰å…¨åœ°è·å–processorï¼Œé¿å…DeepSpeedåŒ…è£…å¯¼è‡´çš„å±æ€§è®¿é—®é”™è¯¯
        try:
            processor = self.model.module.processor
        except AttributeError:
            try:
                processor = self.model.processor
            except AttributeError:
                processor = None
                self.dist_ctx.print_main("âš ï¸ æ— æ³•è·å–æ¨¡å‹processorï¼Œå°†ä»é…ç½®ä¸­é‡æ–°åŠ è½½")
        
        full_eval_loader = create_full_eval_dataloader(self.config, processor)
        
        if full_eval_loader is None:
            self.dist_ctx.print_main("âš ï¸ æ— æ³•åˆ›å»ºå®Œæ•´è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼Œè·³è¿‡å®Œæ•´è¯„ä¼°")
            return
        
        # ç»Ÿä¸€ä½¿ç”¨å¤šæ•°æ®é›†è¯„ä¼°å‡½æ•°
        eval_results = evaluate_multi_dataset(self.model, full_eval_loader, self.dist_ctx.device, self.dataset_configs)
        
        # å‡†å¤‡wandbè®°å½•æ•°æ®
        eval_log_data = {}
        overall_samples = 0
        overall_correct = 0
        
        # å¤„ç†æ•°æ®é›†æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if eval_results and 'dataset_metrics' in eval_results and eval_results['dataset_metrics']:
            # å¤šæ•°æ®é›†æƒ…å†µï¼šè®°å½•æ¯ä¸ªæ•°æ®é›†çš„æŒ‡æ ‡
            for dataset_name, metrics in eval_results['dataset_metrics'].items():
                eval_log_data[f"eval/final_{dataset_name}_loss"] = metrics['loss']
                eval_log_data[f"eval/final_{dataset_name}_accuracy"] = metrics['accuracy']
                eval_log_data[f"eval/final_{dataset_name}_samples"] = metrics['samples']
                
                overall_samples += metrics['samples']
                overall_correct += metrics['correct']
        else:
            # å•æ•°æ®é›†æƒ…å†µï¼šä½¿ç”¨overallæŒ‡æ ‡ä½œä¸ºä¸»è¦æŒ‡æ ‡
            eval_log_data["eval/final_loss"] = eval_results.get('overall_loss', 0)
            eval_log_data["eval/final_accuracy"] = eval_results.get('overall_accuracy', 0)
            overall_samples = eval_results.get('total_samples', 0)
            overall_correct = eval_results.get('total_correct', 0)
        
        # æ·»åŠ æ•´ä½“æŒ‡æ ‡ï¼ˆé€‚ç”¨äºå•æ•°æ®é›†å’Œå¤šæ•°æ®é›†ï¼‰
        if overall_samples > 0:
            overall_accuracy = overall_correct / overall_samples
        else:
            overall_accuracy = eval_results.get('overall_accuracy', 0)
            overall_samples = eval_results.get('total_samples', 0)
            overall_correct = eval_results.get('total_correct', 0)
        
        # æ€»æ˜¯æ·»åŠ æ•´ä½“æŒ‡æ ‡
        eval_log_data["eval/final_overall_loss"] = eval_results.get('overall_loss', 0)
        eval_log_data["eval/final_overall_accuracy"] = overall_accuracy
        eval_log_data["eval/final_overall_samples"] = overall_samples
        eval_log_data["eval/final_overall_correct"] = overall_correct
        
        # è®°å½•åˆ°wandb
        self.monitor.log_metrics(eval_log_data, self.best_model_step)
        
        # æ˜¾ç¤ºç»“æœ
        self.dist_ctx.print_main(f"\nğŸ¯ æœ€ä½³æ¨¡å‹å®Œæ•´è¯„ä¼°ç»“æœ:")
        self.dist_ctx.print_main(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
        self.dist_ctx.print_main(f"   â€¢ æ€»æ ·æœ¬æ•°: {overall_samples:,}")
        self.dist_ctx.print_main(f"   â€¢ æ­£ç¡®æ ·æœ¬æ•°: {overall_correct:,}")
        
        self.dist_ctx.print_main("="*80)
        
        return {
            'overall_loss': eval_results.get('overall_loss', 0),
            'overall_accuracy': overall_accuracy,
            'dataset_metrics': eval_results.get('dataset_metrics', {})
        }
        
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        self.dist_ctx.print_main("å¼€å§‹è®­ç»ƒ...")
        self.monitor.start_training()
        
        num_epochs = self.config['training']['num_epochs']
        logging_steps = self.config['logging_steps']
        save_steps = self.config['save_steps']
        eval_steps = self.config['eval_steps']
        
        # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆè€ƒè™‘DeepSpeedçš„åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
        deepspeed_config = self.config.get('deepspeed', {})
        if isinstance(deepspeed_config, str):
            with open(deepspeed_config, 'r') as f:
                deepspeed_config = json.load(f)
        
        # è·å–DeepSpeedå‚æ•°
        micro_batch_size_per_gpu = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)
        gradient_accumulation_steps = deepspeed_config.get('gradient_accumulation_steps', 1)
        train_batch_size = deepspeed_config.get('train_batch_size', 32)
        
        # è®¡ç®—æœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼ˆåŸºäºå®é™…çš„DataLoaderé•¿åº¦ï¼‰
        dataloader_steps_per_epoch = len(self.train_loader)
        effective_steps_per_epoch = dataloader_steps_per_epoch // gradient_accumulation_steps
        total_effective_steps = effective_steps_per_epoch * num_epochs
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåŸºäºæœ‰æ•ˆè®­ç»ƒæ­¥æ•°ï¼‰
        pbar = tqdm(total=total_effective_steps, desc="Training Steps", disable=not self.dist_ctx.is_main_process)
        
        # è®¡ç®—éªŒè¯ä¿¡æ¯
        dataset_size = len(self.train_loader.dataset)
        samples_per_gpu = dataloader_steps_per_epoch * micro_batch_size_per_gpu
        
        # ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼è¾“å‡ºè®­ç»ƒé…ç½®ä¿¡æ¯
        if self.dist_ctx.is_main_process:
            print("="*80)
            print("ğŸš€ è®­ç»ƒé…ç½®ä¿¡æ¯")
            print("="*80)
            print(f"ğŸ“Š æ•°æ®é›†é…ç½®:")
            print(f"  â€¢ æ€»æ•°æ®é›†å¤§å°: {dataset_size:,}")
            print(f"  â€¢ æ¯GPUå¤„ç†æ ·æœ¬æ•°: {samples_per_gpu:,}")
            print(f"ğŸ“¦ æ‰¹æ¬¡é…ç½®:")
            print(f"  â€¢ æ¯GPUå¾®æ‰¹æ¬¡å¤§å°: {micro_batch_size_per_gpu}")
            print(f"  â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
            print(f"  â€¢ æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_batch_size}")
            print(f"ğŸ“ˆ æ­¥æ•°ç»Ÿè®¡:")
            print(f"  â€¢ æ¯GPU DataLoaderæ­¥æ•°: {dataloader_steps_per_epoch:,}")
            print(f"  â€¢ æœ‰æ•ˆè®­ç»ƒæ­¥æ•°æ¯epoch: {effective_steps_per_epoch:,}")
            print(f"  â€¢ æ€»æœ‰æ•ˆè®­ç»ƒæ­¥æ•°: {total_effective_steps:,}")
            print("="*80)
        
        effective_step = 0  # ç”¨äºè·Ÿè¸ªæœ‰æ•ˆæ­¥æ•°
        flops_profiled = False  # æ ‡è®°æ˜¯å¦å·²ç»æµ‹é‡è¿‡FLOPs
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            self.model.train()
            
            # ä¸ºåˆ†å¸ƒå¼é‡‡æ ·å™¨è®¾ç½®epochï¼ˆç¡®ä¿æ¯ä¸ªepochçš„shuffleæ­£ç¡®ï¼‰
            if hasattr(self.train_loader.sampler, 'set_epoch'):
                self.train_loader.sampler.set_epoch(epoch)
            
            epoch_loss = 0
            epoch_start_time = time.time()
            
            for batch_idx, batch in enumerate(self.train_loader):
                self.current_step += 1
                
                # å‰å‘ä¼ æ’­
                inputs = batch["input_ids"].to(self.dist_ctx.device)
                attention_mask = batch["attention_mask"].to(self.dist_ctx.device)
                pixel_values = batch["pixel_values"].to(self.dist_ctx.device)
                labels = batch["labels"].to(self.dist_ctx.device)
                
                # æ·»åŠ image_grid_thwå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                forward_kwargs = {
                    "input_ids": inputs,
                    "attention_mask": attention_mask,
                    "pixel_values": pixel_values,
                    "labels": labels
                }
                
                # æ£€æŸ¥å¹¶æ·»åŠ image_grid_thwå‚æ•°
                if "image_grid_thw" in batch:
                    forward_kwargs["image_grid_thw"] = batch["image_grid_thw"].to(self.dist_ctx.device)
                
                # æ·»åŠ å¤šæ•°æ®é›†æ”¯æŒçš„å‚æ•°
                if "dataset_names" in batch:
                    forward_kwargs["dataset_names"] = batch["dataset_names"]
                if "num_classes_list" in batch:
                    forward_kwargs["num_classes_list"] = batch["num_classes_list"]
                
                # å†³å®šæ˜¯å¦è¿›è¡Œå®æ—¶FLOPsæµ‹é‡ - å¤§å¹…å‡å°‘é¢‘ç‡ä»¥é¿å…æ€§èƒ½å¼€é”€
                should_measure_flops = (
                    not flops_profiled or  # ä»…ç¬¬ä¸€æ¬¡æµ‹é‡
                    (effective_step > 0 and effective_step % 500 == 0)  # æ¯500ä¸ªæœ‰æ•ˆæ­¥éª¤é‡æ–°æµ‹é‡ï¼ˆè€Œä¸æ˜¯50æ­¥ï¼‰
                )
                
                # å®æ—¶FLOPsæµ‹é‡å’Œæ¨¡å‹å‰å‘+åå‘ä¼ æ’­
                if should_measure_flops and self.dist_ctx.is_main_process:
                    # åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œå®æ—¶FLOPsæµ‹é‡ï¼ˆä»…é™å¿…è¦æ—¶ï¼‰
                    outputs, loss, real_time_flops = self._forward_backward_with_profiling(forward_kwargs)
                    
                    # æ›´æ–°FLOPsä¿¡æ¯
                    if real_time_flops > 0:
                        self.monitor.set_actual_flops(real_time_flops, attention_mask.size(1))
                        if not flops_profiled:
                            print(f"âœ… å®æ—¶æµ‹é‡FLOPs: {real_time_flops:.2e}")
                        flops_profiled = True
                else:
                    # æ­£å¸¸çš„å‰å‘+åå‘ä¼ æ’­ï¼ˆæ— profilingå¼€é”€ï¼‰ - å¤§å¤šæ•°æ­¥éª¤ä½¿ç”¨è¿™ä¸ªè·¯å¾„
                    outputs = self.model(**forward_kwargs)
                    loss = outputs.loss
                    self.model.backward(loss)
                    real_time_flops = None
                
                # ç®€åŒ–åˆ†å¸ƒå¼FLOPsåŒæ­¥ - ä»…åœ¨é¦–æ¬¡æµ‹é‡æ—¶åŒæ­¥ï¼Œé¿å…æ¯æ¬¡éƒ½å¹¿æ’­
                if should_measure_flops and real_time_flops is not None and self.dist_ctx.world_size > 1 and not flops_profiled:
                    import torch.distributed as dist
                    
                    # ä»…åœ¨é¦–æ¬¡æˆåŠŸæµ‹é‡FLOPsæ—¶è¿›è¡Œä¸€æ¬¡åŒæ­¥
                    flops_tensor = torch.tensor(real_time_flops, dtype=torch.float32, device=self.dist_ctx.device)
                    seq_tensor = torch.tensor(attention_mask.size(1), dtype=torch.float32, device=self.dist_ctx.device)
                    
                    try:
                        dist.broadcast(flops_tensor, src=0)
                        dist.broadcast(seq_tensor, src=0)
                        
                        # æ‰€æœ‰è¿›ç¨‹æ›´æ–°FLOPsä¿¡æ¯
                        if not self.dist_ctx.is_main_process:
                            self.monitor.set_actual_flops(flops_tensor.item(), int(seq_tensor.item()))
                    except Exception as e:
                        print(f"âš ï¸  FLOPsåŒæ­¥å¤±è´¥: {e}")
                
                # èšåˆå¤šå¡lossï¼ˆåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼‰
                aggregated_loss = self._aggregate_loss(loss)
                epoch_loss += aggregated_loss
                
                # ä¼˜åŒ–æ•°æ®é›†æŒ‡æ ‡æ›´æ–° - é™ä½é¢‘ç‡ä»¥å‡å°‘å¼€é”€
                if self.enable_dataset_metrics and (self.current_step % 10 == 0):  # æ¯10æ­¥æ›´æ–°ä¸€æ¬¡è€Œä¸æ˜¯æ¯æ­¥
                    self._update_dataset_metrics(batch, outputs, aggregated_loss)
                
                # è·å–æ¢¯åº¦èŒƒæ•°
                grad_norm = self.model.get_global_grad_norm()
                
                # æ›´æ–°å‚æ•°
                self.model.step()
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡ï¼ˆå‡†å¤‡æ•°æ®ï¼‰
                current_lr = self.optimizer.param_groups[0]['lr']
                # ç¡®ä¿grad_normæ˜¯floatç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
                # å¤„ç†grad_normå¯èƒ½ä¸ºNoneçš„æƒ…å†µ
                if grad_norm is None:
                    grad_norm_value = 0.0
                elif hasattr(grad_norm, 'item'):
                    grad_norm_value = float(grad_norm.item())
                else:
                    grad_norm_value = float(grad_norm)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ­¥éª¤ï¼ˆå®Œæˆäº†æ¢¯åº¦ç´¯ç§¯ï¼‰
                is_effective_step = self.current_step % gradient_accumulation_steps == 0
                
                if is_effective_step:
                    effective_step += 1
                    
                    # é™ä½è¿›åº¦æ¡æ›´æ–°é¢‘ç‡ä»¥å‡å°‘å¼€é”€ï¼ˆæ¯10ä¸ªæœ‰æ•ˆæ­¥éª¤æ›´æ–°ä¸€æ¬¡ï¼‰
                    if effective_step % 10 == 0:
                        pbar.update(10)  # ä¸€æ¬¡æ›´æ–°10æ­¥
                        pbar.set_postfix({
                            'loss': f'{aggregated_loss:.4f}',
                            'lr': f'{current_lr:.2e}',
                            'epoch': f'{epoch + batch_idx/len(self.train_loader):.2f}'
                        })
                    
                    # ä¼˜åŒ–ç›‘æ§è®°å½• - ä»…åœ¨å¿…è¦æ—¶ä¼ é€’real_time_flops
                    step_real_time_flops = real_time_flops if should_measure_flops and real_time_flops is not None else None
                    self.monitor.log_step(effective_step, epoch, aggregated_loss, grad_norm_value, current_lr, attention_mask, step_real_time_flops)
                
                    # è¯¦ç»†æ—¥å¿—è®°å½•ï¼ˆåŸºäºæœ‰æ•ˆæ­¥æ•°åˆ¤æ–­è¾“å‡ºé¢‘ç‡ï¼‰
                    if effective_step % logging_steps == 0:
                        # è®°å½•å„æ•°æ®é›†çš„æŒ‡æ ‡
                        self._log_dataset_metrics(effective_step, is_eval=False)
                        
                        # åŸºç¡€æ—¥å¿—ä¿¡æ¯
                        log_message = (
                            f"Step {effective_step:,} | "
                            f"Loss: {aggregated_loss:.4f} | "
                            f"Grad Norm: {grad_norm_value:.4f} | "
                            f"LR: {current_lr:.2e} | "
                            f"Epoch: {epoch + batch_idx/len(self.train_loader):.2f}"
                        )
                        
                        # å¦‚æœè¿›è¡Œäº†å®æ—¶FLOPsæµ‹é‡ï¼Œæ·»åŠ MFUä¿¡æ¯
                        if should_measure_flops and hasattr(self.monitor, 'actual_flops') and self.monitor.actual_flops:
                            # è®¡ç®—å½“å‰æ­¥éª¤çš„æ—¶é—´ï¼ˆä»ä¸Šæ¬¡è®°å½•åˆ°ç°åœ¨ï¼‰
                            current_time = time.time()
                            actual_step_time = current_time - self.monitor.step_start_time
                            
                            current_seq_length = self.monitor._calculate_actual_seq_length(attention_mask)
                            # ä½¿ç”¨å®é™…çš„æ‰¹æ¬¡å¤§å°ï¼ˆè€ƒè™‘åˆ†å¸ƒå¼è®­ç»ƒï¼‰
                            actual_batch_size = inputs.size(0) * self.dist_ctx.world_size
                            current_mfu = calculate_mfu(self.model, actual_batch_size, current_seq_length, 
                                                      actual_step_time, self.monitor.actual_flops)
                            log_message += f" | MFU: {current_mfu:.1%}"
                            
                            if should_measure_flops:
                                log_message += " [ğŸ“Šå®æ—¶æµ‹é‡]"
                        
                        # æ‰“å°æ—¥å¿—ä¿¡æ¯
                        if self.dist_ctx.is_main_process:
                            pbar.write(log_message)
                    
                    # å®šæœŸè¯„ä¼°ï¼ˆåŸºäºæœ‰æ•ˆæ­¥æ•°ï¼‰
                    if effective_step > 0 and effective_step % eval_steps == 0:
                        # æš‚æ—¶åˆ·æ–°è¿›åº¦æ¡ä»¥é¿å…è¾“å‡ºå†²çª
                        pbar.clear()
                        
                        # æ·»åŠ è¯„ä¼°å¼‚å¸¸å¤„ç†ï¼Œé¿å…NCCLè¶…æ—¶å¯¼è‡´è®­ç»ƒä¸­æ–­
                        try:
                            eval_loss, eval_accuracy = self.evaluate(step=effective_step)
                            # æ³¨æ„ï¼šè¯„ä¼°ç»“æœå·²ç»åœ¨evaluateæ–¹æ³•ä¸­è®°å½•åˆ°wandbäº†ï¼Œæ— éœ€é‡å¤è®°å½•
                        except Exception as eval_error:
                            if self.dist_ctx.is_main_process:
                                print(f"âš ï¸  è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {eval_error}")
                                print("âš ï¸  è·³è¿‡æœ¬æ¬¡è¯„ä¼°ï¼Œç»§ç»­è®­ç»ƒ...")
                            # è®°å½•ä¸€ä¸ªå ä½ç¬¦çš„evalç»“æœï¼Œé¿å…wandbå›¾è¡¨ä¸­æ–­
                            try:
                                placeholder_eval_data = {
                                    "eval/overall_loss": 999.0,  # ä½¿ç”¨æ˜æ˜¾çš„å ä½ç¬¦å€¼
                                    "eval/overall_accuracy": 0.0,
                                    "eval/evaluation_failed": 1.0,  # æ ‡è®°è¯„ä¼°å¤±è´¥
                                }
                                self.monitor.log_metrics(placeholder_eval_data, effective_step)
                            except:
                                pass  # å¦‚æœè¿è®°å½•éƒ½å¤±è´¥ï¼Œå°±å®Œå…¨è·³è¿‡
                        
                        self.model.train()
                        # é‡æ–°æ˜¾ç¤ºè¿›åº¦æ¡
                        pbar.refresh()
                    
                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŸºäºæœ‰æ•ˆæ­¥æ•°ï¼‰
                    if effective_step > 0 and effective_step % save_steps == 0:
                        if not self.save_best_only:  # åªæœ‰åœ¨æœªå¯ç”¨"ä»…ä¿å­˜æœ€ä½³æ¨¡å‹"æ—¶æ‰ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
                            pbar.clear()
                            self.save_checkpoint(effective_step)
                            pbar.refresh()
                        elif self.dist_ctx.is_main_process:  # å¦‚æœå¯ç”¨äº†ä»…ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œåªæ˜¾ç¤ºä¿¡æ¯
                            pbar.write(f"ğŸ’¡ ä»…ä¿å­˜æœ€ä½³æ¨¡å‹æ¨¡å¼å·²å¯ç”¨ï¼Œè·³è¿‡æ­¥éª¤ {effective_step} çš„å¸¸è§„æ£€æŸ¥ç‚¹ä¿å­˜")
            
            # Epochç»“æŸç»Ÿè®¡
            epoch_time = time.time() - epoch_start_time
            avg_loss = epoch_loss / len(self.train_loader)
            self.monitor.log_epoch(epoch, avg_loss, epoch_time, effective_step)
            
            # ä½¿ç”¨tqdm.write()è¾“å‡ºepochç»Ÿè®¡ä¿¡æ¯
            epoch_message = (
                f"ğŸ“Š Epoch {epoch+1}/{num_epochs} å®Œæˆ | "
                f"å¹³å‡æŸå¤±: {avg_loss:.4f} | "
                f"è€—æ—¶: {epoch_time:.2f}ç§’ | "
                f"æœ‰æ•ˆæ­¥æ•°: {effective_step:,}"
            )
            if self.dist_ctx.is_main_process:
                pbar.write(epoch_message)
        
        pbar.close()
        
        # è®­ç»ƒç»“æŸå‰è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        if self.dist_ctx.is_main_process:
            print("\nğŸ¯ è®­ç»ƒå³å°†å®Œæˆï¼Œè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        eval_loss, eval_accuracy = self.evaluate(step=effective_step)
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæœªå¯ç”¨ä»…ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
        if not self.save_best_only:
            if self.dist_ctx.is_main_process:
                print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹...")
            self.save_checkpoint(effective_step)
        elif self.dist_ctx.is_main_process:
            print(f"ğŸ’¡ ä»…ä¿å­˜æœ€ä½³æ¨¡å‹æ¨¡å¼å·²å¯ç”¨ï¼Œè·³è¿‡æœ€ç»ˆæ£€æŸ¥ç‚¹ä¿å­˜")
        
        # è¿›è¡Œå®Œæ•´è¯„ä¼°ï¼ˆåœ¨æœ€ä½³æ¨¡å‹ä¸Šï¼‰
        if self.full_eval_at_end:
            self.full_evaluation_on_best_model()
        
        if self.dist_ctx.is_main_process:
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ - æŸå¤±: {eval_loss:.4f}, å‡†ç¡®ç‡: {eval_accuracy:.4f}")
            if self.best_model_enabled:
                print(f"ğŸ† æœ€ä½³æ¨¡å‹ - {self.best_metric_name}: {self.best_metric_value:.4f} (æ­¥éª¤ {self.best_model_step})")
                print(f"ğŸ† æœ€ä½³æ¨¡å‹è·¯å¾„: {self.best_model_path}")
        
        # è®°å½•æœ€ç»ˆè¯„ä¼°ç»“æœå¹¶ç»“æŸwandb run
        # æ³¨æ„ï¼šå¦‚æœæ˜¯å¤šæ•°æ®é›†è¯„ä¼°ï¼Œç»“æœå·²ç»åœ¨evaluateæ–¹æ³•ä¸­è®°å½•äº†
        # åªæœ‰å•æ•°æ®é›†æƒ…å†µä¸‹æ‰éœ€è¦è¿™é‡Œè®°å½•
        if not (self.dataset_configs and self.enable_dataset_metrics):
            self.monitor.log_evaluation(effective_step, eval_loss, eval_accuracy)
        self.monitor.save_logs()
        
        # è®­ç»ƒç»“æŸåè¿›è¡Œæœ€ç»ˆæ¸…ç†
        if self.save_best_only and self.dist_ctx.is_main_process:
            self.dist_ctx.print_main("ğŸ§¹ è¿›è¡Œæœ€ç»ˆæ£€æŸ¥ç‚¹æ¸…ç†...")
            self._cleanup_old_best_models()
        
        self.monitor.finish_training()
        
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            self.model.load_checkpoint(checkpoint_path)
            self.dist_ctx.print_main(f"æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
        else:
            self.dist_ctx.print_main(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            
    def get_training_stats(self):
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return self.monitor.get_avg_metrics() 