from torch.optim import AdamW

def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œæ”¯æŒlrå’Œlearning_rateä¸¤ç§å­—æ®µå
    training_config = config['training']
    lr = training_config.get('lr') or training_config.get('learning_rate')
    weight_decay = training_config['weight_decay']
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºä»é…ç½®ä¸­è¯»å–çš„åŸå§‹å€¼
    print(f"ğŸ” ä¼˜åŒ–å™¨é…ç½®è°ƒè¯•:")
    print(f"  â€¢ åŸå§‹lrå€¼: {lr} (ç±»å‹: {type(lr)})")
    print(f"  â€¢ åŸå§‹weight_decayå€¼: {weight_decay} (ç±»å‹: {type(weight_decay)})")
    
    # ç¡®ä¿å­¦ä¹ ç‡æ˜¯æ•°å­—ç±»å‹
    if isinstance(lr, str):
        lr = float(lr)
    if isinstance(weight_decay, str):
        weight_decay = float(weight_decay)
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç±»å‹è½¬æ¢åçš„å€¼
    print(f"  â€¢ è½¬æ¢ålrå€¼: {lr} (ç±»å‹: {type(lr)})")
    print(f"  â€¢ è½¬æ¢åweight_decayå€¼: {weight_decay} (ç±»å‹: {type(weight_decay)})")
        
    no_decay = ["bias", "LayerNorm.weight"]
    
    # åˆ†åˆ«æ”¶é›†å‚æ•°ï¼Œç¡®ä¿ä¸ä¸ºç©º
    decay_params = [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad]
    no_decay_params = [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad]
    
    grouped = []
    if decay_params:
        grouped.append({
            "params": decay_params,
            "lr": lr,
            "weight_decay": weight_decay,
        })
    if no_decay_params:
        grouped.append({
            "params": no_decay_params,
            "lr": lr,
            "weight_decay": 0.0,
        })
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å‚æ•°ç»„ï¼Œä½¿ç”¨æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
    if not grouped:
        grouped = [{
            "params": [p for p in model.parameters() if p.requires_grad],
            "lr": lr,
            "weight_decay": weight_decay,
        }]
    
    optimizer = AdamW(grouped)
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåˆ›å»ºçš„ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
    print(f"ğŸ” ä¼˜åŒ–å™¨åˆ›å»ºç»“æœ:")
    for i, param_group in enumerate(optimizer.param_groups):
        print(f"  â€¢ å‚æ•°ç»„ {i}: lr={param_group['lr']}, weight_decay={param_group['weight_decay']}")
    
    return optimizer
