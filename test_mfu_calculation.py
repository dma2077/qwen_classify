#!/usr/bin/env python3
"""
MFUè®¡ç®—æµ‹è¯• - åˆ†æå½“å‰MFUè®¡ç®—æ–¹å¼çš„é—®é¢˜
"""

import os
import time
import json
import torch
import wandb
from typing import Dict

# æ¨¡æ‹Ÿé…ç½®
config = {
    'output_dir': './test_output',
    'wandb': {
        'enabled': True,
        'project': 'qwen_classify_test',
        'run_name': 'mfu_calculation_test',
        'tags': ['test', 'mfu']
    },
    'monitor': {
        'freq': {
            'all_freq': 1
        }
    },
    'model': {
        'max_sequence_length': 512
    },
    'deepspeed': {
        'train_batch_size': 32
    }
}

def get_gpu_peak_flops():
    """è·å–GPUå³°å€¼FLOPsæ€§èƒ½"""
    try:
        if not torch.cuda.is_available():
            return 312e12  # é»˜è®¤å€¼
        
        # è·å–GPUåç§°
        gpu_name = torch.cuda.get_device_name(0).upper()
        print(f"ğŸ” æ£€æµ‹åˆ°GPU: {gpu_name}")
        
        # ä¸åŒGPUçš„å³°å€¼æ€§èƒ½ (TFLOPs for FP16/BF16)
        gpu_peak_flops = {
            'A100': 312e12,    # A100 80GB
            'A100-SXM': 312e12,
            'A100-PCIE': 312e12,
            'A800': 280e12,    # A800 80GB
            'H100': 989e12,    # H100 80GB
            'H100-SXM': 989e12,
            'H100-PCIE': 756e12,
            'H800': 850e12,    # H800 80GB
            'V100': 112e12,    # V100 32GB
            'RTX 4090': 165e12,
            'RTX 4080': 112e12,
            'RTX 3090': 71e12,
            'RTX 3080': 58e12,
            'T4': 65e12,
            'L4': 121e12,
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„GPU
        for gpu_model, peak_flops in gpu_peak_flops.items():
            if gpu_model in gpu_name:
                print(f"âœ… è¯†åˆ«GPU: {gpu_name} -> {gpu_model} ({peak_flops/1e12:.0f} TFLOPs)")
                return peak_flops
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„GPUï¼Œä½¿ç”¨é»˜è®¤å€¼
        print(f"âš ï¸  æœªè¯†åˆ«çš„GPUç±»å‹: {gpu_name}ï¼Œä½¿ç”¨é»˜è®¤å³°å€¼æ€§èƒ½ (A100: 312 TFLOPs)")
        return 312e12
        
    except Exception as e:
        print(f"è·å–GPUå³°å€¼æ€§èƒ½é”™è¯¯: {e}")
        return 312e12

def calculate_mfu_simple(actual_flops: float, step_time: float) -> float:
    """ç®€å•çš„MFUè®¡ç®—"""
    try:
        if actual_flops is None or actual_flops <= 0:
            return 0.0
        
        # è®¡ç®—å®é™…FLOPs/s
        actual_flops_per_second = actual_flops / step_time
        
        # è·å–GPUå³°å€¼æ€§èƒ½
        peak_flops_per_second = get_gpu_peak_flops()
        
        # è®¡ç®—MFU
        mfu = actual_flops_per_second / peak_flops_per_second
        return min(mfu, 1.0)  # é™åˆ¶åœ¨100%ä»¥å†…
        
    except Exception as e:
        print(f"MFUè®¡ç®—é”™è¯¯: {e}")
        return 0.0

def estimate_flops_for_qwen2_5_vl(batch_size: int, seq_length: int, num_classes: int = 101):
    """ä¸ºQwen2.5-VLæ¨¡å‹ä¼°ç®—FLOPs"""
    
    # Qwen2.5-VL-7Bçš„å‚æ•°æ•°é‡ï¼ˆè¿‘ä¼¼ï¼‰
    model_params = 7.2e9  # 7.2Bå‚æ•°
    
    # æ›´å‡†ç¡®çš„FLOPsä¼°ç®—ï¼ˆåŸºäºTransformeræ¶æ„ï¼‰
    # å¯¹äº7Bæ¨¡å‹ï¼Œæ¯ä¸ªtokençš„å‰å‘ä¼ æ’­å¤§çº¦éœ€è¦ï¼š
    # - æ³¨æ„åŠ›æœºåˆ¶: ~4 * hidden_size * seq_length
    # - å‰é¦ˆç½‘ç»œ: ~8 * hidden_size * seq_length  
    # - å…¶ä»–æ“ä½œ: ~2 * hidden_size * seq_length
    # æ€»è®¡: ~14 * hidden_size * seq_length per token
    
    hidden_size = 4096  # Qwen2.5-VL-7Bçš„hidden size
    flops_per_token = 14 * hidden_size * seq_length
    
    # å‰å‘ä¼ æ’­FLOPs
    forward_flops = flops_per_token * batch_size
    
    # åå‘ä¼ æ’­FLOPsï¼ˆé€šå¸¸æ˜¯å‰å‘ä¼ æ’­çš„2å€ï¼‰
    backward_flops = 2 * forward_flops
    
    # åˆ†ç±»å¤´FLOPs
    classification_flops = batch_size * hidden_size * num_classes
    
    total_flops = forward_flops + backward_flops + classification_flops
    
    return total_flops

def test_mfu_calculation():
    """æµ‹è¯•MFUè®¡ç®—"""
    print("ğŸš€ å¼€å§‹MFUè®¡ç®—æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å‚æ•°
    batch_sizes = [8, 16, 32, 64]
    seq_lengths = [256, 512, 1024, 2048]
    step_times = [0.1, 0.2, 0.5, 1.0]  # ç§’
    
    print("ğŸ“Š æµ‹è¯•å‚æ•°:")
    print(f"   â€¢ æ‰¹æ¬¡å¤§å°: {batch_sizes}")
    print(f"   â€¢ åºåˆ—é•¿åº¦: {seq_lengths}")
    print(f"   â€¢ æ­¥éª¤æ—¶é—´: {step_times}")
    print("=" * 60)
    
    # è·å–GPUå³°å€¼æ€§èƒ½
    peak_flops = get_gpu_peak_flops()
    print(f"ğŸ“ˆ GPUå³°å€¼æ€§èƒ½: {peak_flops/1e12:.0f} TFLOPs")
    print("=" * 60)
    
    results = []
    
    for batch_size in batch_sizes:
        for seq_length in seq_lengths:
            for step_time in step_times:
                # ä¼°ç®—FLOPs
                estimated_flops = estimate_flops_for_qwen2_5_vl(batch_size, seq_length)
                
                # è®¡ç®—MFU
                mfu = calculate_mfu_simple(estimated_flops, step_time)
                
                # è®¡ç®—å®é™…FLOPs/s
                actual_flops_per_second = estimated_flops / step_time
                
                # è®¡ç®—ååé‡
                tokens_per_second = batch_size * seq_length / step_time
                samples_per_second = batch_size / step_time
                
                result = {
                    'batch_size': batch_size,
                    'seq_length': seq_length,
                    'step_time': step_time,
                    'estimated_flops': estimated_flops,
                    'actual_flops_per_second': actual_flops_per_second,
                    'mfu': mfu,
                    'tokens_per_second': tokens_per_second,
                    'samples_per_second': samples_per_second
                }
                results.append(result)
                
                print(f"ğŸ“Š Batch={batch_size}, Seq={seq_length}, Time={step_time}s:")
                print(f"   â€¢ ä¼°ç®—FLOPs: {estimated_flops:.2e}")
                print(f"   â€¢ FLOPs/s: {actual_flops_per_second:.2e}")
                print(f"   â€¢ MFU: {mfu:.4f} ({mfu*100:.2f}%)")
                print(f"   â€¢ Tokens/s: {tokens_per_second:.0f}")
                print(f"   â€¢ Samples/s: {samples_per_second:.1f}")
                print()
    
    # åˆ†æç»“æœ
    print("=" * 60)
    print("ğŸ“ˆ MFUåˆ†æç»“æœ:")
    
    # æ‰¾å‡ºæœ€é«˜MFU
    max_mfu_result = max(results, key=lambda x: x['mfu'])
    print(f"   â€¢ æœ€é«˜MFU: {max_mfu_result['mfu']:.4f} ({max_mfu_result['mfu']*100:.2f}%)")
    print(f"     å‚æ•°: Batch={max_mfu_result['batch_size']}, Seq={max_mfu_result['seq_length']}, Time={max_mfu_result['step_time']}s")
    
    # æ‰¾å‡ºæœ€ä½MFU
    min_mfu_result = min(results, key=lambda x: x['mfu'])
    print(f"   â€¢ æœ€ä½MFU: {min_mfu_result['mfu']:.4f} ({min_mfu_result['mfu']*100:.2f}%)")
    print(f"     å‚æ•°: Batch={min_mfu_result['batch_size']}, Seq={min_mfu_result['seq_length']}, Time={min_mfu_result['step_time']}s")
    
    # å¹³å‡MFU
    avg_mfu = sum(r['mfu'] for r in results) / len(results)
    print(f"   â€¢ å¹³å‡MFU: {avg_mfu:.4f} ({avg_mfu*100:.2f}%)")
    
    print("=" * 60)
    print("ğŸ’¡ MFUä¼˜åŒ–å»ºè®®:")
    
    if avg_mfu < 0.1:
        print("   â€¢ MFUè¿‡ä½ï¼Œå¯èƒ½çš„åŸå› :")
        print("     - FLOPsä¼°ç®—ä¸å‡†ç¡®")
        print("     - GPUå³°å€¼æ€§èƒ½è®¾ç½®è¿‡é«˜")
        print("     - å®é™…FLOPsæµ‹é‡å¤±è´¥")
        print("     - æ¨¡å‹æ¶æ„ä¸ä¼°ç®—ä¸åŒ¹é…")
    elif avg_mfu < 0.3:
        print("   â€¢ MFUåä½ï¼Œå»ºè®®:")
        print("     - å¢åŠ batch_size")
        print("     - ä¼˜åŒ–åºåˆ—é•¿åº¦")
        print("     - æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½ç“¶é¢ˆ")
    else:
        print("   â€¢ MFUæ­£å¸¸ï¼Œæ€§èƒ½è‰¯å¥½")
    
    print("=" * 60)
    print("ğŸ”§ è°ƒè¯•å»ºè®®:")
    print("   1. æ£€æŸ¥å®é™…FLOPsæµ‹é‡æ˜¯å¦æˆåŠŸ")
    print("   2. éªŒè¯GPUå³°å€¼æ€§èƒ½è®¾ç½®æ˜¯å¦æ­£ç¡®")
    print("   3. å¯¹æ¯”ä¸åŒbatch_sizeå’Œåºåˆ—é•¿åº¦çš„MFU")
    print("   4. æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜å¸¦å®½é™åˆ¶")
    
    return results

if __name__ == "__main__":
    test_mfu_calculation() 