#!/bin/bash

# è¶…å¿«é€Ÿè¯„ä¼°è®­ç»ƒå¯åŠ¨è„šæœ¬
# ä½¿ç”¨å¤§æ‰¹æ¬¡å¤§å°å’Œå…¨é‡è¯„ä¼°æ•°æ®ï¼Œå¤§å¹…æå‡è¯„ä¼°é€Ÿåº¦

echo "ğŸš€ å¯åŠ¨è¶…å¿«é€Ÿè¯„ä¼°è®­ç»ƒ..."
echo "ğŸ“Š ä¼˜åŒ–é…ç½®ï¼š"
echo "  â€¢ è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼š64 x 4 GPU = 256"
echo "  â€¢ è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼š64 x 4 GPU = 256"
echo "  â€¢ è¯„ä¼°é¢‘ç‡ï¼šæ¯500æ­¥ï¼ˆå¤§å¹…å‡å°‘ï¼‰"
echo "  â€¢ è¯„ä¼°æ•°æ®ï¼šå…¨é‡æ•°æ®ï¼ˆ100%ï¼‰"
echo "  â€¢ è¿›åº¦æ¡æ›´æ–°ï¼šæ¯100æ‰¹æ¬¡"
echo ""

# æ£€æŸ¥flash-attnæ˜¯å¦å¯ç”¨
python -c "import flash_attn; print('âœ… FlashAttentionå¯ç”¨')" 2>/dev/null || {
    echo "âš ï¸  FlashAttentionä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›æœºåˆ¶"
    export FLASH_ATTENTION_FORCE_ENABLE=0
    export FLASH_ATTENTION_2=0
}

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1

# å¯åŠ¨è®­ç»ƒ
deepspeed --num_gpus=4 \
    training/complete_train.py \
    --config configs/ultra_fast_eval_config.yaml \
    --deepspeed_config configs/ds_large_eval_batch.json

echo "âœ… è¶…å¿«é€Ÿè¯„ä¼°è®­ç»ƒå®Œæˆ" 