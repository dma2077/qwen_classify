#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯ skip_evaluation å‚æ•°çš„æœ€é«˜ä¼˜å…ˆçº§åŠŸèƒ½
"""

import os
import sys
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_skip_evaluation_priority():
    """æµ‹è¯• skip_evaluation å‚æ•°çš„æœ€é«˜ä¼˜å…ˆçº§åŠŸèƒ½"""
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯• skip_evaluation æœ€é«˜ä¼˜å…ˆçº§åŠŸèƒ½")
    print("=" * 80)
    
    # åŠ è½½æµ‹è¯•é…ç½®æ–‡ä»¶
    config_path = project_root / "configs" / "test_skip_evaluation_priority.yaml"
    
    if not config_path.exists():
        print(f"âŒ æµ‹è¯•é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return False
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print("ğŸ“‹ åŸå§‹é…ç½®å‚æ•°:")
    print(f"  â€¢ skip_evaluation: {config['training']['skip_evaluation']}")
    print(f"  â€¢ save_all_checkpoints: {config['training']['save_all_checkpoints']}")
    print(f"  â€¢ best_model_tracking.enabled: {config['training']['best_model_tracking']['enabled']}")
    print(f"  â€¢ best_model_tracking.save_best_only: {config['training']['best_model_tracking']['save_best_only']}")
    print(f"  â€¢ evaluation.partial_eval_during_training: {config['training']['evaluation']['partial_eval_during_training']}")
    print(f"  â€¢ evaluation.full_eval_at_end: {config['training']['evaluation']['full_eval_at_end']}")
    print(f"  â€¢ evaluation.eval_best_model_only: {config['training']['evaluation']['eval_best_model_only']}")
    print("")
    
    try:
        # å¯¼å…¥ DeepSpeedTrainer å¹¶åˆå§‹åŒ–
        from training.deepspeed_trainer import DeepSpeedTrainer
        
        print("ğŸ”„ åˆå§‹åŒ– DeepSpeedTrainer...")
        trainer = DeepSpeedTrainer(config)
        
        print("âœ… DeepSpeedTrainer åˆå§‹åŒ–æˆåŠŸ")
        print("")
        
        # éªŒè¯å‚æ•°æ˜¯å¦è¢«æ­£ç¡®è¦†ç›–
        print("ğŸ” éªŒè¯å‚æ•°è¦†ç›–ç»“æœ:")
        
        tests = [
            ("skip_evaluation", trainer.skip_evaluation, True, "åº”è¯¥ä¿æŒä¸ºTrue"),
            ("best_model_enabled", trainer.best_model_enabled, False, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºFalse"),
            ("save_best_only", trainer.save_best_only, False, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºFalse"),
            ("save_all_checkpoints", trainer.save_all_checkpoints, True, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºTrue"),
            ("partial_eval_during_training", trainer.partial_eval_during_training, False, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºFalse"),
            ("full_eval_at_end", trainer.full_eval_at_end, False, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºFalse"),
            ("eval_best_model_only", trainer.eval_best_model_only, False, "åº”è¯¥è¢«å¼ºåˆ¶è®¾ä¸ºFalse"),
        ]
        
        all_passed = True
        for param_name, actual_value, expected_value, description in tests:
            if actual_value == expected_value:
                print(f"  âœ… {param_name}: {actual_value} ({description})")
            else:
                print(f"  âŒ {param_name}: {actual_value}, æœŸæœ›: {expected_value} ({description})")
                all_passed = False
        
        print("")
        
        # éªŒè¯é…ç½®ä¹Ÿè¢«æ­£ç¡®ä¿®æ”¹
        print("ğŸ” éªŒè¯é…ç½®æ–‡ä»¶å†…çš„å‚æ•°ä¹Ÿè¢«è¦†ç›–:")
        config_tests = [
            ("training.best_model_tracking.enabled", config['training']['best_model_tracking']['enabled'], False),
            ("training.best_model_tracking.save_best_only", config['training']['best_model_tracking']['save_best_only'], False),
            ("training.save_all_checkpoints", config['training']['save_all_checkpoints'], True),
            ("training.evaluation.partial_eval_during_training", config['training']['evaluation']['partial_eval_during_training'], False),
            ("training.evaluation.full_eval_at_end", config['training']['evaluation']['full_eval_at_end'], False),
            ("training.evaluation.eval_best_model_only", config['training']['evaluation']['eval_best_model_only'], False),
        ]
        
        for param_path, actual_value, expected_value in config_tests:
            if actual_value == expected_value:
                print(f"  âœ… {param_path}: {actual_value}")
            else:
                print(f"  âŒ {param_path}: {actual_value}, æœŸæœ›: {expected_value}")
                all_passed = False
        
        print("")
        
        # æµ‹è¯•è¯„ä¼°å‡½æ•°æ˜¯å¦æ­£ç¡®è·³è¿‡
        print("ğŸ” æµ‹è¯•è¯„ä¼°å‡½æ•°æ˜¯å¦æ­£ç¡®è·³è¿‡:")
        try:
            eval_loss, eval_accuracy = trainer.evaluate(step=1, log_to_wandb=False)
            if eval_loss == 0.0 and eval_accuracy == 0.0:
                print("  âœ… evaluate() æ–¹æ³•æ­£ç¡®è¿”å›é»˜è®¤å€¼ (0.0, 0.0)")
            else:
                print(f"  âŒ evaluate() æ–¹æ³•è¿”å›äº†éé»˜è®¤å€¼: ({eval_loss}, {eval_accuracy})")
                all_passed = False
        except Exception as e:
            print(f"  âŒ evaluate() æ–¹æ³•è°ƒç”¨å¤±è´¥: {e}")
            all_passed = False
        
        # æµ‹è¯•å¸¦è¿”å›ç»“æœçš„è¯„ä¼°å‡½æ•°
        try:
            eval_loss, eval_accuracy, eval_results = trainer.evaluate(step=1, log_to_wandb=False, return_results=True)
            expected_results = {'overall_loss': 0.0, 'overall_accuracy': 0.0}
            if eval_loss == 0.0 and eval_accuracy == 0.0 and eval_results == expected_results:
                print("  âœ… evaluate() æ–¹æ³• (return_results=True) æ­£ç¡®è¿”å›é»˜è®¤å€¼")
            else:
                print(f"  âŒ evaluate() æ–¹æ³• (return_results=True) è¿”å›äº†éé»˜è®¤å€¼")
                print(f"    eval_loss: {eval_loss}, eval_accuracy: {eval_accuracy}")
                print(f"    eval_results: {eval_results}")
                all_passed = False
        except Exception as e:
            print(f"  âŒ evaluate() æ–¹æ³• (return_results=True) è°ƒç”¨å¤±è´¥: {e}")
            all_passed = False
        
        return all_passed
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_without_skip_evaluation():
    """æµ‹è¯•ä¸è®¾ç½® skip_evaluation æ—¶çš„æ­£å¸¸è¡Œä¸º"""
    
    print("\n" + "=" * 80)
    print("ğŸ§ª å¯¹æ¯”æµ‹è¯•ï¼šä¸è®¾ç½® skip_evaluation")
    print("=" * 80)
    
    # åˆ›å»ºå¯¹æ¯”é…ç½®ï¼ˆä¸è®¾ç½® skip_evaluationï¼‰
    config = {
        'model': {
            'pretrained_name': "/llm_reco/dehua/model/Qwen2.5-VL-7B-Instruct",
            'num_labels': 101
        },
        'training': {
            'lr': 1e-5,
            'output_dir': "/tmp/test_normal",
            'logging_steps': 10,
            'save_steps': 100,
            'eval_steps': 50,
            'best_model_tracking': {
                'enabled': True,
                'save_best_only': True
            },
            'evaluation': {
                'partial_eval_during_training': True,
                'full_eval_at_end': True,
                'eval_best_model_only': True
            }
        },
        'datasets': {
            'dataset_configs': {
                'food101': {'num_classes': 101}
            }
        }
    }
    
    try:
        from training.deepspeed_trainer import DeepSpeedTrainer
        
        trainer = DeepSpeedTrainer(config)
        
        print("ğŸ“‹ ä¸è®¾ç½® skip_evaluation æ—¶çš„å‚æ•°å€¼:")
        print(f"  â€¢ skip_evaluation: {trainer.skip_evaluation}")
        print(f"  â€¢ best_model_enabled: {trainer.best_model_enabled}")
        print(f"  â€¢ save_best_only: {trainer.save_best_only}")
        print(f"  â€¢ partial_eval_during_training: {trainer.partial_eval_during_training}")
        print(f"  â€¢ full_eval_at_end: {trainer.full_eval_at_end}")
        print(f"  â€¢ eval_best_model_only: {trainer.eval_best_model_only}")
        
        # éªŒè¯å‚æ•°åº”è¯¥ä¿æŒåŸå§‹å€¼
        expected_values = {
            'skip_evaluation': False,
            'best_model_enabled': True,
            'save_best_only': True,
            'partial_eval_during_training': True,
            'full_eval_at_end': True,
            'eval_best_model_only': True
        }
        
        all_correct = True
        for param_name, expected_value in expected_values.items():
            actual_value = getattr(trainer, param_name)
            if actual_value == expected_value:
                print(f"  âœ… {param_name}: {actual_value} (æ­£ç¡®)")
            else:
                print(f"  âŒ {param_name}: {actual_value}, æœŸæœ›: {expected_value}")
                all_correct = False
        
        return all_correct
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ skip_evaluation æœ€é«˜ä¼˜å…ˆçº§åŠŸèƒ½æµ‹è¯•")
    print("è¿™ä¸ªæµ‹è¯•å°†éªŒè¯ skip_evaluation=true æ˜¯å¦èƒ½å¼ºåˆ¶è¦†ç›–æ‰€æœ‰ç›¸å…³å‚æ•°")
    print("")
    
    # è¿è¡Œæµ‹è¯•
    test1_pass = test_skip_evaluation_priority()
    test2_pass = test_without_skip_evaluation()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"  â€¢ skip_evaluation æœ€é«˜ä¼˜å…ˆçº§æµ‹è¯•: {'âœ… é€šè¿‡' if test1_pass else 'âŒ å¤±è´¥'}")
    print(f"  â€¢ å¯¹æ¯”æµ‹è¯•ï¼ˆæ­£å¸¸æ¨¡å¼ï¼‰: {'âœ… é€šè¿‡' if test2_pass else 'âŒ å¤±è´¥'}")
    
    if test1_pass and test2_pass:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼skip_evaluation æœ€é«˜ä¼˜å…ˆçº§åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("ğŸ’¡ ç°åœ¨ skip_evaluation=true ä¼šå¼ºåˆ¶è¦†ç›–æ‰€æœ‰ç›¸å…³çš„è¯„ä¼°å‚æ•°")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ä¿®æ”¹")
        sys.exit(1) 