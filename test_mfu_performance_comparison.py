#!/usr/bin/env python3
"""
MFUè®¡ç®—æ–¹æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•

æµ‹è¯•ä¸åŒMFUè®¡ç®—æ–¹æ³•çš„ï¼š
1. è®¡ç®—ç²¾åº¦
2. æ€§èƒ½å¼€é”€
3. å†…å­˜ä½¿ç”¨
4. é€‚ç”¨åœºæ™¯
"""

import time
import torch
import psutil
import gc
from typing import Dict, List, Tuple
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from training.utils.monitor import (
    calculate_mfu, 
    calculate_precise_mfu,
    get_gpu_peak_flops
)

def create_dummy_model(batch_size: int = 4, seq_length: int = 512):
    """åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æ¨¡å‹ç”¨äºæµ‹è¯•"""
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„Transformeræ¨¡å‹ç”¨äºæµ‹è¯•
        from transformers import AutoModelForImageClassification, AutoProcessor
        
        # ä½¿ç”¨ä¸€ä¸ªå°çš„é¢„è®­ç»ƒæ¨¡å‹
        model_name = "microsoft/resnet-50"
        model = AutoModelForImageClassification.from_pretrained(model_name)
        processor = AutoProcessor.from_pretrained(model_name)
        
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        # åˆ›å»ºè™šæ‹Ÿbatch
        dummy_batch = {
            "input_ids": torch.randint(0, 1000, (batch_size, seq_length), device=device),
            "attention_mask": torch.ones(batch_size, seq_length, device=device),
            "pixel_values": torch.randn(batch_size, 3, 224, 224, device=device),
            "labels": torch.randint(0, 10, (batch_size,), device=device)
        }
        
        return model, dummy_batch, processor
        
    except Exception as e:
        print(f"åˆ›å»ºè™šæ‹Ÿæ¨¡å‹å¤±è´¥: {e}")
        return None, None, None

def measure_performance_overhead(func, *args, **kwargs) -> Dict:
    """æµ‹é‡å‡½æ•°æ‰§è¡Œçš„æ€§èƒ½å¼€é”€"""
    # é¢„çƒ­
    for _ in range(3):
        try:
            func(*args, **kwargs)
        except:
            pass
    
    # æµ‹é‡å†…å­˜ä½¿ç”¨
    process = psutil.Process()
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # æµ‹é‡æ‰§è¡Œæ—¶é—´
    start_time = time.time()
    result = func(*args, **kwargs)
    execution_time = time.time() - start_time
    
    # æµ‹é‡å†…å­˜ä½¿ç”¨
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_used = memory_after - memory_before
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return {
        'result': result,
        'execution_time': execution_time,
        'memory_used': memory_used,
        'memory_before': memory_before,
        'memory_after': memory_after
    }

def test_mfu_methods():
    """æµ‹è¯•ä¸åŒçš„MFUè®¡ç®—æ–¹æ³•"""
    print("=" * 80)
    print("ğŸ§ª MFUè®¡ç®—æ–¹æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model, dummy_batch, processor = create_dummy_model()
    if model is None:
        print("âŒ æ— æ³•åˆ›å»ºæµ‹è¯•æ¨¡å‹ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    print(f"âœ… æµ‹è¯•æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   è®¾å¤‡: {next(model.parameters()).device}")
    print(f"   æ‰¹æ¬¡å¤§å°: {dummy_batch['input_ids'].size(0)}")
    print(f"   åºåˆ—é•¿åº¦: {dummy_batch['input_ids'].size(1)}")
    
    # è·å–GPUå³°å€¼æ€§èƒ½
    peak_flops = get_gpu_peak_flops()
    print(f"   GPUå³°å€¼æ€§èƒ½: {peak_flops/1e12:.1f} TFLOPs")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤æ—¶é—´
    step_time = 0.1  # 100ms per step
    
    print("\n" + "=" * 80)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ")
    print("=" * 80)
    
    # æµ‹è¯•ä¸åŒçš„MFUè®¡ç®—æ–¹æ³•
    methods = [
        ("åŸå§‹æ–¹æ³• (estimate)", lambda: calculate_mfu(model, 4, 512, step_time)),
        ("æ™ºèƒ½æ¨¡å¼ (smart)", lambda: calculate_precise_mfu(model, 4, 512, step_time, "smart")),
        ("Profileræ¨¡å¼ (profiler)", lambda: calculate_precise_mfu(model, 4, 512, step_time, "profiler")),
        ("ä¼°ç®—æ¨¡å¼ (estimate)", lambda: calculate_precise_mfu(model, 4, 512, step_time, "estimate")),
        ("æ··åˆæ¨¡å¼ (hybrid)", lambda: calculate_precise_mfu(model, 4, 512, step_time, "hybrid")),
    ]
    
    results = {}
    
    for method_name, method_func in methods:
        print(f"\nğŸ” æµ‹è¯•æ–¹æ³•: {method_name}")
        print("-" * 50)
        
        try:
            # æµ‹é‡æ€§èƒ½å¼€é”€
            perf_result = measure_performance_overhead(method_func)
            
            results[method_name] = {
                'mfu_value': perf_result['result'],
                'execution_time_ms': perf_result['execution_time'] * 1000,
                'memory_used_mb': perf_result['memory_used'],
                'success': True
            }
            
            print(f"   MFUå€¼: {perf_result['result']:.4f}")
            print(f"   æ‰§è¡Œæ—¶é—´: {perf_result['execution_time']*1000:.2f} ms")
            print(f"   å†…å­˜ä½¿ç”¨: {perf_result['memory_used']:.1f} MB")
            print(f"   âœ… æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
            results[method_name] = {
                'mfu_value': 0.0,
                'execution_time_ms': 0.0,
                'memory_used_mb': 0.0,
                'success': False,
                'error': str(e)
            }
    
    # åˆ†æç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    successful_results = {k: v for k, v in results.items() if v['success']}
    
    if successful_results:
        # æ‰¾å‡ºæœ€å¿«çš„å’Œæœ€æ…¢çš„æ–¹æ³•
        fastest_method = min(successful_results.items(), key=lambda x: x[1]['execution_time_ms'])
        slowest_method = max(successful_results.items(), key=lambda x: x[1]['execution_time_ms'])
        
        # æ‰¾å‡ºå†…å­˜ä½¿ç”¨æœ€å°‘å’Œæœ€å¤šçš„æ–¹æ³•
        lowest_memory = min(successful_results.items(), key=lambda x: x[1]['memory_used_mb'])
        highest_memory = max(successful_results.items(), key=lambda x: x[1]['memory_used_mb'])
        
        print(f"ğŸƒ æœ€å¿«æ–¹æ³•: {fastest_method[0]} ({fastest_method[1]['execution_time_ms']:.2f} ms)")
        print(f"ğŸŒ æœ€æ…¢æ–¹æ³•: {slowest_method[0]} ({slowest_method[1]['execution_time_ms']:.2f} ms)")
        print(f"ğŸ’¾ å†…å­˜æœ€å°‘: {lowest_memory[0]} ({lowest_memory[1]['memory_used_mb']:.1f} MB)")
        print(f"ğŸ’¾ å†…å­˜æœ€å¤š: {highest_memory[0]} ({highest_memory[1]['memory_used_mb']:.1f} MB)")
        
        # è®¡ç®—æ€§èƒ½å¼€é”€æ¯”ä¾‹
        if fastest_method[1]['execution_time_ms'] > 0:
            speedup_ratio = slowest_method[1]['execution_time_ms'] / fastest_method[1]['execution_time_ms']
            print(f"âš¡ é€Ÿåº¦å·®å¼‚: æœ€æ…¢æ–¹æ³•æ¯”æœ€å¿«æ–¹æ³•æ…¢ {speedup_ratio:.1f}x")
        
        if lowest_memory[1]['memory_used_mb'] > 0:
            memory_ratio = highest_memory[1]['memory_used_mb'] / lowest_memory[1]['memory_used_mb']
            print(f"ğŸ’¾ å†…å­˜å·®å¼‚: æœ€å¤šå†…å­˜æ¯”æœ€å°‘å†…å­˜å¤š {memory_ratio:.1f}x")
    
    # æ¨èä½¿ç”¨åœºæ™¯
    print("\n" + "=" * 80)
    print("ğŸ’¡ ä½¿ç”¨åœºæ™¯æ¨è")
    print("=" * 80)
    
    print("ğŸ¯ ä¸åŒåœºæ™¯çš„æ¨èæ–¹æ³•:")
    print()
    print("1. ğŸš€ ç”Ÿäº§ç¯å¢ƒè®­ç»ƒ (æ€§èƒ½ä¼˜å…ˆ):")
    print("   - æ¨è: æ™ºèƒ½æ¨¡å¼ (smart)")
    print("   - åŸå› : é¦–æ¬¡ç²¾ç¡®æµ‹é‡ï¼Œåç»­ä½¿ç”¨æ ¡å‡†ä¼°ç®—ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½")
    print()
    print("2. ğŸ”¬ ç ”ç©¶/è°ƒè¯• (ç²¾åº¦ä¼˜å…ˆ):")
    print("   - æ¨è: Profileræ¨¡å¼ (profiler)")
    print("   - åŸå› : æ¯æ¬¡ä½¿ç”¨PyTorch Profilerï¼Œè·å¾—æœ€ç²¾ç¡®çš„FLOPsæµ‹é‡")
    print()
    print("3. âš¡ å¿«é€ŸåŸå‹/æµ‹è¯• (é€Ÿåº¦ä¼˜å…ˆ):")
    print("   - æ¨è: ä¼°ç®—æ¨¡å¼ (estimate)")
    print("   - åŸå› : æ— profilingå¼€é”€ï¼Œé€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£")
    print()
    print("4. ğŸ”§ æ··åˆç¯å¢ƒ (å¹³è¡¡æ¨¡å¼):")
    print("   - æ¨è: æ··åˆæ¨¡å¼ (hybrid)")
    print("   - åŸå› : å°è¯•ç¡¬ä»¶è®¡æ•°å™¨ï¼Œå›é€€åˆ°profilerï¼Œé€‚åˆå¤æ‚ç¯å¢ƒ")
    print()
    print("5. ğŸ”„ ç°æœ‰ä»£ç å…¼å®¹:")
    print("   - æ¨è: åŸå§‹æ–¹æ³• (estimate)")
    print("   - åŸå› : ä¿æŒç°æœ‰è¡Œä¸ºï¼Œæ— é¢å¤–å¼€é”€")
    
    # æ€§èƒ½å½±å“æ€»ç»“
    print("\n" + "=" * 80)
    print("âš ï¸  æ€§èƒ½å½±å“æ€»ç»“")
    print("=" * 80)
    
    print("ğŸ“Š ä¸åŒæ–¹æ³•çš„æ€§èƒ½å¼€é”€:")
    print()
    print("â€¢ ä¼°ç®—æ–¹æ³• (estimate):")
    print("  - CPUå¼€é”€: < 1%")
    print("  - GPUå¼€é”€: 0%")
    print("  - å†…å­˜å¼€é”€: < 10MB")
    print("  - ç²¾åº¦: ä¸­ç­‰ (åŸºäºæ¨¡å‹ç»“æ„ä¼°ç®—)")
    print()
    print("â€¢ æ™ºèƒ½æ–¹æ³• (smart):")
    print("  - CPUå¼€é”€: é¦–æ¬¡ 5-15%, åç»­ < 1%")
    print("  - GPUå¼€é”€: é¦–æ¬¡ 2-8%, åç»­ 0%")
    print("  - å†…å­˜å¼€é”€: é¦–æ¬¡ 100-500MB, åç»­ < 10MB")
    print("  - ç²¾åº¦: é«˜ (é¦–æ¬¡ç²¾ç¡®æµ‹é‡ + æ ¡å‡†)")
    print()
    print("â€¢ Profileræ–¹æ³• (profiler):")
    print("  - CPUå¼€é”€: 5-15%")
    print("  - GPUå¼€é”€: 2-8%")
    print("  - å†…å­˜å¼€é”€: 100-500MB")
    print("  - ç²¾åº¦: æœ€é«˜ (æ¯æ¬¡ç²¾ç¡®æµ‹é‡)")
    print()
    print("â€¢ æ··åˆæ–¹æ³• (hybrid):")
    print("  - CPUå¼€é”€: 2-10%")
    print("  - GPUå¼€é”€: 1-5%")
    print("  - å†…å­˜å¼€é”€: 50-300MB")
    print("  - ç²¾åº¦: é«˜ (ç¡¬ä»¶è®¡æ•°å™¨ + profilerå›é€€)")
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 80)

def test_training_scenario():
    """æ¨¡æ‹ŸçœŸå®è®­ç»ƒåœºæ™¯çš„æ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸ¯ çœŸå®è®­ç»ƒåœºæ™¯æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    
    model, dummy_batch, processor = create_dummy_model()
    if model is None:
        return
    
    # æ¨¡æ‹Ÿ1000ä¸ªè®­ç»ƒæ­¥éª¤
    num_steps = 1000
    step_time = 0.1  # 100ms per step
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿ {num_steps} ä¸ªè®­ç»ƒæ­¥éª¤çš„æ€§èƒ½å½±å“")
    print()
    
    # æµ‹è¯•ä¸åŒæ–¹æ³•åœ¨é•¿æœŸè®­ç»ƒä¸­çš„æ€§èƒ½
    methods = [
        ("ä¼°ç®—æ–¹æ³•", "estimate"),
        ("æ™ºèƒ½æ–¹æ³•", "smart"),
        ("Profileræ–¹æ³•", "profiler"),
    ]
    
    for method_name, method_mode in methods:
        print(f"ğŸ” æµ‹è¯•æ–¹æ³•: {method_name}")
        
        # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
        start_time = time.time()
        total_mfu = 0.0
        successful_steps = 0
        
        for step in range(num_steps):
            try:
                if method_mode == "estimate":
                    mfu = calculate_precise_mfu(model, 4, 512, step_time, "estimate")
                elif method_mode == "smart":
                    mfu = calculate_precise_mfu(model, 4, 512, step_time, "smart")
                elif method_mode == "profiler":
                    mfu = calculate_precise_mfu(model, 4, 512, step_time, "profiler")
                
                total_mfu += mfu
                successful_steps += 1
                
            except Exception as e:
                print(f"   æ­¥éª¤ {step} å¤±è´¥: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        avg_mfu = total_mfu / successful_steps if successful_steps > 0 else 0
        
        # è®¡ç®—æ€§èƒ½å¼€é”€
        baseline_time = num_steps * step_time  # å‡è®¾æ­£å¸¸è®­ç»ƒæ—¶é—´
        overhead_time = total_time - baseline_time
        overhead_percentage = (overhead_time / baseline_time) * 100 if baseline_time > 0 else 0
        
        print(f"   âœ… æˆåŠŸæ­¥éª¤: {successful_steps}/{num_steps}")
        print(f"   ğŸ“Š å¹³å‡MFU: {avg_mfu:.4f}")
        print(f"   â±ï¸  æ€»æ—¶é—´: {total_time:.2f}s")
        print(f"   ğŸ“ˆ æ€§èƒ½å¼€é”€: {overhead_time:.2f}s ({overhead_percentage:.1f}%)")
        print(f"   ğŸš€ è®­ç»ƒé€Ÿåº¦: {num_steps/total_time:.1f} steps/s")
        print()

if __name__ == "__main__":
    try:
        # åŸºç¡€æ€§èƒ½æµ‹è¯•
        test_mfu_methods()
        
        # çœŸå®è®­ç»ƒåœºæ™¯æµ‹è¯•
        test_training_scenario()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 