#!/usr/bin/env python3
"""
ç®€åŒ–çš„evalæŒ‡æ ‡å’ŒMFUè®¡ç®—æµ‹è¯•
ä¸ä¾èµ–å®é™…æ•°æ®æ–‡ä»¶ï¼Œåªæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import time
import torch
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from training.deepspeed_trainer import DeepSpeedTrainer
from training.utils.config_utils import prepare_config
from training.utils.monitor import TrainingMonitor

def test_eval_metrics_building():
    """æµ‹è¯•evalæŒ‡æ ‡æ„å»ºåŠŸèƒ½"""
    print("ğŸ“Š æµ‹è¯•evalæŒ‡æ ‡æ„å»ºåŠŸèƒ½...")
    
    # åˆ›å»ºmonitorå®ä¾‹
    monitor = TrainingMonitor(
        output_dir='./test_output',
        config={'training': {'batch_size': 2}},
        flops_profile_freq=5
    )
    
    # æ¨¡æ‹Ÿevalç»“æœ
    eval_loss = 0.5
    eval_accuracy = 0.85
    eval_results = {
        'total_samples': 100,
        'total_correct': 85,
        'dataset_metrics': {
            'food101': {
                'loss': 0.4,
                'accuracy': 0.9,
                'samples': 50,
                'correct': 45
            },
            'cifar10': {
                'loss': 0.6,
                'accuracy': 0.8,
                'samples': 50,
                'correct': 40
            }
        }
    }
    
    # åˆ›å»ºtrainerå®ä¾‹æ¥æµ‹è¯•_build_eval_metricsæ–¹æ³•
    trainer = DeepSpeedTrainer({'output_dir': './test_output'})
    
    # æ„å»ºevalæŒ‡æ ‡
    eval_data = trainer._build_eval_metrics(eval_loss, eval_accuracy, eval_results)
    
    print(f"    ğŸ“‹ æ„å»ºçš„evalæŒ‡æ ‡:")
    for key, value in eval_data.items():
        print(f"      {key}: {value}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„æŒ‡æ ‡
    expected_keys = [
        'eval/overall_loss',
        'eval/overall_accuracy',
        'eval/overall_samples',
        'eval/overall_correct',
        'eval/food101_loss',
        'eval/food101_accuracy',
        'eval/food101_samples',
        'eval/food101_correct',
        'eval/cifar10_loss',
        'eval/cifar10_accuracy',
        'eval/cifar10_samples',
        'eval/cifar10_correct'
    ]
    
    missing_keys = [key for key in expected_keys if key not in eval_data]
    if missing_keys:
        print(f"    âš ï¸ ç¼ºå¤±çš„evalæŒ‡æ ‡: {missing_keys}")
        return False
    else:
        print(f"    âœ… æ‰€æœ‰é¢„æœŸçš„evalæŒ‡æ ‡éƒ½å·²åŒ…å«")
        return True

def test_mfu_calculation_logic():
    """æµ‹è¯•MFUè®¡ç®—é€»è¾‘ï¼ˆä¸ä¾èµ–å®é™…æ¨¡å‹ï¼‰"""
    print("ğŸ” æµ‹è¯•MFUè®¡ç®—é€»è¾‘...")
    
    # åˆ›å»ºmonitorå®ä¾‹
    monitor = TrainingMonitor(
        output_dir='./test_output',
        config={'training': {'batch_size': 2}},
        flops_profile_freq=5
    )
    
    # æ¨¡æ‹Ÿä¸€äº›å€¼
    monitor.actual_flops = 1e12  # 1 TFLOPs
    monitor.model_ref = "dummy_model"
    
    # åˆ›å»ºtrainerå®ä¾‹
    trainer = DeepSpeedTrainer({'output_dir': './test_output'})
    trainer.monitor = monitor
    trainer.dist_ctx = type('obj', (object,), {
        'is_main_process': lambda: True,
        'world_size': 1
    })()
    
    # æµ‹è¯•ä¸åŒçš„step_timeå€¼
    test_cases = [
        (0.1, "æ­£å¸¸step_time"),
        (0.0, "é›¶step_time"),
        (None, "None step_time")
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„tensor
    inputs = torch.randn(2, 10)  # batch_size=2, seq_len=10
    attention_mask = torch.ones(2, 10)
    
    all_tests_passed = True
    
    for step_time, description in test_cases:
        print(f"    ğŸ” æµ‹è¯• {description}...")
        
        # è®¡ç®—MFU
        mfu = trainer._calculate_mfu(1, inputs, attention_mask, step_time or 0.0)
        
        if mfu is not None:
            print(f"      âœ… MFUè®¡ç®—æˆåŠŸ: {mfu:.4f}")
        else:
            print(f"      âš ï¸ MFUè®¡ç®—è¿”å›None (é¢„æœŸè¡Œä¸º)")
            if description == "é›¶step_time" or description == "None step_time":
                print(f"        âœ… è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºstep_timeæ— æ•ˆ")
            else:
                print(f"        âŒ è¿™ä¸åº”è¯¥å‘ç”Ÿ")
                all_tests_passed = False
    
    # æµ‹è¯•actual_flopsä¸ºNoneçš„æƒ…å†µ
    print(f"    ğŸ” æµ‹è¯•actual_flopsä¸ºNone...")
    original_flops = monitor.actual_flops
    monitor.actual_flops = None
    
    mfu = trainer._calculate_mfu(1, inputs, attention_mask, 0.1)
    if mfu is None:
        print(f"      âœ… MFUè®¡ç®—æ­£ç¡®è¿”å›None (actual_flopsä¸ºNone)")
    else:
        print(f"      âŒ MFUè®¡ç®—åº”è¯¥è¿”å›None")
        all_tests_passed = False
    
    # æ¢å¤original_flops
    monitor.actual_flops = original_flops
    
    return all_tests_passed

def test_combined_metrics_logging():
    """æµ‹è¯•åˆå¹¶æŒ‡æ ‡è®°å½•é€»è¾‘"""
    print("ğŸ“ æµ‹è¯•åˆå¹¶æŒ‡æ ‡è®°å½•é€»è¾‘...")
    
    # æ¨¡æ‹Ÿtrainingå’Œevalæ•°æ®
    training_data = {
        "training/loss": 0.3,
        "training/lr": 1e-5,
        "training/epoch": 0.5,
        "training/grad_norm": 0.1,
        "perf/step_time": 0.05,
        "perf/mfu": 0.75,
        "step": 10
    }
    
    eval_data = {
        "eval/overall_loss": 0.5,
        "eval/overall_accuracy": 0.85,
        "eval/overall_samples": 100,
        "eval/overall_correct": 85
    }
    
    # æµ‹è¯•åˆå¹¶é€»è¾‘
    combined_data = training_data.copy()
    combined_data.update(eval_data)
    
    print(f"    ğŸ“‹ åˆå¹¶åçš„æŒ‡æ ‡:")
    for key, value in combined_data.items():
        print(f"      {key}: {value}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æŒ‡æ ‡
    expected_training_keys = [k for k in training_data.keys() if k.startswith('training/')]
    expected_perf_keys = [k for k in training_data.keys() if k.startswith('perf/')]
    expected_eval_keys = [k for k in eval_data.keys() if k.startswith('eval/')]
    
    missing_training = [k for k in expected_training_keys if k not in combined_data]
    missing_perf = [k for k in expected_perf_keys if k not in combined_data]
    missing_eval = [k for k in expected_eval_keys if k not in combined_data]
    
    if missing_training or missing_perf or missing_eval:
        print(f"    âŒ ç¼ºå¤±æŒ‡æ ‡:")
        if missing_training:
            print(f"      training: {missing_training}")
        if missing_perf:
            print(f"      perf: {missing_perf}")
        if missing_eval:
            print(f"      eval: {missing_eval}")
        return False
    else:
        print(f"    âœ… æ‰€æœ‰æŒ‡æ ‡éƒ½å·²æ­£ç¡®åˆå¹¶")
        return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹ç®€åŒ–æµ‹è¯•evalæŒ‡æ ‡å’ŒMFUè®¡ç®—ä¿®å¤...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('./test_output', exist_ok=True)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("EvalæŒ‡æ ‡æ„å»º", test_eval_metrics_building),
        ("MFUè®¡ç®—é€»è¾‘", test_mfu_calculation_logic),
        ("åˆå¹¶æŒ‡æ ‡è®°å½•", test_combined_metrics_logging)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•: {test_name}")
        print(f"{'='*50}")
        
        try:
            if test_func():
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                passed_tests += 1
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºé”™: {e}")
    
    print(f"\n{'='*50}")
    print(f"æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    print(f"{'='*50}")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤åº”è¯¥æœ‰æ•ˆã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")

if __name__ == "__main__":
    main() 