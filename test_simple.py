import torch
import os
from PIL import Image
import numpy as np

def test_pooling_logic():
    """æµ‹è¯•poolingé€»è¾‘ï¼ˆä¸éœ€è¦åŠ è½½å¤§æ¨¡å‹ï¼‰"""
    print("ğŸ§ª æµ‹è¯•Poolingé€»è¾‘")
    print("="*50)
    
    # æ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®
    batch_size = 1
    visual_tokens = 576  # å¸¸è§çš„è§†è§‰tokensæ•°é‡
    text_tokens = 50     # æ–‡æœ¬tokensæ•°é‡
    total_length = visual_tokens + text_tokens + 10  # åŠ ä¸€äº›padding
    hidden_dim = 768
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®è®¾ç½®:")
    print(f"  â€¢ Visual tokens: {visual_tokens}")
    print(f"  â€¢ Text tokens: {text_tokens}")
    print(f"  â€¢ Total sequence length: {total_length}")
    print(f"  â€¢ Padding: {total_length - visual_tokens - text_tokens}")
    
    # åˆ›å»ºattention_mask
    # å‰(visual_tokens + text_tokens)ä¸ªä½ç½®æœ‰æ•ˆï¼Œåé¢æ˜¯padding
    attention_mask = torch.zeros(batch_size, total_length)
    attention_mask[:, :visual_tokens + text_tokens] = 1
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„input_idsï¼ˆåªåŒ…å«æ–‡æœ¬éƒ¨åˆ†ï¼‰
    input_ids = torch.randint(1, 1000, (batch_size, text_tokens))
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„hidden_states
    hidden_states = torch.randn(batch_size, total_length, hidden_dim)
    
    print(f"\nğŸ¯ æ•°æ®å½¢çŠ¶:")
    print(f"  â€¢ input_ids: {input_ids.shape}")
    print(f"  â€¢ attention_mask: {attention_mask.shape}")
    print(f"  â€¢ hidden_states: {hidden_states.shape}")
    
    # æµ‹è¯•æ—§çš„poolingæ–¹æ³•ï¼ˆé”™è¯¯çš„ï¼‰
    print(f"\nâŒ æ—§æ–¹æ³• (é”™è¯¯çš„):")
    pad_token_id = 0
    mask = input_ids.ne(pad_token_id)
    old_last_positions = mask.sum(dim=1) - 1
    print(f"  â€¢ åŸºäºinput_idsçš„last_positions: {old_last_positions}")
    print(f"  â€¢ è¿™ä¸ªä½ç½®çš„attentionå€¼: {attention_mask[0, old_last_positions].item()}")
    print(f"  â€¢ é—®é¢˜: è¿™ä¸ªä½ç½®åœ¨visual tokensä¸­é—´ï¼Œä¸æ˜¯åºåˆ—æœ«å°¾ï¼")
    
    # æµ‹è¯•æ–°çš„poolingæ–¹æ³•ï¼ˆæ­£ç¡®çš„ï¼‰
    print(f"\nâœ… æ–°æ–¹æ³• (æ­£ç¡®çš„):")
    if attention_mask is not None:
        valid_lengths = attention_mask.sum(dim=1)
        new_last_positions = valid_lengths - 1
        new_last_positions = torch.clamp(new_last_positions, min=0, max=hidden_states.size(1)-1)
        
        print(f"  â€¢ åŸºäºattention_maskçš„valid_lengths: {valid_lengths}")
        print(f"  â€¢ åŸºäºattention_maskçš„last_positions: {new_last_positions}")
        print(f"  â€¢ è¿™ä¸ªä½ç½®çš„attentionå€¼: {attention_mask[0, new_last_positions].item()}")
        print(f"  â€¢ æ­£ç¡®: è¿™æ˜¯åºåˆ—çš„çœŸæ­£æœ«å°¾ä½ç½®ï¼")
    
    # æ˜¾ç¤ºå·®å¼‚
    print(f"\nğŸ“ ä½ç½®å·®å¼‚:")
    old_pos = old_last_positions.item()
    new_pos = new_last_positions.item()
    print(f"  â€¢ æ—§æ–¹æ³•ä½ç½®: {old_pos}")
    print(f"  â€¢ æ–°æ–¹æ³•ä½ç½®: {new_pos}")
    print(f"  â€¢ å·®å¼‚: {new_pos - old_pos} tokens")
    print(f"  â€¢ æ—§æ–¹æ³•è·³è¿‡äº† {new_pos - old_pos} ä¸ªvisual tokens!")
    
    # æ˜¾ç¤ºattention_maskæ¨¡å¼
    print(f"\nğŸ¯ Attention Mask æ¨¡å¼:")
    mask_values = attention_mask[0].tolist()
    print(f"  â€¢ å‰10ä¸ªä½ç½®: {mask_values[:10]}")
    print(f"  â€¢ ä¸­é—´10ä¸ªä½ç½® (pos {total_length//2-5} to {total_length//2+4}): {mask_values[total_length//2-5:total_length//2+5]}")
    print(f"  â€¢ å10ä¸ªä½ç½®: {mask_values[-10:]}")
    
    # éªŒè¯è¾¹ç•Œæƒ…å†µ
    print(f"\nğŸ” è¾¹ç•ŒéªŒè¯:")
    print(f"  â€¢ ä½ç½® {new_pos-1}: attention = {attention_mask[0, new_pos-1].item()}")
    print(f"  â€¢ ä½ç½® {new_pos}: attention = {attention_mask[0, new_pos].item()}")
    if new_pos + 1 < attention_mask.size(1):
        print(f"  â€¢ ä½ç½® {new_pos+1}: attention = {attention_mask[0, new_pos+1].item()}")
    
    return old_pos, new_pos, new_pos - old_pos

def test_with_real_data():
    """å¦‚æœå¯ä»¥çš„è¯ï¼Œç”¨çœŸå®æ•°æ®æµ‹è¯•"""
    print(f"\nğŸ”„ å°è¯•ç”¨çœŸå®æ•°æ®æµ‹è¯•...")
    
    try:
        from transformers import AutoProcessor
        
        # å°è¯•åŠ è½½processorï¼ˆä¸åŠ è½½å¤§æ¨¡å‹ï¼‰
        model_path = "/llm_reco/dehua/model/Qwen2.5-VL-7B-Instruct"
        if os.path.exists(model_path):
            processor = AutoProcessor.from_pretrained(model_path)
            print("âœ… ProcessoråŠ è½½æˆåŠŸ")
            
            # åˆ›å»ºæµ‹è¯•å›¾ç‰‡å’Œæ–‡æœ¬
            test_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
            text = "This is an image of food, what dish is it?"
            
            # å¤„ç†è¾“å…¥
            inputs = processor(
                text=[text],
                images=[test_image],
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            print(f"ğŸ“Š çœŸå®æ•°æ®åˆ†æ:")
            print(f"  â€¢ input_ids shape: {inputs['input_ids'].shape}")
            print(f"  â€¢ attention_mask shape: {inputs['attention_mask'].shape}")
            print(f"  â€¢ pixel_values shape: {inputs['pixel_values'].shape}")
            
            if 'image_grid_thw' in inputs:
                print(f"  â€¢ image_grid_thw: {inputs['image_grid_thw']}")
                visual_tokens = inputs['image_grid_thw'][0, 0] * inputs['image_grid_thw'][0, 1] * inputs['image_grid_thw'][0, 2]
                print(f"  â€¢ è®¡ç®—çš„visual tokens: {visual_tokens}")
            
            # åˆ†æåºåˆ—é•¿åº¦
            text_len = inputs['input_ids'].size(1)
            total_len = inputs['attention_mask'].size(1)
            valid_len = inputs['attention_mask'].sum(dim=1).item()
            
            print(f"  â€¢ æ–‡æœ¬é•¿åº¦: {text_len}")
            print(f"  â€¢ æ€»åºåˆ—é•¿åº¦: {total_len}")
            print(f"  â€¢ æœ‰æ•ˆé•¿åº¦: {valid_len}")
            print(f"  â€¢ æ¨æ–­çš„visual tokens: {valid_len - text_len}")
            
            # æµ‹è¯•pooling
            attention_mask = inputs['attention_mask']
            valid_lengths = attention_mask.sum(dim=1)
            last_positions = valid_lengths - 1
            
            print(f"  â€¢ Poolingä½ç½®: {last_positions.item()}")
            print(f"  â€¢ è¯¥ä½ç½®attentionå€¼: {attention_mask[0, last_positions.item()].item()}")
            
        else:
            print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡çœŸå®æ•°æ®æµ‹è¯•")
            
    except Exception as e:
        print(f"âŒ çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œé€»è¾‘æµ‹è¯•
    old_pos, new_pos, diff = test_pooling_logic()
    
    # å°è¯•çœŸå®æ•°æ®æµ‹è¯•
    test_with_real_data()
    
    # æ€»ç»“
    print(f"\nğŸ“‹ æ€»ç»“:")
    print("="*50)
    print(f"âœ… ä¿®å¤å‰poolingä½ç½®: {old_pos}")
    print(f"âœ… ä¿®å¤åpoolingä½ç½®: {new_pos}")
    print(f"âœ… ä¿®å¤æ•ˆæœ: æ­£ç¡®å¤„ç†äº†{diff}ä¸ªvisual tokens")
    print(f"âœ… ç°åœ¨ä»çœŸæ­£çš„åºåˆ—æœ«å°¾æå–ç‰¹å¾")
    print("="*50) 