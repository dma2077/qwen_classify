#!/usr/bin/env python3
"""
æµ‹è¯•stepä¿®å¤ - éªŒè¯effective_stepå’Œglobal_stepçš„ä½¿ç”¨
"""

import os
import time
import json
import torch
import wandb
from typing import Dict

# æ¨¡æ‹Ÿé…ç½®
config = {
    'output_dir': './test_output',
    'wandb': {
        'enabled': True,
        'project': 'qwen_classify_test',
        'run_name': 'step_fix_test',
        'tags': ['test', 'step_fix']
    },
    'monitor': {
        'freq': {
            'all_freq': 1  # æ¯æ­¥éƒ½è®°å½•
        }
    },
    'model': {
        'max_sequence_length': 512
    },
    'deepspeed': {
        'train_batch_size': 32
    }
}

class StepTestMonitor:
    """æµ‹è¯•stepä½¿ç”¨çš„ç›‘æ§å™¨"""
    
    def __init__(self, output_dir: str, config: Dict):
        self.output_dir = output_dir
        self.config = config
        self.start_time = time.time()
        self.step_start_time = time.time()
        
        # åˆå§‹åŒ–wandb
        self._init_wandb()
        
    def _init_wandb(self):
        """åˆå§‹åŒ–wandb"""
        try:
            wandb.init(
                project=self.config['wandb']['project'],
                name=self.config['wandb']['run_name'],
                tags=self.config['wandb']['tags'],
                config=self.config
            )
            
            # å®šä¹‰æ‰€æœ‰æŒ‡æ ‡ç»„
            wandb.define_metric("step")
            wandb.define_metric("training/*", step_metric="step")
            wandb.define_metric("perf/*", step_metric="step")
            wandb.define_metric("eval/*", step_metric="step")
            
            print("âœ… WandBåˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸ“Š é¡¹ç›®: {wandb.run.project}")
            print(f"ğŸ”— è¿è¡Œ: {wandb.run.name}")
            print(f"ğŸš€ æŸ¥çœ‹åœ°å€: {wandb.run.url}")
            
        except Exception as e:
            print(f"âŒ WandBåˆå§‹åŒ–å¤±è´¥: {e}")
            return
    
    def log_step(self, step: int, epoch: int, loss: float, grad_norm: float, learning_rate: float):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        current_time = time.time()
        step_time = current_time - self.step_start_time
        
        # å‡†å¤‡æŒ‡æ ‡æ•°æ®
        wandb_data = {
            "step": int(step),
            "training/loss": float(loss),
            "training/lr": float(learning_rate),
            "training/epoch": float(epoch),
            "training/grad_norm": float(grad_norm),
            "perf/step_time": float(step_time),
            "perf/steps_per_second": float(1.0 / step_time) if step_time > 0 else 0.0,
        }
        
        # è®°å½•åˆ°wandb
        try:
            wandb.log(wandb_data, step=int(step), commit=True)
            print(f"âœ… Step {step}: å·²è®°å½•è®­ç»ƒæŒ‡æ ‡")
        except Exception as e:
            print(f"âŒ è®°å½•è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
        
        self.step_start_time = current_time
    
    def log_evaluation(self, step: int, eval_loss: float, eval_accuracy: float):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        eval_data = {
            "step": int(step),
            "eval/overall_loss": float(eval_loss),
            "eval/overall_accuracy": float(eval_accuracy),
        }
        
        try:
            wandb.log(eval_data, step=int(step), commit=True)
            print(f"âœ… Eval Step {step}: å·²è®°å½•è¯„ä¼°æŒ‡æ ‡")
        except Exception as e:
            print(f"âŒ è®°å½•è¯„ä¼°æŒ‡æ ‡å¤±è´¥: {e}")
    
    def finish(self):
        """ç»“æŸè®­ç»ƒ"""
        if wandb.run is not None:
            total_time = time.time() - self.start_time
            wandb.log({"training/total_time": total_time}, commit=True)
            wandb.finish()
            print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

def simulate_training_with_gradient_accumulation():
    """æ¨¡æ‹Ÿå¸¦æ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒè¿‡ç¨‹"""
    print("ğŸš€ å¼€å§‹æ¨¡æ‹Ÿå¸¦æ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = StepTestMonitor('./test_output', config)
    
    # æ¨¡æ‹Ÿå‚æ•°
    gradient_accumulation_steps = 4  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    total_effective_steps = 20       # æ€»æœ‰æ•ˆæ­¥æ•°
    eval_steps = 5                   # è¯„ä¼°é—´éš”
    
    print(f"ğŸ“ˆ è®­ç»ƒå‚æ•°:")
    print(f"   â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"   â€¢ æ€»æœ‰æ•ˆæ­¥æ•°: {total_effective_steps}")
    print(f"   â€¢ è¯„ä¼°é—´éš”: {eval_steps}")
    print(f"   â€¢ æ€»globalæ­¥æ•°: {total_effective_steps * gradient_accumulation_steps}")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    global_step = 0
    effective_step = 0
    
    for epoch in range(2):  # 2ä¸ªepoch
        print(f"ğŸ“Š Epoch {epoch + 1}/2")
        
        for batch_idx in range(total_effective_steps * gradient_accumulation_steps):
            global_step += 1
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
            loss = 2.0 - (effective_step * 0.05) + (torch.rand(1).item() * 0.1)
            grad_norm = 0.5 + (torch.rand(1).item() * 0.3)
            learning_rate = 1e-5 * (0.95 ** (effective_step // 10))
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ­¥éª¤ï¼ˆå®Œæˆäº†æ¢¯åº¦ç´¯ç§¯ï¼‰
            is_effective_step = global_step % gradient_accumulation_steps == 0
            
            if is_effective_step:
                effective_step += 1
                
                print(f"   Global Step {global_step} -> Effective Step {effective_step}")
                
                # è®°å½•è®­ç»ƒæ­¥éª¤ï¼ˆä½¿ç”¨effective_stepï¼‰
                monitor.log_step(effective_step, epoch, loss, grad_norm, learning_rate)
                
                # æ¯eval_stepsæ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
                if effective_step % eval_steps == 0:
                    # æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®
                    eval_loss = loss * 0.8 + (torch.rand(1).item() * 0.1)
                    eval_accuracy = 0.3 + (effective_step * 0.02) + (torch.rand(1).item() * 0.05)
                    eval_accuracy = min(eval_accuracy, 0.95)
                    
                    # è®°å½•è¯„ä¼°æŒ‡æ ‡ï¼ˆä½¿ç”¨effective_stepï¼‰
                    monitor.log_evaluation(effective_step, eval_loss, eval_accuracy)
                    print(f"     ğŸ“Š è¯„ä¼°: loss={eval_loss:.4f}, acc={eval_accuracy:.4f}")
            
            # çŸ­æš‚æš‚åœ
            time.sleep(0.05)
    
    # ç»“æŸè®­ç»ƒ
    monitor.finish()
    
    print("=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š è¯·åœ¨WandBç•Œé¢ä¸­æ£€æŸ¥ï¼š")
    print("   â€¢ æ˜¯å¦è¿˜æœ‰'Steps must be monotonically increasing'è­¦å‘Š")
    print("   â€¢ æ‰€æœ‰æŒ‡æ ‡æ˜¯å¦éƒ½ä½¿ç”¨ç»Ÿä¸€çš„stepè½´")
    print("   â€¢ trainingã€perfã€evalæŒ‡æ ‡æ˜¯å¦éƒ½æ­£å¸¸æ˜¾ç¤º")

if __name__ == "__main__":
    simulate_training_with_gradient_accumulation() 